Fri Jun 30 19:41:58 AEST 2023
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/Renee_run_finetune_kids_myST-OGI.py
Started: 30/06/2023 19:41:59

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: 20211026-base-myST-OGI-TLT
datasetdict_id: myST-OGI-TLT-finetune
base_fp: /srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/s5/
train_name: myST-OGI-TLT17
train_filename: THESIS_C/myST-OGI-TLT_data_finetune_light
evaluation_name: myST
evaluation_filename: THESIS_C/myST_data_dev_light
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base-960h
eval_pretrained: False
baseline_model: facebook/wav2vec2-base-960h

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 15
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/chacmod/renee_thesis/s5/myST-OGI_local/THESIS_C/myST-OGI_data_finetune_light.csv
--> data_test_fp: /srv/scratch/chacmod/renee_thesis/s5/myST_local/THESIS_C/myST_data_dev_light.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/myST-OGI-finetune
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/Renee_MyST_OGI/vocab_20211015_2-base-myST-OGI.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/Renee_MyST_OGI/20211015_2-base-myST-OGI
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/Renee_MyST_OGI/20211015_2-base-myST-OGI_baseline_results.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/Renee_MyST_OGI/20211015_2-base-myST-OGI_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base-960h

------> PREPARING DATASET... ------------------------------------

Downloading and preparing dataset csv/default to /srv/scratch/chacmod/.cache/huggingface/datasets/myST-OGI-finetune/csv/default-cb3adfd9ad251ba8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 12965.39it/s]
Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files:  50%|█████     | 1/2 [00:00<00:00,  4.89it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 50000 examples [00:00, 297095.84 examples/s]Generating train split: 128484 examples [00:00, 494530.91 examples/s]                                                                     Traceback (most recent call last):
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 1882, in _prepare_split_single
    num_examples, num_bytes = writer.finalize()
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/arrow_writer.py", line 590, in finalize
    self.stream.close()
  File "/home/z5313567/.local/lib/python3.10/site-packages/fsspec/implementations/local.py", line 399, in close
    return self.f.close()
OSError: [Errno 122] Disk quota exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/srv/scratch/z5313567/thesis/wav2vec2/code/Renee_run_finetune_kids_myST-OGI.py", line 317, in <module>
    data = load_dataset('csv', 
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/load.py", line 1797, in load_dataset
    builder_instance.download_and_prepare(
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 890, in download_and_prepare
    self._download_and_prepare(
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 985, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 1746, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 1891, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
Fri Jun 30 20:49:28 AEST 2023
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/Renee_run_finetune_kids_myST-OGI.py
Started: 30/06/2023 20:49:29

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: 20211026-base-myST-OGI-TLT
datasetdict_id: myST-OGI-TLT-finetune
base_fp: /srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/s5/
train_name: myST-OGI-TLT17
train_filename: THESIS_C/myST-OGI-TLT_data_finetune_light
evaluation_name: myST
evaluation_filename: THESIS_C/myST_data_dev_light
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base-960h
eval_pretrained: False
baseline_model: facebook/wav2vec2-base-960h

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 15
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/chacmod/renee_thesis/s5/myST-OGI_local/THESIS_C/myST-OGI_data_finetune_light.csv
--> data_test_fp: /srv/scratch/chacmod/renee_thesis/s5/myST_local/THESIS_C/myST_data_dev_light.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/myST-OGI-finetune
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/Renee_MyST_OGI/vocab_20211015_2-base-myST-OGI.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/Renee_MyST_OGI/20211015_2-base-myST-OGI
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/Renee_MyST_OGI/20211015_2-base-myST-OGI_baseline_results.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/Renee_MyST_OGI/20211015_2-base-myST-OGI_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base-960h

------> PREPARING DATASET... ------------------------------------

Downloading and preparing dataset csv/default to /srv/scratch/chacmod/.cache/huggingface/datasets/myST-OGI-finetune/csv/default-cb3adfd9ad251ba8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 12576.62it/s]
Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00,  8.81it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00,  8.81it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 50000 examples [00:00, 335215.00 examples/s]Generating train split: 120000 examples [00:00, 436856.48 examples/s]                                                                     Traceback (most recent call last):
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 1882, in _prepare_split_single
    num_examples, num_bytes = writer.finalize()
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/arrow_writer.py", line 590, in finalize
    self.stream.close()
  File "/home/z5313567/.local/lib/python3.10/site-packages/fsspec/implementations/local.py", line 399, in close
    return self.f.close()
OSError: [Errno 122] Disk quota exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/srv/scratch/z5313567/thesis/wav2vec2/code/Renee_run_finetune_kids_myST-OGI.py", line 317, in <module>
    data = load_dataset('csv', 
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/load.py", line 1797, in load_dataset
    builder_instance.download_and_prepare(
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 890, in download_and_prepare
    self._download_and_prepare(
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 985, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 1746, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/builder.py", line 1891, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
Mon Jul 10 00:47:54 AEST 2023
