Sun Jun 18 11:30:41 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-579e7edb07087a41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
Started: 18/06/2023 11:30:42

------------ Importing libraries... ------------

pandas version: 2.0.1
json version: 2.0.9
librosa version: 0.10.0.post2
Numpy version: 1.23.1
Transformers version: 4.29.2
Torch version: 2.0.1+cu117
Test cuda_device_count 4
Test cuda_is_available True
Test get_device_name Tesla V100-SXM2-32GB

------------------ Model arguments... ------------------

ctc_loss_reduction: mean

------------------ Training arguments... ------------------

group_by_length: True
per_device_train_batch_size: 32
evaluation_strategy: steps
max_steps: 12000
fp16: True
gradient_checkpointing: True
save_steps: 500
eval_steps: 500
logging_steps: 500
learning_rate: 0.0001
weight_decay: 0.005
warmup_ratio: 0.025
save_total_limit: 2

------------------ Experiment arguments... ------------------

use_checkpoint: False
training: True

------------------ Loading files... ------------------

Training dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/10min_datasets/OGI_scripted_train_dataframe_10min.csv

Development dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/10min_datasets/OGI_scripted_dev_dataframe_10min.csv

Testing dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/10min_datasets/OGI_scripted_test_dataframe_10min.csv

Cache filepath is /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune

Model filepath is /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/10min/10min_model_OGI_American_20230618_2

Vocab filepath is /srv/scratch/z5313567/thesis/wav2vec2/vocab/OGI_American/10min/10min_vocab_OGI_American_20230618_2.json

Fine-tuned result filepath is /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/OGI_American/10min/10min_result_OGI_American_20230618_2.csv

Pretrained model is facebook/wav2vec2-base


------------------ Loading datasets... ------------------

  0%|          | 0/3 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 135.89it/s]
dataset is: DatasetDict({
    train: Dataset({
        features: ['filepath', 'duration', 'speaker_id', 'transcription'],
        num_rows: 170
    })
    dev: Dataset({
        features: ['filepath', 'duration', 'speaker_id', 'transcription'],
        num_rows: 34
    })
    test: Dataset({
        features: ['filepath', 'duration', 'speaker_id', 'transcription'],
        num_rows: 34
    })
})

------------------ Showing some random elements... ------------------

                                  transcription
0                pay albert for the fresh water
1                                       wiseguy
2                        because my leg is sore
3              fred didn't try the carrot juice
4                                      flagpole
5                                        hourly
6                                      cashflow
7  the herd became angry and started a stampede
8                                         cross
9          the flood took a car down the street

------------------ Extracting individual characters... ------------------

Map:   0%|          | 0/170 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/34 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/34 [00:00<?, ? examples/s]                                                  Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-579e7edb07087a41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f86c579529890ce4_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-579e7edb07087a41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-08f952530193341e_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-579e7edb07087a41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b04db53fc82ae91d_*_of_00004.arrow
vocabs is: DatasetDict({
    train: Dataset({
        features: ['vocab', 'all_text'],
        num_rows: 1
    })
    dev: Dataset({
        features: ['vocab', 'all_text'],
        num_rows: 1
    })
    test: Dataset({
        features: ['vocab', 'all_text'],
        num_rows: 1
    })
})
vocab_dict is: {'r': 0, 'j': 1, 'm': 2, 'y': 3, 'w': 4, 'h': 5, 'o': 6, 'k': 7, 'i': 8, 'a': 9, 'd': 10, 'n': 11, ' ': 12, "'": 13, 'f': 14, 'z': 15, 's': 16, 'p': 17, 'u': 18, 'e': 19, 'c': 20, 't': 21, 'b': 22, 'g': 23, 'l': 24, 'q': 25, 'v': 26, 'x': 27}
The lendth of vocab_list is 30
Successfully created vocab.json file at: /srv/scratch/z5313567/thesis/wav2vec2/vocab/OGI_American/10min/10min_vocab_OGI_American_20230618_2.json

------------------ Creating tokenizer... ------------------


------------------ Creating feature extractor... ------------------


------------------ Creating processor... ------------------


------------------ Obtaining speech arrays, sampling rates, and target texts... ------------------

dataset is: DatasetDict({
    train: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 170
    })
    dev: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 34
    })
    test: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 34
    })
})

------------------Ckech sampling rates of training datasets... ------------------

MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining

------------------Ckeck sampling rates of development datasets... ------------------

MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining

------------------Ckech sampling rates of testing datasets... ------------------

MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining

------------------ Verifying some ramdom samples... ------------------

Target text: squeegee
Input array shape: (31495,)
Sampling rate: 16000


Target text: cliffhanger
Input array shape: (37425,)
Sampling rate: 16000


Target text: hawthorne
Input array shape: (57462,)
Sampling rate: 16000


Target text: average
Input array shape: (44711,)
Sampling rate: 16000


Target text: the flood took a car down the street
Input array shape: (79483,)
Sampling rate: 16000


Target text: her veins pop out when she is mad
Input array shape: (97524,)
Sampling rate: 16000



------------------ Obtaining input values, input length, and labels... ------------------

Map (num_proc=4):   0%|          | 0/170 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):   4%|â–Ž         | 6/170 [00:00<00:03, 42.50 examples/s]Map (num_proc=4):  14%|â–ˆâ–Ž        | 23/170 [00:00<00:01, 104.58 examples/s]Map (num_proc=4):  25%|â–ˆâ–ˆâ–Œ       | 43/170 [00:00<00:00, 140.09 examples/s]Map (num_proc=4):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 68/170 [00:00<00:00, 173.50 examples/s]Map (num_proc=4):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 86/170 [00:00<00:00, 167.68 examples/s]Map (num_proc=4):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 108/170 [00:00<00:00, 168.34 examples/s]Map (num_proc=4):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 130/170 [00:00<00:00, 178.85 examples/s]Map (num_proc=4):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 151/170 [00:00<00:00, 162.30 examples/s]Map (num_proc=4):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 169/170 [00:01<00:00, 135.28 examples/s]                                                                           Map (num_proc=4):   0%|          | 0/34 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  24%|â–ˆâ–ˆâ–Ž       | 8/34 [00:00<00:00, 62.65 examples/s]Map (num_proc=4):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 28/34 [00:00<00:00, 124.96 examples/s]                                                                         Map (num_proc=4):   0%|          | 0/34 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  24%|â–ˆâ–ˆâ–Ž       | 8/34 [00:00<00:00, 60.39 examples/s]Map (num_proc=4):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 26/34 [00:00<00:00, 108.16 examples/s]                                                                         /srv/scratch/z5313567/thesis/wav2vec2/code/OGI_American/Finetune_Wav2vec2_American_10min_2.py:411: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.bias']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/z5313567/.local/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
dataset is: DatasetDict({
    train: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 170
    })
    dev: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 34
    })
    test: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 34
    })
})

------------------ Setting up the padding data collator... ------------------


------------------ Setting up WER metric... ------------------


------------------ Loading a pretrained checkpount... ------------------


------------------ Setting TrainingArguments... ------------------


------------------ Setting Trainer... ------------------


------------------ Starting training... ------------------

  0%|          | 0/12000 [00:00<?, ?it/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/12000 [00:11<38:28:05, 11.54s/it]  0%|          | 2/12000 [00:13<20:08:40,  6.04s/it]  0%|          | 3/12000 [00:18<18:24:33,  5.52s/it]  0%|          | 4/12000 [00:20<13:43:36,  4.12s/it]  0%|          | 5/12000 [00:25<14:41:50,  4.41s/it]  0%|          | 6/12000 [00:27<12:00:37,  3.60s/it]  0%|          | 7/12000 [00:32<13:25:44,  4.03s/it]  0%|          | 8/12000 [00:34<11:21:22,  3.41s/it]  0%|          | 9/12000 [00:39<12:55:59,  3.88s/it]  0%|          | 10/12000 [00:41<11:00:22,  3.30s/it]  0%|          | 11/12000 [00:46<12:38:31,  3.80s/it]  0%|          | 12/12000 [00:48<10:50:35,  3.26s/it]  0%|          | 13/12000 [00:53<12:35:23,  3.78s/it]  0%|          | 14/12000 [00:55<10:49:29,  3.25s/it]  0%|          | 15/12000 [01:00<12:37:27,  3.79s/it]  0%|          | 16/12000 [01:02<10:49:20,  3.25s/it]  0%|          | 17/12000 [01:07<12:29:46,  3.75s/it]  0%|          | 18/12000 [01:09<10:45:27,  3.23s/it]  0%|          | 19/12000 [01:14<12:21:48,  3.71s/it]  0%|          | 20/12000 [01:16<10:44:23,  3.23s/it]  0%|          | 21/12000 [01:21<12:33:41,  3.78s/it]  0%|          | 22/12000 [01:23<10:48:35,  3.25s/it]  0%|          | 23/12000 [01:28<12:22:44,  3.72s/it]  0%|          | 24/12000 [01:30<10:44:58,  3.23s/it]  0%|          | 25/12000 [01:35<12:20:51,  3.71s/it]  0%|          | 26/12000 [01:37<10:48:39,  3.25s/it]  0%|          | 27/12000 [01:42<12:32:55,  3.77s/it]  0%|          | 28/12000 [01:44<10:43:17,  3.22s/it]  0%|          | 29/12000 [01:48<11:58:17,  3.60s/it]  0%|          | 30/12000 [01:50<10:12:49,  3.07s/it]  0%|          | 31/12000 [01:55<12:11:14,  3.67s/it]  0%|          | 32/12000 [01:57<10:24:30,  3.13s/it]  0%|          | 33/12000 [02:02<12:20:43,  3.71s/it]  0%|          | 34/12000 [02:04<10:37:38,  3.20s/it]  0%|          | 35/12000 [02:09<12:17:25,  3.70s/it]  0%|          | 36/12000 [02:11<10:27:34,  3.15s/it]  0%|          | 37/12000 [02:15<11:44:58,  3.54s/it]  0%|          | 38/12000 [02:17<10:02:43,  3.02s/it]  0%|          | 39/12000 [02:21<11:18:38,  3.40s/it]  0%|          | 40/12000 [02:23<9:51:36,  2.97s/it]   0%|          | 41/12000 [02:28<11:18:00,  3.40s/it]  0%|          | 42/12000 [02:30<10:01:50,  3.02s/it]  0%|          | 43/12000 [02:35<11:45:30,  3.54s/it]  0%|          | 44/12000 [02:37<10:20:17,  3.11s/it]  0%|          | 45/12000 [02:42<12:10:31,  3.67s/it]  0%|          | 46/12000 [02:44<10:34:44,  3.19s/it]  0%|          | 47/12000 [02:49<12:15:21,  3.69s/it]  0%|          | 48/12000 [02:51<10:36:11,  3.19s/it]  0%|          | 49/12000 [02:56<12:20:00,  3.72s/it]  0%|          | 50/12000 [02:58<10:46:33,  3.25s/it]  0%|          | 51/12000 [03:02<11:52:17,  3.58s/it]  0%|          | 52/12000 [03:04<10:10:50,  3.07s/it]  0%|          | 53/12000 [03:08<11:29:15,  3.46s/it]  0%|          | 54/12000 [03:10<9:58:08,  3.00s/it]   0%|          | 55/12000 [03:15<11:22:19,  3.43s/it]  0%|          | 56/12000 [03:16<9:42:26,  2.93s/it]   0%|          | 57/12000 [03:21<11:07:25,  3.35s/it]  0%|          | 58/12000 [03:23<9:48:32,  2.96s/it]   0%|          | 59/12000 [03:27<11:28:08,  3.46s/it]  0%|          | 60/12000 [03:29<9:50:10,  2.97s/it] 