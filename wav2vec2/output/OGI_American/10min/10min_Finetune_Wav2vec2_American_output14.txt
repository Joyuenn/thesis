Sun Jun 18 11:30:32 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-579e7edb07087a41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
Started: 18/06/2023 11:30:33

------------ Importing libraries... ------------

pandas version: 2.0.1
json version: 2.0.9
librosa version: 0.10.0.post2
Numpy version: 1.23.1
Transformers version: 4.29.2
Torch version: 2.0.1+cu117
Test cuda_device_count 4
Test cuda_is_available True
Test get_device_name Tesla V100-SXM2-32GB

------------------ Model arguments... ------------------

ctc_loss_reduction: mean

------------------ Training arguments... ------------------

group_by_length: True
per_device_train_batch_size: 32
evaluation_strategy: steps
max_steps: 12000
fp16: True
gradient_checkpointing: True
save_steps: 500
eval_steps: 500
logging_steps: 500
learning_rate: 5e-06
weight_decay: 0.005
warmup_ratio: 0.1
save_total_limit: 2

------------------ Experiment arguments... ------------------

use_checkpoint: False
training: True

------------------ Loading files... ------------------

Training dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/10min_datasets/OGI_scripted_train_dataframe_10min.csv

Development dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/10min_datasets/OGI_scripted_dev_dataframe_10min.csv

Testing dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/10min_datasets/OGI_scripted_test_dataframe_10min.csv

Cache filepath is /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune

Model filepath is /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/10min/10min_model_OGI_American_20230618

Vocab filepath is /srv/scratch/z5313567/thesis/wav2vec2/vocab/OGI_American/10min/10min_vocab_OGI_American_20230618.json

Fine-tuned result filepath is /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/OGI_American/10min/10min_result_OGI_American_20230618.csv

Pretrained model is facebook/wav2vec2-base


------------------ Loading datasets... ------------------

  0%|          | 0/3 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 299.96it/s]
dataset is: DatasetDict({
    train: Dataset({
        features: ['filepath', 'duration', 'speaker_id', 'transcription'],
        num_rows: 170
    })
    dev: Dataset({
        features: ['filepath', 'duration', 'speaker_id', 'transcription'],
        num_rows: 34
    })
    test: Dataset({
        features: ['filepath', 'duration', 'speaker_id', 'transcription'],
        num_rows: 34
    })
})

------------------ Showing some random elements... ------------------

                          transcription
0                              sheepdog
1                                trauma
2      we should try to save the whales
3  the angry chef cooked his vegetables
4                                  hoof
5     san diego is very green this year
6                            background
7     turn the volume on the radio down
8                                  push
9         who will the soldier vote for

------------------ Extracting individual characters... ------------------

Map:   0%|          | 0/170 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/34 [00:00<?, ? examples/s]                                                  Map:   0%|          | 0/34 [00:00<?, ? examples/s]                                                  Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-579e7edb07087a41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f69cebe2b96ba2f4_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-579e7edb07087a41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-d0c99669b58ef250_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-579e7edb07087a41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ed7bdb144eefc063_*_of_00004.arrow
vocabs is: DatasetDict({
    train: Dataset({
        features: ['vocab', 'all_text'],
        num_rows: 1
    })
    dev: Dataset({
        features: ['vocab', 'all_text'],
        num_rows: 1
    })
    test: Dataset({
        features: ['vocab', 'all_text'],
        num_rows: 1
    })
})
vocab_dict is: {'m': 0, 'n': 1, 'u': 2, 'w': 3, 'y': 4, 'j': 5, ' ': 6, 'r': 7, 'z': 8, 'p': 9, 'c': 10, 'd': 11, 'f': 12, 'g': 13, 'o': 14, 's': 15, 'l': 16, 'q': 17, 'v': 18, 'b': 19, 'e': 20, 'a': 21, 'x': 22, 'k': 23, 'h': 24, "'": 25, 'i': 26, 't': 27}
The lendth of vocab_list is 30
Successfully created vocab.json file at: /srv/scratch/z5313567/thesis/wav2vec2/vocab/OGI_American/10min/10min_vocab_OGI_American_20230618.json

------------------ Creating tokenizer... ------------------


------------------ Creating feature extractor... ------------------


------------------ Creating processor... ------------------


------------------ Obtaining speech arrays, sampling rates, and target texts... ------------------

dataset is: DatasetDict({
    train: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 170
    })
    dev: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 34
    })
    test: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 34
    })
})

------------------Ckech sampling rates of training datasets... ------------------

MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining

------------------Ckeck sampling rates of development datasets... ------------------

MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining

------------------Ckech sampling rates of testing datasets... ------------------

MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining

------------------ Verifying some ramdom samples... ------------------

Target text: hourly
Input array shape: (41858,)
Sampling rate: 16000


Target text: san diego is very green this year
Input array shape: (97523,)
Sampling rate: 16000


Target text: spoons
Input array shape: (50496,)
Sampling rate: 16000


Target text: explosion
Input array shape: (25405,)
Sampling rate: 16000


Target text: flagpole
Input array shape: (50477,)
Sampling rate: 16000


Target text: knights don't get scared of dragons
Input array shape: (85504,)
Sampling rate: 16000



------------------ Obtaining input values, input length, and labels... ------------------

Map (num_proc=4):   0%|          | 0/170 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):   4%|â–Ž         | 6/170 [00:00<00:04, 39.68 examples/s]Map (num_proc=4):  14%|â–ˆâ–Ž        | 23/170 [00:00<00:01, 101.04 examples/s]Map (num_proc=4):  25%|â–ˆâ–ˆâ–Œ       | 43/170 [00:00<00:00, 135.25 examples/s]Map (num_proc=4):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 67/170 [00:00<00:00, 163.50 examples/s]Map (num_proc=4):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 86/170 [00:00<00:00, 167.76 examples/s]Map (num_proc=4):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 106/170 [00:00<00:00, 176.82 examples/s]Map (num_proc=4):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 127/170 [00:00<00:00, 185.23 examples/s]Map (num_proc=4):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 146/170 [00:00<00:00, 172.79 examples/s]Map (num_proc=4):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 166/170 [00:01<00:00, 138.16 examples/s]                                                                           Map (num_proc=4):   0%|          | 0/34 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  18%|â–ˆâ–Š        | 6/34 [00:00<00:00, 48.60 examples/s]Map (num_proc=4):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 27/34 [00:00<00:00, 119.47 examples/s]                                                                         Map (num_proc=4):   0%|          | 0/34 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  21%|â–ˆâ–ˆ        | 7/34 [00:00<00:00, 52.68 examples/s]Map (num_proc=4):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25/34 [00:00<00:00, 106.68 examples/s]                                                                         /srv/scratch/z5313567/thesis/wav2vec2/code/OGI_American/Finetune_Wav2vec2_American_10min.py:411: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_q.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/z5313567/.local/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
dataset is: DatasetDict({
    train: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 170
    })
    dev: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 34
    })
    test: Dataset({
        features: ['speech', 'sampling_rate', 'target_text'],
        num_rows: 34
    })
})

------------------ Setting up the padding data collator... ------------------


------------------ Setting up WER metric... ------------------


------------------ Loading a pretrained checkpount... ------------------


------------------ Setting TrainingArguments... ------------------


------------------ Setting Trainer... ------------------


------------------ Starting training... ------------------

  0%|          | 0/12000 [00:00<?, ?it/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/12000 [00:12<42:09:20, 12.65s/it]  0%|          | 2/12000 [00:14<21:47:44,  6.54s/it]  0%|          | 3/12000 [00:19<19:30:11,  5.85s/it]  0%|          | 4/12000 [00:22<14:30:36,  4.35s/it]  0%|          | 5/12000 [00:27<15:21:14,  4.61s/it]  0%|          | 6/12000 [00:28<12:17:04,  3.69s/it]  0%|          | 7/12000 [00:33<13:36:32,  4.09s/it]  0%|          | 8/12000 [00:36<11:33:40,  3.47s/it]  0%|          | 9/12000 [00:41<13:21:59,  4.01s/it]  0%|          | 10/12000 [00:43<11:15:35,  3.38s/it]  0%|          | 11/12000 [00:48<12:46:16,  3.83s/it]  0%|          | 12/12000 [00:50<10:57:35,  3.29s/it]  0%|          | 13/12000 [00:55<12:48:20,  3.85s/it]  0%|          | 14/12000 [00:57<10:45:46,  3.23s/it]  0%|          | 15/12000 [01:01<12:06:54,  3.64s/it]  0%|          | 16/12000 [01:03<10:24:37,  3.13s/it]  0%|          | 17/12000 [01:08<11:59:39,  3.60s/it]  0%|          | 18/12000 [01:10<10:16:33,  3.09s/it]  0%|          | 19/12000 [01:14<11:30:41,  3.46s/it]  0%|          | 20/12000 [01:16<10:03:12,  3.02s/it]  0%|          | 21/12000 [01:21<11:49:06,  3.55s/it]  0%|          | 22/12000 [01:23<10:05:36,  3.03s/it]  0%|          | 23/12000 [01:27<11:31:10,  3.46s/it]  0%|          | 24/12000 [01:29<10:02:20,  3.02s/it]  0%|          | 25/12000 [01:34<11:34:54,  3.48s/it]  0%|          | 26/12000 [01:36<10:05:58,  3.04s/it]  0%|          | 27/12000 [01:40<11:38:10,  3.50s/it]  0%|          | 28/12000 [01:42<10:01:09,  3.01s/it]  0%|          | 29/12000 [01:47<11:43:17,  3.52s/it]  0%|          | 30/12000 [01:49<10:01:49,  3.02s/it]  0%|          | 31/12000 [01:53<11:39:56,  3.51s/it]  0%|          | 32/12000 [01:55<10:00:27,  3.01s/it]  0%|          | 33/12000 [02:00<11:42:15,  3.52s/it]  0%|          | 34/12000 [02:02<10:01:39,  3.02s/it]  0%|          | 35/12000 [02:06<11:26:54,  3.44s/it]  0%|          | 36/12000 [02:08<10:00:19,  3.01s/it]  0%|          | 37/12000 [02:13<11:40:14,  3.51s/it]  0%|          | 38/12000 [02:15<9:57:29,  3.00s/it]   0%|          | 39/12000 [02:19<11:19:54,  3.41s/it]  0%|          | 40/12000 [02:21<9:57:32,  3.00s/it]   0%|          | 41/12000 [02:26<11:32:17,  3.47s/it]  0%|          | 42/12000 [02:28<10:01:36,  3.02s/it]  0%|          | 43/12000 [02:32<11:22:44,  3.43s/it]  0%|          | 44/12000 [02:34<9:59:47,  3.01s/it]   0%|          | 45/12000 [02:39<11:37:44,  3.50s/it]  0%|          | 46/12000 [02:40<10:01:44,  3.02s/it]  0%|          | 47/12000 [02:45<11:28:38,  3.46s/it]  0%|          | 48/12000 [02:47<9:59:56,  3.01s/it]   0%|          | 49/12000 [02:52<11:35:06,  3.49s/it]  0%|          | 50/12000 [02:53<10:00:53,  3.02s/it]  0%|          | 51/12000 [02:58<11:33:26,  3.48s/it]  0%|          | 52/12000 [03:00<10:00:39,  3.02s/it]  0%|          | 53/12000 [03:05<11:33:44,  3.48s/it]  0%|          | 54/12000 [03:06<10:00:55,  3.02s/it]  0%|          | 55/12000 [03:11<11:35:27,  3.49s/it]  0%|          | 56/12000 [03:13<9:57:56,  3.00s/it]   0%|          | 57/12000 [03:17<11:28:20,  3.46s/it]  0%|          | 58/12000 [03:19<9:58:45,  3.01s/it] 