Thu Jul 13 10:17:30 AEST 2023
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_OGI.py
Started: 13/07/2023 10:17:30

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: OGI_American
experiment_id: eval_202307013
cache_name: OGI-eval
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/Renee_myST_OGI_TLT/20211016_2-base-myST-OGI-TLT17
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base-960h
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/Renee_myST_OGI_TLT/20211016_2-base-myST-OGI-TLT17
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/OGI_local/OGI_spontaneous_test_dataframe.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/OGI_local/OGI_spontaneous_test_dataframe.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/OGI-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/OGI_American/eval_202307013_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/eval_202307013
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/OGI_American/eval_202307013_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/OGI_American/eval_202307013_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/Renee_myST_OGI_TLT/20211016_2-base-myST-OGI-TLT17
--> pretrained_tokenizer: facebook/wav2vec2-base-960h

------> PREPARING DATASET... ------------------------------------

Downloading and preparing dataset csv/default to /srv/scratch/chacmod/.cache/huggingface/datasets/OGI-eval/csv/default-d640b7a8e73afa90/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 12885.73it/s]
Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 60.36it/s]
Generating train split: 0 examples [00:00, ? examples/s]                                                        Generating test split: 0 examples [00:00, ? examples/s]                                                       Dataset csv downloaded and prepared to /srv/scratch/chacmod/.cache/huggingface/datasets/OGI-eval/csv/default-d640b7a8e73afa90/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 126.59it/s]
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription', 'duration', 'speaker_id'],
        num_rows: 166
    })
    test: Dataset({
        features: ['filepath', 'transcription', 'duration', 'speaker_id'],
        num_rows: 166
    })
})
--> Printing some random samples...
                                            filepath  ... speaker_id
0  /srv/scratch/chacmod/OGI/speech/spontaneous/08...  ...      ks80r
1  /srv/scratch/chacmod/OGI/speech/spontaneous/01...  ...      ks10e
2  /srv/scratch/chacmod/OGI/speech/spontaneous/03...  ...      kse2s
3  /srv/scratch/chacmod/OGI/speech/spontaneous/05...  ...      ksg34
4  /srv/scratch/chacmod/OGI/speech/spontaneous/09...  ...      ksk0o

[5 rows x 4 columns]
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------

Map:   0%|          | 0/166 [00:00<?, ? examples/s]                                                   Traceback (most recent call last):
  File "/srv/scratch/z5313567/thesis/wav2vec2/code/eval_OGI.py", line 404, in <module>
    data = data.map(process_transcription)
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/dataset_dict.py", line 851, in map
    {
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/dataset_dict.py", line 852, in <dictcomp>
    k: dataset.map(
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 578, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 543, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3073, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3427, in _map_single
    example = apply_function_on_filtered_inputs(example, i, offset=offset)
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3330, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/srv/scratch/z5313567/thesis/wav2vec2/code/eval_OGI.py", line 400, in process_transcription
    batch["transcription_clean"] = batch["transcription_clean"].upper()
  File "/home/z5313567/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 270, in __getitem__
    value = self.data[key]
KeyError: 'transcription_clean'
