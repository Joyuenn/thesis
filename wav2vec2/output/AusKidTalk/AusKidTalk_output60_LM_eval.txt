Wed Aug 30 22:51:09 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py
Started: 30/08/2023 22:51:10

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: AusKidTalk
experiment_id: eval_AusKidTalk_spontaneous_20230830
cache_name: AusKidTalk-eval
Language model: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829_with_lm_AusKidTalk_LM_combined
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
use_pretrained_tokenizer: True
pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/full/full_model_OGI_American_20230702
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/AusKidTalk/eval_AusKidTalk_spontaneous_20230830
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
--> pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/full/full_model_OGI_American_20230702

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 308.01it/s]
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                 there wasn't any girl in the video
1  /srv/scratch/chacmod/auskidtalk_spontaneous/22...       then he listened to the egg when he fell off
2  /srv/scratch/chacmod/auskidtalk_spontaneous/22...                           he can't find his friend
3  /srv/scratch/chacmod/auskidtalk_spontaneous/65...                          he's hiding behind a tree
4  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                           he was on the skateboard
5  /srv/scratch/chacmod/auskidtalk_spontaneous/10...  the green skined baby is riding a skateboard h...
6  /srv/scratch/chacmod/auskidtalk_spontaneous/81...  the dinosaur tries to play with hulk but hulk ...
7  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                                 he's skateboarding
8  /srv/scratch/chacmod/auskidtalk_spontaneous/11...     and the baby dinosaur didn't know where he was
9  /srv/scratch/chacmod/auskidtalk_spontaneous/11...                            he put his ear up to it
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------

Map:   0%|          | 0/111 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/111 [00:00<?, ? examples/s]                                                   Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]Map (num_proc=4):   1%|          | 1/111 [00:02<04:13,  2.30s/ examples]Map (num_proc=4):  16%|█▌        | 18/111 [00:02<00:09, 10.30 examples/s]Map (num_proc=4):  34%|███▍      | 38/111 [00:02<00:02, 24.75 examples/s]Map (num_proc=4):  53%|█████▎    | 59/111 [00:02<00:01, 41.80 examples/s]Map (num_proc=4):  72%|███████▏  | 80/111 [00:02<00:00, 61.03 examples/s]Map (num_proc=4):  89%|████████▉ | 99/111 [00:02<00:00, 74.31 examples/s]                                                                         Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]Map (num_proc=4):   1%|          | 1/111 [00:01<02:06,  1.15s/ examples]Map (num_proc=4):  69%|██████▉   | 77/111 [00:01<00:00, 83.79 examples/s]                                                                         --> Verifying data with a random sample...
Target text: and then he was sad
Input array shape: (21737,)
Sampling rate: 16000
Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
Map (num_proc=4):   7%|▋         | 8/111 [00:00<00:02, 44.50 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|███▌      | 40/111 [00:00<00:00, 141.83 examples/s]Map (num_proc=4):  58%|█████▊    | 64/111 [00:00<00:00, 158.42 examples/s]Map (num_proc=4):  76%|███████▌  | 84/111 [00:00<00:00, 158.27 examples/s]Map (num_proc=4):  96%|█████████▋| 107/111 [00:00<00:00, 131.13 examples/s]                                                                           Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):   7%|▋         | 8/111 [00:00<00:02, 45.09 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|███▌      | 40/111 [00:00<00:00, 147.91 examples/s]Map (num_proc=4):  58%|█████▊    | 64/111 [00:00<00:00, 164.56 examples/s]Map (num_proc=4):  76%|███████▌  | 84/111 [00:00<00:00, 156.29 examples/s]Map (num_proc=4):  96%|█████████▋| 107/111 [00:00<00:00, 124.92 examples/s]                                                                           /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py:607: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.

------> EVALUATING MODEL... ------------------------------------------ 

Map:   0%|          | 0/111 [00:00<?, ? examples/s]Map:   1%|          | 1/111 [00:02<03:41,  2.01s/ examples]Map:   2%|▏         | 2/111 [00:02<02:18,  1.27s/ examples]Map:   3%|▎         | 3/111 [00:03<01:51,  1.04s/ examples]Map:   4%|▎         | 4/111 [00:04<01:38,  1.09 examples/s]Map:   5%|▍         | 5/111 [00:05<01:31,  1.16 examples/s]Map:   5%|▌         | 6/111 [00:05<01:27,  1.20 examples/s]Map:   6%|▋         | 7/111 [00:06<01:23,  1.25 examples/s]Map:   7%|▋         | 8/111 [00:07<01:20,  1.29 examples/s]Map:   8%|▊         | 9/111 [00:08<01:18,  1.30 examples/s]Map:   9%|▉         | 10/111 [00:08<01:16,  1.31 examples/s]Map:  10%|▉         | 11/111 [00:09<01:15,  1.33 examples/s]Map:  11%|█         | 12/111 [00:10<01:15,  1.32 examples/s]Map:  12%|█▏        | 13/111 [00:11<01:16,  1.28 examples/s]Map:  13%|█▎        | 14/111 [00:11<01:15,  1.28 examples/s]Map:  14%|█▎        | 15/111 [00:12<01:15,  1.28 examples/s]Map:  14%|█▍        | 16/111 [00:13<01:14,  1.27 examples/s]Map:  15%|█▌        | 17/111 [00:14<01:15,  1.25 examples/s]Map:  16%|█▌        | 18/111 [00:15<01:13,  1.26 examples/s]Map:  17%|█▋        | 19/111 [00:15<01:12,  1.27 examples/s]Map:  18%|█▊        | 20/111 [00:16<01:10,  1.29 examples/s]Map:  19%|█▉        | 21/111 [00:17<01:09,  1.29 examples/s]Map:  20%|█▉        | 22/111 [00:18<01:08,  1.30 examples/s]Map:  21%|██        | 23/111 [00:18<01:09,  1.27 examples/s]Map:  22%|██▏       | 24/111 [00:19<01:10,  1.24 examples/s]Map:  23%|██▎       | 25/111 [00:20<01:08,  1.25 examples/s]Map:  23%|██▎       | 26/111 [00:21<01:07,  1.25 examples/s]Map:  24%|██▍       | 27/111 [00:22<01:06,  1.27 examples/s]Map:  25%|██▌       | 28/111 [00:22<01:04,  1.28 examples/s]Map:  26%|██▌       | 29/111 [00:23<01:03,  1.28 examples/s]Map:  27%|██▋       | 30/111 [00:24<01:02,  1.29 examples/s]Map:  28%|██▊       | 31/111 [00:25<01:02,  1.29 examples/s]Map:  29%|██▉       | 32/111 [00:26<01:01,  1.28 examples/s]Map:  30%|██▉       | 33/111 [00:26<01:00,  1.29 examples/s]Map:  31%|███       | 34/111 [00:27<01:00,  1.27 examples/s]Map:  32%|███▏      | 35/111 [00:28<01:00,  1.27 examples/s]Map:  32%|███▏      | 36/111 [00:29<00:59,  1.26 examples/s]Map:  33%|███▎      | 37/111 [00:29<00:58,  1.26 examples/s]Map:  34%|███▍      | 38/111 [00:30<00:57,  1.27 examples/s]Map:  35%|███▌      | 39/111 [00:31<00:56,  1.27 examples/s]Map:  36%|███▌      | 40/111 [00:32<00:55,  1.27 examples/s]Map:  37%|███▋      | 41/111 [00:33<00:55,  1.26 examples/s]Map:  38%|███▊      | 42/111 [00:33<00:54,  1.26 examples/s]Map:  39%|███▊      | 43/111 [00:34<00:54,  1.26 examples/s]Map:  40%|███▉      | 44/111 [00:35<00:54,  1.23 examples/s]Map:  41%|████      | 45/111 [00:36<00:53,  1.23 examples/s]Map:  41%|████▏     | 46/111 [00:37<00:52,  1.24 examples/s]Map:  42%|████▏     | 47/111 [00:38<00:51,  1.24 examples/s]Map:  43%|████▎     | 48/111 [00:38<00:50,  1.25 examples/s]Map:  44%|████▍     | 49/111 [00:39<00:50,  1.23 examples/s]Map:  45%|████▌     | 50/111 [00:40<00:49,  1.24 examples/s]Map:  46%|████▌     | 51/111 [00:41<00:48,  1.24 examples/s]Map:  47%|████▋     | 52/111 [00:42<00:47,  1.23 examples/s]Map:  48%|████▊     | 53/111 [00:42<00:46,  1.23 examples/s]Map:  49%|████▊     | 54/111 [00:43<00:46,  1.23 examples/s]Map:  50%|████▉     | 55/111 [00:44<00:46,  1.21 examples/s]Map:  50%|█████     | 56/111 [00:45<00:44,  1.22 examples/s]Map:  51%|█████▏    | 57/111 [00:46<00:44,  1.22 examples/s]Map:  52%|█████▏    | 58/111 [00:46<00:43,  1.22 examples/s]Map:  53%|█████▎    | 59/111 [00:47<00:42,  1.24 examples/s]Map:  54%|█████▍    | 60/111 [00:48<00:41,  1.22 examples/s]Map:  55%|█████▍    | 61/111 [00:49<00:41,  1.22 examples/s]Map:  56%|█████▌    | 62/111 [00:50<00:40,  1.22 examples/s]Map:  57%|█████▋    | 63/111 [00:51<00:39,  1.21 examples/s]Map:  58%|█████▊    | 64/111 [00:51<00:39,  1.18 examples/s]Map:  59%|█████▊    | 65/111 [00:52<00:38,  1.19 examples/s]Map:  59%|█████▉    | 66/111 [00:53<00:37,  1.19 examples/s]Map:  60%|██████    | 67/111 [00:54<00:37,  1.18 examples/s]Map:  61%|██████▏   | 68/111 [00:55<00:36,  1.19 examples/s]Map:  62%|██████▏   | 69/111 [00:56<00:35,  1.19 examples/s]Map:  63%|██████▎   | 70/111 [00:57<00:34,  1.19 examples/s]Map:  64%|██████▍   | 71/111 [00:57<00:33,  1.19 examples/s]Map:  65%|██████▍   | 72/111 [00:58<00:32,  1.21 examples/s]Map:  66%|██████▌   | 73/111 [00:59<00:31,  1.19 examples/s]Map:  67%|██████▋   | 74/111 [01:00<00:31,  1.18 examples/s]Map:  68%|██████▊   | 75/111 [01:01<00:30,  1.17 examples/s]Map:  68%|██████▊   | 76/111 [01:02<00:30,  1.16 examples/s]Map:  69%|██████▉   | 77/111 [01:02<00:29,  1.17 examples/s]Map:  70%|███████   | 78/111 [01:03<00:27,  1.18 examples/s]Map:  71%|███████   | 79/111 [01:04<00:27,  1.16 examples/s]Map:  72%|███████▏  | 80/111 [01:05<00:26,  1.16 examples/s]Map:  73%|███████▎  | 81/111 [01:06<00:26,  1.15 examples/s]Map:  74%|███████▍  | 82/111 [01:07<00:24,  1.16 examples/s]Map:  75%|███████▍  | 83/111 [01:08<00:23,  1.17 examples/s]Map:  76%|███████▌  | 84/111 [01:09<00:23,  1.16 examples/s]Map:  77%|███████▋  | 85/111 [01:09<00:22,  1.17 examples/s]Map:  77%|███████▋  | 86/111 [01:10<00:21,  1.17 examples/s]Map:  78%|███████▊  | 87/111 [01:11<00:20,  1.18 examples/s]Map:  79%|███████▉  | 88/111 [01:12<00:20,  1.14 examples/s]Map:  80%|████████  | 89/111 [01:13<00:19,  1.15 examples/s]Map:  81%|████████  | 90/111 [01:14<00:18,  1.16 examples/s]Map:  82%|████████▏ | 91/111 [01:15<00:17,  1.16 examples/s]Map:  83%|████████▎ | 92/111 [01:15<00:16,  1.15 examples/s]Map:  84%|████████▍ | 93/111 [01:16<00:15,  1.15 examples/s]Map:  85%|████████▍ | 94/111 [01:17<00:14,  1.14 examples/s]Map:  86%|████████▌ | 95/111 [01:18<00:14,  1.14 examples/s]Map:  86%|████████▋ | 96/111 [01:19<00:13,  1.13 examples/s]Map:  87%|████████▋ | 97/111 [01:20<00:12,  1.15 examples/s]Map:  88%|████████▊ | 98/111 [01:21<00:11,  1.13 examples/s]Map:  89%|████████▉ | 99/111 [01:22<00:10,  1.13 examples/s]Map:  90%|█████████ | 100/111 [01:22<00:09,  1.14 examples/s]Map:  91%|█████████ | 101/111 [01:23<00:08,  1.15 examples/s]Map:  92%|█████████▏| 102/111 [01:24<00:07,  1.14 examples/s]Map:  93%|█████████▎| 103/111 [01:25<00:07,  1.13 examples/s]Map:  94%|█████████▎| 104/111 [01:26<00:06,  1.10 examples/s]Map:  95%|█████████▍| 105/111 [01:27<00:05,  1.09 examples/s]Map:  95%|█████████▌| 106/111 [01:28<00:04,  1.08 examples/s]Map:  96%|█████████▋| 107/111 [01:29<00:03,  1.08 examples/s]Map:  97%|█████████▋| 108/111 [01:30<00:02,  1.04 examples/s]Map:  98%|█████████▊| 109/111 [01:31<00:01,  1.05 examples/s]Map:  99%|█████████▉| 110/111 [01:32<00:00,  1.06 examples/s]Map: 100%|██████████| 111/111 [01:33<00:00,  1.06 examples/s]                                                             Saved results to: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_finetuned_result.csv
--> Getting fine-tuned test results...
Fine-tuned Test WER Without Language Model: 0.248
Fine-tuned Test WER With Language Model: 0.155


Fine-tuned Test CER Without Language Model: 0.104
Fine-tuned Test CER With Language Model: 0.080


--> Showing some fine-tuned prediction errors...
                                         target_text  ...                                   pred_str_with_LM
0                        and then the boy felt sorry  ...                        and then the boy felt sorry
1  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bull of milk whe...
2            a dinosaur jumps out and hulk is scared  ...           a dinosaur jumps out and chalk is scared
3                    so he ran and hid behind a tree  ...                    so he ran and hid behind a tree
4                                  the baby feel sad  ...                                  the baby feel sad
5               he was sad that there was a dinosaur  ...               he was sad that there was a dinosaur
6                     he hears what's inside the egg  ...                     he hears what's inside the egg
7  the green baby is holding the bottle of milk a...  ...  the green baby is holding the bull of milk and...
8  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bull of milk we ...
9                  because the dinosaur looked funny  ...                  because the dinosaur looked funny

[10 rows x 3 columns]
--> Taking a deeper look...
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] h [PAD] [PAD] [PAD] [PAD] [PAD] e a l [PAD] k [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] w w [PAD] a [PAD] s [PAD] | | [PAD] h [PAD] [PAD] [PAD] [PAD] [PAD] i [PAD] d d d i n n g [PAD] | | [PAD] [PAD] [PAD] b [PAD] [PAD] [PAD] a c c k [PAD] [PAD] [PAD] w [PAD] a r r d [PAD] s | [PAD] [PAD] o n [PAD] | | t h h e | | s s [PAD] [PAD] c [PAD] [PAD] [PAD] a y y [PAD] [PAD] [PAD] [PAD] [PAD] b [PAD] [PAD] [PAD] [PAD] o a r r [PAD] d [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] a a n d | | h [PAD] e [PAD] [PAD] | [PAD] h h [PAD] [PAD] i t t [PAD] [PAD] [PAD] [PAD] [PAD] s s [PAD] [PAD] | [PAD] [PAD] [PAD] a n d | | [PAD] [PAD] [PAD] [PAD] e [PAD] g g [PAD] g [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 30/08/2023 22:53:13
Wed Aug 30 23:18:48 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py
Started: 30/08/2023 23:18:48

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: AusKidTalk
experiment_id: eval_AusKidTalk_spontaneous_20230830
cache_name: AusKidTalk-eval
Language model: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829_with_lm_AusKidTalk_LM_combined
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
use_pretrained_tokenizer: True
pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/AusKidTalk/eval_AusKidTalk_spontaneous_20230830
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
--> pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 170.56it/s]
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fadff2182aaf1453.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-84e4ca2fd5125a54.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a8aa6bdffbb05b00_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b17bf95f4683362b_*_of_00004.arrow
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                                 he's skateboarding
1  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                    the dinosaur poked out his head
2  /srv/scratch/chacmod/auskidtalk_spontaneous/65...                  the boy said that he made him sad
3  /srv/scratch/chacmod/auskidtalk_spontaneous/81...                                the dinosaur is sad
4  /srv/scratch/chacmod/auskidtalk_spontaneous/10...   the green dinosaur is looking for the green baby
5  /srv/scratch/chacmod/auskidtalk_spontaneous/22...       then he listened to the egg when he fell off
6  /srv/scratch/chacmod/auskidtalk_spontaneous/10...  the green baby is holding the bottle of milk w...
7  /srv/scratch/chacmod/auskidtalk_spontaneous/10...  the green baby is laughing while the green din...
8  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                               there was a dinosaur
9  /srv/scratch/chacmod/auskidtalk_spontaneous/10...                       the boy snare a dinosaur egg
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

--> Verifying data with a random sample...
Target text: the boy said that he made him sad
Input array shape: (64927,)
Sampling rate: 16000
Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):   7%|▋         | 8/111 [00:00<00:02, 42.10 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|███▌      | 40/111 [00:00<00:00, 147.20 examples/s]Map (num_proc=4):  58%|█████▊    | 64/111 [00:00<00:00, 169.52 examples/s]Map (num_proc=4):  76%|███████▌  | 84/111 [00:00<00:00, 156.94 examples/s]Map (num_proc=4):  96%|█████████▋| 107/111 [00:00<00:00, 128.86 examples/s]                                                                           Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):   7%|▋         | 8/111 [00:00<00:02, 44.21 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|███▌      | 40/111 [00:00<00:00, 143.31 examples/s]Map (num_proc=4):  58%|█████▊    | 64/111 [00:00<00:00, 163.46 examples/s]Map (num_proc=4):  76%|███████▌  | 84/111 [00:00<00:00, 158.52 examples/s]Map (num_proc=4):  96%|█████████▋| 107/111 [00:00<00:00, 123.88 examples/s]                                                                           /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py:607: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.

------> EVALUATING MODEL... ------------------------------------------ 

Map:   0%|          | 0/111 [00:00<?, ? examples/s]Map:   1%|          | 1/111 [00:02<03:46,  2.06s/ examples]Map:   2%|▏         | 2/111 [00:02<02:24,  1.32s/ examples]Map:   3%|▎         | 3/111 [00:03<01:57,  1.09s/ examples]Map:   4%|▎         | 4/111 [00:04<01:43,  1.03 examples/s]Map:   5%|▍         | 5/111 [00:05<01:37,  1.09 examples/s]Map:   5%|▌         | 6/111 [00:06<01:32,  1.13 examples/s]Map:   6%|▋         | 7/111 [00:06<01:28,  1.17 examples/s]Map:   7%|▋         | 8/111 [00:07<01:25,  1.21 examples/s]Map:   8%|▊         | 9/111 [00:08<01:24,  1.21 examples/s]Map:   9%|▉         | 10/111 [00:09<01:22,  1.23 examples/s]Map:  10%|▉         | 11/111 [00:10<01:20,  1.24 examples/s]Map:  11%|█         | 12/111 [00:10<01:20,  1.23 examples/s]Map:  12%|█▏        | 13/111 [00:11<01:22,  1.19 examples/s]Map:  13%|█▎        | 14/111 [00:12<01:20,  1.20 examples/s]Map:  14%|█▎        | 15/111 [00:13<01:19,  1.20 examples/s]Map:  14%|█▍        | 16/111 [00:14<01:19,  1.20 examples/s]Map:  15%|█▌        | 17/111 [00:15<01:19,  1.18 examples/s]Map:  16%|█▌        | 18/111 [00:16<01:18,  1.19 examples/s]Map:  17%|█▋        | 19/111 [00:16<01:16,  1.20 examples/s]Map:  18%|█▊        | 20/111 [00:17<01:14,  1.21 examples/s]Map:  19%|█▉        | 21/111 [00:18<01:13,  1.22 examples/s]Map:  20%|█▉        | 22/111 [00:19<01:13,  1.21 examples/s]Map:  21%|██        | 23/111 [00:20<01:13,  1.20 examples/s]Map:  22%|██▏       | 24/111 [00:20<01:13,  1.19 examples/s]Map:  23%|██▎       | 25/111 [00:21<01:12,  1.18 examples/s]Map:  23%|██▎       | 26/111 [00:22<01:12,  1.17 examples/s]Map:  24%|██▍       | 27/111 [00:23<01:10,  1.19 examples/s]Map:  25%|██▌       | 28/111 [00:24<01:08,  1.21 examples/s]Map:  26%|██▌       | 29/111 [00:25<01:08,  1.20 examples/s]Map:  27%|██▋       | 30/111 [00:25<01:06,  1.21 examples/s]Map:  28%|██▊       | 31/111 [00:26<01:05,  1.21 examples/s]Map:  29%|██▉       | 32/111 [00:27<01:05,  1.21 examples/s]Map:  30%|██▉       | 33/111 [00:28<01:04,  1.21 examples/s]Map:  31%|███       | 34/111 [00:29<01:04,  1.20 examples/s]Map:  32%|███▏      | 35/111 [00:30<01:03,  1.19 examples/s]Map:  32%|███▏      | 36/111 [00:30<01:02,  1.20 examples/s]Map:  33%|███▎      | 37/111 [00:31<01:02,  1.18 examples/s]Map:  34%|███▍      | 38/111 [00:32<01:01,  1.19 examples/s]Map:  35%|███▌      | 39/111 [00:33<01:00,  1.20 examples/s]Map:  36%|███▌      | 40/111 [00:34<00:59,  1.20 examples/s]Map:  37%|███▋      | 41/111 [00:35<00:58,  1.19 examples/s]Map:  38%|███▊      | 42/111 [00:36<00:57,  1.19 examples/s]Map:  39%|███▊      | 43/111 [00:36<00:56,  1.20 examples/s]Map:  40%|███▉      | 44/111 [00:37<00:57,  1.17 examples/s]Map:  41%|████      | 45/111 [00:38<00:56,  1.16 examples/s]Map:  41%|████▏     | 46/111 [00:39<00:55,  1.17 examples/s]Map:  42%|████▏     | 47/111 [00:40<00:54,  1.17 examples/s]Map:  43%|████▎     | 48/111 [00:41<00:53,  1.18 examples/s]Map:  44%|████▍     | 49/111 [00:42<00:53,  1.17 examples/s]Map:  45%|████▌     | 50/111 [00:42<00:52,  1.17 examples/s]Map:  46%|████▌     | 51/111 [00:43<00:51,  1.16 examples/s]Map:  47%|████▋     | 52/111 [00:44<00:50,  1.16 examples/s]Map:  48%|████▊     | 53/111 [00:45<00:49,  1.16 examples/s]Map:  49%|████▊     | 54/111 [00:46<00:49,  1.16 examples/s]Map:  50%|████▉     | 55/111 [00:47<00:49,  1.14 examples/s]Map:  50%|█████     | 56/111 [00:48<00:47,  1.15 examples/s]Map:  51%|█████▏    | 57/111 [00:48<00:46,  1.15 examples/s]Map:  52%|█████▏    | 58/111 [00:49<00:45,  1.15 examples/s]Map:  53%|█████▎    | 59/111 [00:50<00:44,  1.16 examples/s]Map:  54%|█████▍    | 60/111 [00:51<00:44,  1.15 examples/s]Map:  55%|█████▍    | 61/111 [00:52<00:43,  1.14 examples/s]Map:  56%|█████▌    | 62/111 [00:53<00:42,  1.14 examples/s]Map:  57%|█████▋    | 63/111 [00:54<00:42,  1.14 examples/s]Map:  58%|█████▊    | 64/111 [00:55<00:42,  1.11 examples/s]Map:  59%|█████▊    | 65/111 [00:56<00:41,  1.12 examples/s]Map:  59%|█████▉    | 66/111 [00:56<00:39,  1.13 examples/s]Map:  60%|██████    | 67/111 [00:57<00:39,  1.12 examples/s]Map:  61%|██████▏   | 68/111 [00:58<00:38,  1.13 examples/s]Map:  62%|██████▏   | 69/111 [00:59<00:37,  1.13 examples/s]Map:  63%|██████▎   | 70/111 [01:00<00:36,  1.13 examples/s]Map:  64%|██████▍   | 71/111 [01:01<00:35,  1.13 examples/s]Map:  65%|██████▍   | 72/111 [01:02<00:34,  1.14 examples/s]Map:  66%|██████▌   | 73/111 [01:03<00:33,  1.12 examples/s]Map:  67%|██████▋   | 74/111 [01:04<00:33,  1.12 examples/s]Map:  68%|██████▊   | 75/111 [01:04<00:32,  1.10 examples/s]Map:  68%|██████▊   | 76/111 [01:05<00:31,  1.10 examples/s]Map:  69%|██████▉   | 77/111 [01:06<00:30,  1.11 examples/s]Map:  70%|███████   | 78/111 [01:07<00:29,  1.12 examples/s]Map:  71%|███████   | 79/111 [01:08<00:29,  1.10 examples/s]Map:  72%|███████▏  | 80/111 [01:09<00:28,  1.10 examples/s]Map:  73%|███████▎  | 81/111 [01:10<00:27,  1.10 examples/s]Map:  74%|███████▍  | 82/111 [01:11<00:26,  1.11 examples/s]Map:  75%|███████▍  | 83/111 [01:12<00:25,  1.11 examples/s]Map:  76%|███████▌  | 84/111 [01:13<00:24,  1.11 examples/s]Map:  77%|███████▋  | 85/111 [01:13<00:23,  1.12 examples/s]Map:  77%|███████▋  | 86/111 [01:14<00:22,  1.12 examples/s]Map:  78%|███████▊  | 87/111 [01:15<00:21,  1.12 examples/s]Map:  79%|███████▉  | 88/111 [01:16<00:20,  1.10 examples/s]Map:  80%|████████  | 89/111 [01:17<00:19,  1.10 examples/s]Map:  81%|████████  | 90/111 [01:18<00:18,  1.11 examples/s]Map:  82%|████████▏ | 91/111 [01:19<00:18,  1.10 examples/s]Map:  83%|████████▎ | 92/111 [01:20<00:17,  1.08 examples/s]Map:  84%|████████▍ | 93/111 [01:21<00:16,  1.07 examples/s]Map:  85%|████████▍ | 94/111 [01:22<00:15,  1.06 examples/s]Map:  86%|████████▌ | 95/111 [01:23<00:15,  1.07 examples/s]Map:  86%|████████▋ | 96/111 [01:24<00:16,  1.07s/ examples]Map:  87%|████████▋ | 97/111 [01:25<00:14,  1.02s/ examples]Map:  88%|████████▊ | 98/111 [01:26<00:14,  1.13s/ examples]Map:  89%|████████▉ | 99/111 [01:27<00:12,  1.07s/ examples]Map:  90%|█████████ | 100/111 [01:28<00:11,  1.03s/ examples]Map:  91%|█████████ | 101/111 [01:29<00:09,  1.01 examples/s]Map:  92%|█████████▏| 102/111 [01:31<00:10,  1.13s/ examples]Map:  93%|█████████▎| 103/111 [01:32<00:10,  1.33s/ examples]Map:  94%|█████████▎| 104/111 [01:33<00:08,  1.23s/ examples]Map:  95%|█████████▍| 105/111 [01:34<00:06,  1.15s/ examples]Map:  95%|█████████▌| 106/111 [01:35<00:05,  1.11s/ examples]Map:  96%|█████████▋| 107/111 [01:36<00:04,  1.07s/ examples]Map:  97%|█████████▋| 108/111 [01:37<00:03,  1.08s/ examples]Map:  98%|█████████▊| 109/111 [01:38<00:02,  1.04s/ examples]Map:  99%|█████████▉| 110/111 [01:39<00:01,  1.02s/ examples]Map: 100%|██████████| 111/111 [01:40<00:00,  1.01s/ examples]                                                             Saved results to: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_finetuned_result.csv
--> Getting fine-tuned test results...
Fine-tuned Test WER Without Language Model: 0.248
Fine-tuned Test WER With Language Model: 0.155


Fine-tuned Test CER Without Language Model: 0.104
Fine-tuned Test CER With Language Model: 0.080


--> Showing some fine-tuned prediction errors...
                                         target_text  ...                                   pred_str_with_LM
0                        and then the boy felt sorry  ...                        and then the boy felt sorry
1  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bull of milk whe...
2            a dinosaur jumps out and hulk is scared  ...           a dinosaur jumps out and chalk is scared
3                    so he ran and hid behind a tree  ...                    so he ran and hid behind a tree
4                                  the baby feel sad  ...                                  the baby feel sad
5               he was sad that there was a dinosaur  ...               he was sad that there was a dinosaur
6                     he hears what's inside the egg  ...                     he hears what's inside the egg
7  the green baby is holding the bottle of milk a...  ...  the green baby is holding the bull of milk and...
8  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bull of milk we ...
9                  because the dinosaur looked funny  ...                  because the dinosaur looked funny

[10 rows x 3 columns]
--> Taking a deeper look...
[pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] h [pad] [pad] [pad] [pad] [pad] e a l [pad] k [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] | | [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] w w [pad] a [pad] s [pad] | | [pad] h [pad] [pad] [pad] [pad] [pad] i [pad] d d d i n n g [pad] | | [pad] [pad] [pad] b [pad] [pad] [pad] a c c k [pad] [pad] [pad] w [pad] a r r d [pad] s | [pad] [pad] o n [pad] | | t h h e | | s s [pad] [pad] c [pad] [pad] [pad] a y y [pad] [pad] [pad] [pad] [pad] b [pad] [pad] [pad] [pad] o a r r [pad] d [pad] [pad] [pad] [pad] | [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] a a n d | | h [pad] e [pad] [pad] | [pad] h h [pad] [pad] i t t [pad] [pad] [pad] [pad] [pad] s s [pad] [pad] | [pad] [pad] [pad] a n d | | [pad] [pad] [pad] [pad] e [pad] g g [pad] g [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]

------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 30/08/2023 23:20:54
Wed Aug 30 23:35:13 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py
Started: 30/08/2023 23:35:13

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: AusKidTalk
experiment_id: eval_AusKidTalk_spontaneous_20230830
cache_name: AusKidTalk-eval
Language model: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829_with_lm_AusKidTalk_LM_combined
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
use_pretrained_tokenizer: True
pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/AusKidTalk/eval_AusKidTalk_spontaneous_20230830
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
--> pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 513.16it/s]
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fadff2182aaf1453.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-84e4ca2fd5125a54.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a8aa6bdffbb05b00_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b17bf95f4683362b_*_of_00004.arrow
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                                      he was hiding
1  /srv/scratch/chacmod/auskidtalk_spontaneous/11...                    so he ran and hid behind a tree
2  /srv/scratch/chacmod/auskidtalk_spontaneous/22...  and that's him lying on the groung like being ...
3  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                     the dinosaur lies on the floor
4  /srv/scratch/chacmod/auskidtalk_spontaneous/22...  but the dinosaur looked around and then he got...
5  /srv/scratch/chacmod/auskidtalk_spontaneous/81...  the egg starts cracking near the top and hulk ...
6  /srv/scratch/chacmod/auskidtalk_spontaneous/11...  baby hulk was riding on a skateboard and then ...
7  /srv/scratch/chacmod/auskidtalk_spontaneous/22...                        he is riding his skateboard
8  /srv/scratch/chacmod/auskidtalk_spontaneous/10...  the dinosaur is still there by himself but on ...
9  /srv/scratch/chacmod/auskidtalk_spontaneous/81...            a dinosaur jumps out and hulk is scared
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

--> Verifying data with a random sample...
Target text: there was a dinosaur
Input array shape: (42834,)
Sampling rate: 16000
Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
Map (num_proc=4):   7%|▋         | 8/111 [00:00<00:02, 42.52 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|███▌      | 40/111 [00:00<00:00, 138.54 examples/s]Map (num_proc=4):  58%|█████▊    | 64/111 [00:00<00:00, 160.77 examples/s]Map (num_proc=4):  76%|███████▌  | 84/111 [00:00<00:00, 165.06 examples/s]Map (num_proc=4):  96%|█████████▋| 107/111 [00:00<00:00, 121.97 examples/s]                                                                           Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
Map (num_proc=4):   7%|▋         | 8/111 [00:00<00:02, 44.00 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|███▌      | 40/111 [00:00<00:00, 143.75 examples/s]Map (num_proc=4):  58%|█████▊    | 64/111 [00:00<00:00, 161.51 examples/s]Map (num_proc=4):  76%|███████▌  | 84/111 [00:00<00:00, 151.04 examples/s]Map (num_proc=4):  96%|█████████▋| 107/111 [00:00<00:00, 122.22 examples/s]                                                                           /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py:607: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.

------> EVALUATING MODEL... ------------------------------------------ 

Map:   0%|          | 0/111 [00:00<?, ? examples/s]Map:   1%|          | 1/111 [00:01<02:22,  1.29s/ examples]Map:   2%|▏         | 2/111 [00:02<01:47,  1.01 examples/s]Map:   3%|▎         | 3/111 [00:02<01:36,  1.12 examples/s]Map:   4%|▎         | 4/111 [00:03<01:29,  1.20 examples/s]Map:   5%|▍         | 5/111 [00:04<01:26,  1.22 examples/s]Map:   5%|▌         | 6/111 [00:05<01:24,  1.24 examples/s]Map:   6%|▋         | 7/111 [00:05<01:22,  1.27 examples/s]Map:   7%|▋         | 8/111 [00:06<01:19,  1.29 examples/s]Map:   8%|▊         | 9/111 [00:07<01:19,  1.28 examples/s]Map:   9%|▉         | 10/111 [00:08<01:18,  1.29 examples/s]Map:  10%|▉         | 11/111 [00:08<01:17,  1.30 examples/s]Map:  11%|█         | 12/111 [00:09<01:17,  1.28 examples/s]Map:  12%|█▏        | 13/111 [00:10<01:18,  1.25 examples/s]Map:  13%|█▎        | 14/111 [00:11<01:17,  1.25 examples/s]Map:  14%|█▎        | 15/111 [00:12<01:16,  1.25 examples/s]Map:  14%|█▍        | 16/111 [00:13<01:16,  1.24 examples/s]Map:  15%|█▌        | 17/111 [00:13<01:16,  1.22 examples/s]Map:  16%|█▌        | 18/111 [00:14<01:15,  1.23 examples/s]Map:  17%|█▋        | 19/111 [00:15<01:13,  1.25 examples/s]Map:  18%|█▊        | 20/111 [00:16<01:12,  1.26 examples/s]Map:  19%|█▉        | 21/111 [00:17<01:11,  1.26 examples/s]Map:  20%|█▉        | 22/111 [00:17<01:10,  1.26 examples/s]Map:  21%|██        | 23/111 [00:18<01:10,  1.25 examples/s]Map:  22%|██▏       | 24/111 [00:19<01:10,  1.24 examples/s]Map:  23%|██▎       | 25/111 [00:20<01:09,  1.23 examples/s]Map:  23%|██▎       | 26/111 [00:21<01:09,  1.22 examples/s]Map:  24%|██▍       | 27/111 [00:21<01:07,  1.24 examples/s]Map:  25%|██▌       | 28/111 [00:22<01:06,  1.25 examples/s]Map:  26%|██▌       | 29/111 [00:23<01:05,  1.25 examples/s]Map:  27%|██▋       | 30/111 [00:24<01:04,  1.26 examples/s]Map:  28%|██▊       | 31/111 [00:25<01:03,  1.26 examples/s]Map:  29%|██▉       | 32/111 [00:25<01:02,  1.25 examples/s]Map:  30%|██▉       | 33/111 [00:26<01:01,  1.26 examples/s]Map:  31%|███       | 34/111 [00:27<01:01,  1.24 examples/s]Map:  32%|███▏      | 35/111 [00:28<01:01,  1.24 examples/s]Map:  32%|███▏      | 36/111 [00:29<01:00,  1.24 examples/s]Map:  33%|███▎      | 37/111 [00:29<01:00,  1.23 examples/s]Map:  34%|███▍      | 38/111 [00:30<00:58,  1.24 examples/s]Map:  35%|███▌      | 39/111 [00:31<00:57,  1.25 examples/s]Map:  36%|███▌      | 40/111 [00:32<00:56,  1.25 examples/s]Map:  37%|███▋      | 41/111 [00:33<00:56,  1.24 examples/s]Map:  38%|███▊      | 42/111 [00:33<00:55,  1.24 examples/s]Map:  39%|███▊      | 43/111 [00:34<00:54,  1.24 examples/s]Map:  40%|███▉      | 44/111 [00:35<00:55,  1.21 examples/s]Map:  41%|████      | 45/111 [00:36<00:54,  1.21 examples/s]Map:  41%|████▏     | 46/111 [00:37<00:53,  1.21 examples/s]Map:  42%|████▏     | 47/111 [00:38<00:52,  1.22 examples/s]Map:  43%|████▎     | 48/111 [00:38<00:51,  1.23 examples/s]Map:  44%|████▍     | 49/111 [00:39<00:51,  1.21 examples/s]Map:  45%|████▌     | 50/111 [00:40<00:50,  1.21 examples/s]Map:  46%|████▌     | 51/111 [00:41<00:49,  1.21 examples/s]Map:  47%|████▋     | 52/111 [00:42<00:48,  1.21 examples/s]Map:  48%|████▊     | 53/111 [00:43<00:47,  1.21 examples/s]Map:  49%|████▊     | 54/111 [00:43<00:47,  1.21 examples/s]Map:  50%|████▉     | 55/111 [00:44<00:46,  1.19 examples/s]Map:  50%|█████     | 56/111 [00:45<00:45,  1.20 examples/s]Map:  51%|█████▏    | 57/111 [00:46<00:44,  1.20 examples/s]Map:  52%|█████▏    | 58/111 [00:47<00:43,  1.21 examples/s]Map:  53%|█████▎    | 59/111 [00:47<00:42,  1.22 examples/s]Map:  54%|█████▍    | 60/111 [00:48<00:42,  1.20 examples/s]Map:  55%|█████▍    | 61/111 [00:49<00:41,  1.19 examples/s]Map:  56%|█████▌    | 62/111 [00:50<00:40,  1.20 examples/s]Map:  57%|█████▋    | 63/111 [00:51<00:40,  1.19 examples/s]Map:  58%|█████▊    | 64/111 [00:52<00:40,  1.16 examples/s]Map:  59%|█████▊    | 65/111 [00:53<00:39,  1.17 examples/s]Map:  59%|█████▉    | 66/111 [00:53<00:38,  1.18 examples/s]Map:  60%|██████    | 67/111 [00:54<00:37,  1.17 examples/s]Map:  61%|██████▏   | 68/111 [00:55<00:36,  1.18 examples/s]Map:  62%|██████▏   | 69/111 [00:56<00:35,  1.18 examples/s]Map:  63%|██████▎   | 70/111 [00:57<00:35,  1.17 examples/s]Map:  64%|██████▍   | 71/111 [00:58<00:34,  1.17 examples/s]Map:  65%|██████▍   | 72/111 [00:59<00:33,  1.18 examples/s]Map:  66%|██████▌   | 73/111 [00:59<00:32,  1.16 examples/s]Map:  67%|██████▋   | 74/111 [01:00<00:32,  1.16 examples/s]Map:  68%|██████▊   | 75/111 [01:01<00:31,  1.14 examples/s]Map:  68%|██████▊   | 76/111 [01:02<00:30,  1.14 examples/s]Map:  69%|██████▉   | 77/111 [01:03<00:29,  1.14 examples/s]Map:  70%|███████   | 78/111 [01:04<00:28,  1.15 examples/s]Map:  71%|███████   | 79/111 [01:05<00:28,  1.14 examples/s]Map:  72%|███████▏  | 80/111 [01:06<00:27,  1.14 examples/s]Map:  73%|███████▎  | 81/111 [01:06<00:26,  1.14 examples/s]Map:  74%|███████▍  | 82/111 [01:07<00:25,  1.15 examples/s]Map:  75%|███████▍  | 83/111 [01:08<00:24,  1.15 examples/s]Map:  76%|███████▌  | 84/111 [01:09<00:23,  1.15 examples/s]Map:  77%|███████▋  | 85/111 [01:10<00:22,  1.15 examples/s]Map:  77%|███████▋  | 86/111 [01:11<00:21,  1.16 examples/s]Map:  78%|███████▊  | 87/111 [01:12<00:20,  1.16 examples/s]Map:  79%|███████▉  | 88/111 [01:13<00:20,  1.14 examples/s]Map:  80%|████████  | 89/111 [01:13<00:19,  1.14 examples/s]Map:  81%|████████  | 90/111 [01:14<00:18,  1.15 examples/s]Map:  82%|████████▏ | 91/111 [01:15<00:17,  1.15 examples/s]Map:  83%|████████▎ | 92/111 [01:16<00:16,  1.13 examples/s]Map:  84%|████████▍ | 93/111 [01:17<00:16,  1.12 examples/s]Map:  85%|████████▍ | 94/111 [01:18<00:15,  1.11 examples/s]Map:  86%|████████▌ | 95/111 [01:19<00:14,  1.12 examples/s]Map:  86%|████████▋ | 96/111 [01:20<00:15,  1.03s/ examples]Map:  87%|████████▋ | 97/111 [01:21<00:13,  1.02 examples/s]Map:  88%|████████▊ | 98/111 [01:22<00:14,  1.10s/ examples]Map:  89%|████████▉ | 99/111 [01:23<00:12,  1.03s/ examples]Map:  90%|█████████ | 100/111 [01:24<00:10,  1.01 examples/s]Map:  91%|█████████ | 101/111 [01:25<00:09,  1.05 examples/s]Map:  92%|█████████▏| 102/111 [01:26<00:09,  1.08s/ examples]Map:  93%|█████████▎| 103/111 [01:28<00:10,  1.28s/ examples]Map:  94%|█████████▎| 104/111 [01:29<00:08,  1.19s/ examples]Map:  95%|█████████▍| 105/111 [01:30<00:06,  1.11s/ examples]Map:  95%|█████████▌| 106/111 [01:31<00:05,  1.07s/ examples]Map:  96%|█████████▋| 107/111 [01:32<00:04,  1.03s/ examples]Map:  97%|█████████▋| 108/111 [01:33<00:03,  1.04s/ examples]Map:  98%|█████████▊| 109/111 [01:34<00:02,  1.01s/ examples]Map:  99%|█████████▉| 110/111 [01:35<00:00,  1.01 examples/s]Map: 100%|██████████| 111/111 [01:36<00:00,  1.03 examples/s]                                                             Saved results to: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_finetuned_result.csv
--> Getting fine-tuned test results...
Fine-tuned Test WER Without Language Model: 0.248
Fine-tuned Test WER With Language Model: 0.155


Fine-tuned Test CER Without Language Model: 0.104
Fine-tuned Test CER With Language Model: 0.080


--> Showing some fine-tuned prediction errors...
                                         target_text  ...                                   pred_str_with_LM
0                        and then the boy felt sorry  ...                        and then the boy felt sorry
1  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bull of milk whe...
2            a dinosaur jumps out and hulk is scared  ...           a dinosaur jumps out and chalk is scared
3                    so he ran and hid behind a tree  ...                    so he ran and hid behind a tree
4                                  the baby feel sad  ...                                  the baby feel sad
5               he was sad that there was a dinosaur  ...               he was sad that there was a dinosaur
6                     he hears what's inside the egg  ...                     he hears what's inside the egg
7  the green baby is holding the bottle of milk a...  ...  the green baby is holding the bull of milk and...
8  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bull of milk we ...
9                  because the dinosaur looked funny  ...                  because the dinosaur looked funny

[10 rows x 3 columns]
--> Taking a deeper look...
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] h [PAD] [PAD] [PAD] [PAD] [PAD] e a l [PAD] k [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] w w [PAD] a [PAD] s [PAD] | | [PAD] h [PAD] [PAD] [PAD] [PAD] [PAD] i [PAD] d d d i n n g [PAD] | | [PAD] [PAD] [PAD] b [PAD] [PAD] [PAD] a c c k [PAD] [PAD] [PAD] w [PAD] a r r d [PAD] s | [PAD] [PAD] o n [PAD] | | t h h e | | s s [PAD] [PAD] c [PAD] [PAD] [PAD] a y y [PAD] [PAD] [PAD] [PAD] [PAD] b [PAD] [PAD] [PAD] [PAD] o a r r [PAD] d [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] a a n d | | h [PAD] e [PAD] [PAD] | [PAD] h h [PAD] [PAD] i t t [PAD] [PAD] [PAD] [PAD] [PAD] s s [PAD] [PAD] | [PAD] [PAD] [PAD] a n d | | [PAD] [PAD] [PAD] [PAD] e [PAD] g g [PAD] g [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 30/08/2023 23:37:06Wed Sep 6 19:16:34 AEST 2023Wed Sep 6 19:20:38 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py
Started: 06/09/2023 19:20:38

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: AusKidTalk
experiment_id: eval_AusKidTalk_spontaneous_20230830
cache_name: AusKidTalk-eval
Language model: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829_with_lm_AusKidTalk_LM_combined_v1_lowercase
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
use_pretrained_tokenizer: True
pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/AusKidTalk/eval_AusKidTalk_spontaneous_20230830
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829
--> pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/CU_MyST_AusKidTalk/finetune_CU_MyST_AusKidTalk_lowercase_20230829

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 279.30it/s]
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fadff2182aaf1453.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-84e4ca2fd5125a54.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a8aa6bdffbb05b00_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b17bf95f4683362b_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-d325caffdb7162b9_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b5bcc239e47f5bcc_*_of_00004.arrow
/srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py:607: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/auskidtalk_spontaneous/10...  the green baby is holding the bottle of milk w...
1  /srv/scratch/chacmod/auskidtalk_spontaneous/65...                        there's a dinosaur near him
2  /srv/scratch/chacmod/auskidtalk_spontaneous/10...                           the boy is on skateboard
3  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                     he hears what's inside the egg
4  /srv/scratch/chacmod/auskidtalk_spontaneous/81...                   hulk feeds the dinosaur the milk
5  /srv/scratch/chacmod/auskidtalk_spontaneous/11...  baby hulk was riding on a skateboard and then ...
6  /srv/scratch/chacmod/auskidtalk_spontaneous/10...  the green baby is laughing while the green din...
7  /srv/scratch/chacmod/auskidtalk_spontaneous/22...  but the dinosaur looked around and then he got...
8  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                               there was a dinosaur
9  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                  and he feed the dinosaur his milk
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

--> Verifying data with a random sample...
Target text: he hide behind the tree
Input array shape: (34041,)
Sampling rate: 16000
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.

------> EVALUATING MODEL... ------------------------------------------ 

Map:   0%|          | 0/111 [00:00<?, ? examples/s]Map:   1%|          | 1/111 [00:01<02:28,  1.35s/ examples]Map:   2%|▏         | 2/111 [00:02<01:52,  1.03s/ examples]Map:   3%|▎         | 3/111 [00:02<01:41,  1.06 examples/s]Map:   4%|▎         | 4/111 [00:03<01:34,  1.13 examples/s]Map:   5%|▍         | 5/111 [00:04<01:31,  1.15 examples/s]Map:   5%|▌         | 6/111 [00:05<01:30,  1.16 examples/s]Map:   6%|▋         | 7/111 [00:06<01:27,  1.19 examples/s]Map:   7%|▋         | 8/111 [00:07<01:24,  1.22 examples/s]Map:   8%|▊         | 9/111 [00:07<01:23,  1.22 examples/s]Map:   9%|▉         | 10/111 [00:08<01:22,  1.23 examples/s]Map:  10%|▉         | 11/111 [00:09<01:20,  1.24 examples/s]Map:  11%|█         | 12/111 [00:10<01:20,  1.23 examples/s]Map:  12%|█▏        | 13/111 [00:11<01:22,  1.19 examples/s]Map:  13%|█▎        | 14/111 [00:12<01:22,  1.18 examples/s]Map:  14%|█▎        | 15/111 [00:12<01:21,  1.18 examples/s]Map:  14%|█▍        | 16/111 [00:13<01:21,  1.17 examples/s]Map:  15%|█▌        | 17/111 [00:14<01:21,  1.16 examples/s]Map:  16%|█▌        | 18/111 [00:15<01:19,  1.17 examples/s]Map:  17%|█▋        | 19/111 [00:16<01:17,  1.19 examples/s]Map:  18%|█▊        | 20/111 [00:17<01:15,  1.20 examples/s]Map:  19%|█▉        | 21/111 [00:17<01:14,  1.20 examples/s]Map:  20%|█▉        | 22/111 [00:18<01:14,  1.20 examples/s]Map:  21%|██        | 23/111 [00:19<01:14,  1.19 examples/s]Map:  22%|██▏       | 24/111 [00:20<01:13,  1.18 examples/s]Map:  23%|██▎       | 25/111 [00:21<01:12,  1.18 examples/s]Map:  23%|██▎       | 26/111 [00:22<01:13,  1.16 examples/s]Map:  24%|██▍       | 27/111 [00:23<01:10,  1.19 examples/s]Map:  25%|██▌       | 28/111 [00:23<01:09,  1.19 examples/s]Map:  26%|██▌       | 29/111 [00:24<01:08,  1.19 examples/s]Map:  27%|██▋       | 30/111 [00:25<01:07,  1.20 examples/s]Map:  28%|██▊       | 31/111 [00:26<01:06,  1.20 examples/s]Map:  29%|██▉       | 32/111 [00:27<01:06,  1.19 examples/s]Map:  30%|██▉       | 33/111 [00:28<01:05,  1.20 examples/s]Map:  31%|███       | 34/111 [00:28<01:05,  1.18 examples/s]Map:  32%|███▏      | 35/111 [00:29<01:04,  1.19 examples/s]Map:  32%|███▏      | 36/111 [00:30<01:03,  1.19 examples/s]Map:  33%|███▎      | 37/111 [00:31<01:02,  1.19 examples/s]Map:  34%|███▍      | 38/111 [00:32<01:01,  1.19 examples/s]Map:  35%|███▌      | 39/111 [00:33<01:00,  1.20 examples/s]Map:  36%|███▌      | 40/111 [00:33<01:00,  1.18 examples/s]Map:  37%|███▋      | 41/111 [00:34<01:00,  1.17 examples/s]Map:  38%|███▊      | 42/111 [00:35<00:59,  1.16 examples/s]Map:  39%|███▊      | 43/111 [00:36<00:59,  1.15 examples/s]Map:  40%|███▉      | 44/111 [00:37<00:59,  1.12 examples/s]Map:  41%|████      | 45/111 [00:38<00:58,  1.12 examples/s]Map:  41%|████▏     | 46/111 [00:39<00:57,  1.13 examples/s]Map:  42%|████▏     | 47/111 [00:40<00:56,  1.13 examples/s]Map:  43%|████▎     | 48/111 [00:41<00:55,  1.14 examples/s]Map:  44%|████▍     | 49/111 [00:41<00:55,  1.13 examples/s]Map:  45%|████▌     | 50/111 [00:42<00:53,  1.13 examples/s]Map:  46%|████▌     | 51/111 [00:43<00:52,  1.14 examples/s]Map:  47%|████▋     | 52/111 [00:44<00:52,  1.13 examples/s]Map:  48%|████▊     | 53/111 [00:45<00:50,  1.14 examples/s]Map:  49%|████▊     | 54/111 [00:46<00:50,  1.14 examples/s]Map:  50%|████▉     | 55/111 [00:47<00:50,  1.11 examples/s]Map:  50%|█████     | 56/111 [00:48<00:48,  1.12 examples/s]Map:  51%|█████▏    | 57/111 [00:49<00:47,  1.13 examples/s]Map:  52%|█████▏    | 58/111 [00:49<00:47,  1.13 examples/s]Map:  53%|█████▎    | 59/111 [00:50<00:45,  1.14 examples/s]Map:  54%|█████▍    | 60/111 [00:51<00:45,  1.13 examples/s]Map:  55%|█████▍    | 61/111 [00:52<00:44,  1.12 examples/s]Map:  56%|█████▌    | 62/111 [00:53<00:43,  1.12 examples/s]Map:  57%|█████▋    | 63/111 [00:54<00:43,  1.11 examples/s]Map:  58%|█████▊    | 64/111 [00:55<00:43,  1.09 examples/s]Map:  59%|█████▊    | 65/111 [00:56<00:41,  1.10 examples/s]Map:  59%|█████▉    | 66/111 [00:57<00:40,  1.10 examples/s]Map:  60%|██████    | 67/111 [00:58<00:40,  1.10 examples/s]Map:  61%|██████▏   | 68/111 [00:58<00:38,  1.10 examples/s]Map:  62%|██████▏   | 69/111 [00:59<00:37,  1.11 examples/s]Map:  63%|██████▎   | 70/111 [01:00<00:36,  1.11 examples/s]Map:  64%|██████▍   | 71/111 [01:01<00:35,  1.12 examples/s]Map:  65%|██████▍   | 72/111 [01:02<00:34,  1.12 examples/s]Map:  66%|██████▌   | 73/111 [01:03<00:34,  1.11 examples/s]Map:  67%|██████▋   | 74/111 [01:04<00:33,  1.10 examples/s]Map:  68%|██████▊   | 75/111 [01:05<00:33,  1.08 examples/s]Map:  68%|██████▊   | 76/111 [01:06<00:32,  1.08 examples/s]Map:  69%|██████▉   | 77/111 [01:07<00:31,  1.08 examples/s]Map:  70%|███████   | 78/111 [01:08<00:30,  1.09 examples/s]Map:  71%|███████   | 79/111 [01:09<00:29,  1.08 examples/s]Map:  72%|███████▏  | 80/111 [01:09<00:28,  1.08 examples/s]Map:  73%|███████▎  | 81/111 [01:10<00:27,  1.08 examples/s]Map:  74%|███████▍  | 82/111 [01:11<00:26,  1.09 examples/s]Map:  75%|███████▍  | 83/111 [01:12<00:25,  1.09 examples/s]Map:  76%|███████▌  | 84/111 [01:13<00:24,  1.09 examples/s]Map:  77%|███████▋  | 85/111 [01:14<00:23,  1.10 examples/s]Map:  77%|███████▋  | 86/111 [01:15<00:22,  1.10 examples/s]Map:  78%|███████▊  | 87/111 [01:16<00:21,  1.10 examples/s]Map:  79%|███████▉  | 88/111 [01:17<00:21,  1.08 examples/s]Map:  80%|████████  | 89/111 [01:18<00:20,  1.09 examples/s]Map:  81%|████████  | 90/111 [01:19<00:19,  1.09 examples/s]Map:  82%|████████▏ | 91/111 [01:20<00:18,  1.09 examples/s]Map:  83%|████████▎ | 92/111 [01:20<00:17,  1.08 examples/s]Map:  84%|████████▍ | 93/111 [01:21<00:16,  1.08 examples/s]Map:  85%|████████▍ | 94/111 [01:22<00:15,  1.07 examples/s]Map:  86%|████████▌ | 95/111 [01:23<00:14,  1.07 examples/s]Map:  86%|████████▋ | 96/111 [01:24<00:14,  1.07 examples/s]Map:  87%|████████▋ | 97/111 [01:25<00:13,  1.08 examples/s]Map:  88%|████████▊ | 98/111 [01:26<00:12,  1.06 examples/s]Map:  89%|████████▉ | 99/111 [01:27<00:11,  1.06 examples/s]Map:  90%|█████████ | 100/111 [01:28<00:10,  1.07 examples/s]Map:  91%|█████████ | 101/111 [01:29<00:09,  1.07 examples/s]Map:  92%|█████████▏| 102/111 [01:30<00:08,  1.05 examples/s]Map:  93%|█████████▎| 103/111 [01:31<00:07,  1.04 examples/s]Map:  94%|█████████▎| 104/111 [01:32<00:06,  1.01 examples/s]Map:  95%|█████████▍| 105/111 [01:33<00:05,  1.01 examples/s]Map:  95%|█████████▌| 106/111 [01:34<00:05,  1.00s/ examples]Map:  96%|█████████▋| 107/111 [01:35<00:04,  1.00s/ examples]Map:  97%|█████████▋| 108/111 [01:36<00:03,  1.04s/ examples]Map:  98%|█████████▊| 109/111 [01:37<00:02,  1.02s/ examples]Map:  99%|█████████▉| 110/111 [01:38<00:01,  1.02s/ examples]Map: 100%|██████████| 111/111 [01:39<00:00,  1.01s/ examples]                                                             Saved results to: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230830_finetuned_result.csv
--> Getting fine-tuned test results...
Fine-tuned Test WER Without Language Model: 0.248
Fine-tuned Test WER With Language Model: 0.155


Fine-tuned Test CER Without Language Model: 0.104
Fine-tuned Test CER With Language Model: 0.080


--> Showing some fine-tuned prediction errors...
                                         target_text  ...                                   pred_str_with_LM
0                        and then the boy felt sorry  ...                        and then the boy felt sorry
1  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bull of milk whe...
2            a dinosaur jumps out and hulk is scared  ...           a dinosaur jumps out and chalk is scared
3                    so he ran and hid behind a tree  ...                    so he ran and hid behind a tree
4                                  the baby feel sad  ...                                  the baby feel sad
5               he was sad that there was a dinosaur  ...               he was sad that there was a dinosaur
6                     he hears what's inside the egg  ...                     he hears what's inside the egg
7  the green baby is holding the bottle of milk a...  ...  the green baby is holding the bull of milk and...
8  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bull of milk we ...
9                  because the dinosaur looked funny  ...                  because the dinosaur looked funny

[10 rows x 3 columns]
--> Taking a deeper look...
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] h [PAD] [PAD] [PAD] [PAD] [PAD] e a l [PAD] k [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] w w [PAD] a [PAD] s [PAD] | | [PAD] h [PAD] [PAD] [PAD] [PAD] [PAD] i [PAD] d d d i n n g [PAD] | | [PAD] [PAD] [PAD] b [PAD] [PAD] [PAD] a c c k [PAD] [PAD] [PAD] w [PAD] a r r d [PAD] s | [PAD] [PAD] o n [PAD] | | t h h e | | s s [PAD] [PAD] c [PAD] [PAD] [PAD] a y y [PAD] [PAD] [PAD] [PAD] [PAD] b [PAD] [PAD] [PAD] [PAD] o a r r [PAD] d [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] a a n d | | h [PAD] e [PAD] [PAD] | [PAD] h h [PAD] [PAD] i t t [PAD] [PAD] [PAD] [PAD] [PAD] s s [PAD] [PAD] | [PAD] [PAD] [PAD] a n d | | [PAD] [PAD] [PAD] [PAD] e [PAD] g g [PAD] g [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 06/09/2023 19:22:33