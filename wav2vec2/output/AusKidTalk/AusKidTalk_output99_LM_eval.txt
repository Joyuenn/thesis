Wed Sep 20 04:20:32 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py
Started: 20/09/2023 04:20:32

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: AusKidTalk
experiment_id: eval_AusKidTalk_spontaneous_20230920_3
cache_name: AusKidTalk-eval
Language model: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914_with_lm_AusKidTalk_LM_combined_lowercase_v1
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914
use_pretrained_tokenizer: True
pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914
--> pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 380.54it/s]
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fadff2182aaf1453.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-84e4ca2fd5125a54.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a8aa6bdffbb05b00_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b17bf95f4683362b_*_of_00004.arrow
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/auskidtalk_spontaneous/10...  the green baby is holding the bottle of milk w...
1  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                               the egg was cracking
2  /srv/scratch/chacmod/auskidtalk_spontaneous/22...               and he ran and hid from the dinosaur
3  /srv/scratch/chacmod/auskidtalk_spontaneous/65...                                 there's a dinosaur
4  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                               the dinosaur was sad
5  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                     the dinosaur lies on the floor
6  /srv/scratch/chacmod/auskidtalk_spontaneous/11...         and went up to baby hulk but he was scared
7  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                               there was a dinosaur
8  /srv/scratch/chacmod/auskidtalk_spontaneous/22...                  because the dinosaur looked funny
9  /srv/scratch/chacmod/auskidtalk_spontaneous/10...                    the boy is hiding behind a tree
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

--> Verifying data with a random sample...
Target text: he's lonely
Input array shape: (20106,)
Sampling rate: 16000
Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):   7%|â–‹         | 8/111 [00:00<00:02, 39.21 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 40/111 [00:00<00:00, 145.95 examples/s]Map (num_proc=4):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/111 [00:00<00:00, 166.10 examples/s]Map (num_proc=4):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 84/111 [00:00<00:00, 152.52 examples/s]Map (num_proc=4):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 107/111 [00:00<00:00, 128.42 examples/s]                                                                           Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):   7%|â–‹         | 8/111 [00:00<00:02, 43.71 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  29%|â–ˆâ–ˆâ–‰       | 32/111 [00:00<00:00, 127.13 examples/s]Map (num_proc=4):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/111 [00:00<00:00, 169.11 examples/s]Map (num_proc=4):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 84/111 [00:00<00:00, 151.42 examples/s]Map (num_proc=4):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 107/111 [00:00<00:00, 115.71 examples/s]                                                                           /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py:567: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.

------> EVALUATING MODEL... ------------------------------------------ 

Map:   0%|          | 0/111 [00:00<?, ? examples/s]Map:   1%|          | 1/111 [00:01<02:25,  1.32s/ examples]Map:   2%|â–         | 2/111 [00:02<01:49,  1.01s/ examples]Map:   3%|â–Ž         | 3/111 [00:02<01:38,  1.10 examples/s]Map:   4%|â–Ž         | 4/111 [00:03<01:31,  1.17 examples/s]Map:   5%|â–         | 5/111 [00:04<01:29,  1.18 examples/s]Map:   5%|â–Œ         | 6/111 [00:05<01:27,  1.19 examples/s]Map:   6%|â–‹         | 7/111 [00:06<01:26,  1.20 examples/s]Map:   7%|â–‹         | 8/111 [00:06<01:23,  1.23 examples/s]Map:   8%|â–Š         | 9/111 [00:07<01:23,  1.22 examples/s]Map:   9%|â–‰         | 10/111 [00:08<01:21,  1.23 examples/s]Map:  10%|â–‰         | 11/111 [00:09<01:20,  1.24 examples/s]Map:  11%|â–ˆ         | 12/111 [00:10<01:20,  1.24 examples/s]Map:  12%|â–ˆâ–        | 13/111 [00:11<01:20,  1.22 examples/s]Map:  13%|â–ˆâ–Ž        | 14/111 [00:11<01:19,  1.22 examples/s]Map:  14%|â–ˆâ–Ž        | 15/111 [00:13<01:30,  1.06 examples/s]Map:  14%|â–ˆâ–        | 16/111 [00:14<01:38,  1.04s/ examples]Map:  15%|â–ˆâ–Œ        | 17/111 [00:15<01:32,  1.01 examples/s]Map:  16%|â–ˆâ–Œ        | 18/111 [00:15<01:26,  1.07 examples/s]Map:  17%|â–ˆâ–‹        | 19/111 [00:16<01:21,  1.12 examples/s]Map:  18%|â–ˆâ–Š        | 20/111 [00:17<01:19,  1.15 examples/s]Map:  19%|â–ˆâ–‰        | 21/111 [00:18<01:16,  1.17 examples/s]Map:  20%|â–ˆâ–‰        | 22/111 [00:19<01:15,  1.18 examples/s]Map:  21%|â–ˆâ–ˆ        | 23/111 [00:20<01:13,  1.19 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 24/111 [00:20<01:13,  1.19 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 25/111 [00:21<01:12,  1.19 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 26/111 [00:22<01:20,  1.05 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 27/111 [00:23<01:15,  1.12 examples/s]Map:  25%|â–ˆâ–ˆâ–Œ       | 28/111 [00:24<01:22,  1.01 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 29/111 [00:26<01:27,  1.06s/ examples]Map:  27%|â–ˆâ–ˆâ–‹       | 30/111 [00:26<01:19,  1.02 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 31/111 [00:28<01:24,  1.05s/ examples]Map:  29%|â–ˆâ–ˆâ–‰       | 32/111 [00:29<01:36,  1.22s/ examples]Map:  30%|â–ˆâ–ˆâ–‰       | 33/111 [00:30<01:25,  1.09s/ examples]Map:  31%|â–ˆâ–ˆâ–ˆ       | 34/111 [00:31<01:18,  1.02s/ examples]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 35/111 [00:32<01:13,  1.04 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 36/111 [00:33<01:08,  1.09 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 37/111 [00:33<01:05,  1.13 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 38/111 [00:34<01:03,  1.16 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 39/111 [00:35<01:01,  1.17 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 40/111 [00:36<00:59,  1.20 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 41/111 [00:37<00:57,  1.21 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 42/111 [00:37<00:56,  1.22 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 43/111 [00:38<00:55,  1.22 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 44/111 [00:39<00:56,  1.19 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 45/111 [00:40<00:55,  1.19 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/111 [00:41<00:54,  1.20 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 47/111 [00:42<00:52,  1.21 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 48/111 [00:42<00:51,  1.22 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 49/111 [00:43<00:51,  1.19 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 50/111 [00:44<00:51,  1.19 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 51/111 [00:45<00:49,  1.20 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 52/111 [00:46<00:49,  1.20 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 53/111 [00:47<00:48,  1.19 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 54/111 [00:47<00:48,  1.18 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 55/111 [00:48<00:47,  1.18 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 56/111 [00:49<00:46,  1.18 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 57/111 [00:50<00:45,  1.17 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 58/111 [00:51<00:44,  1.18 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 59/111 [00:52<00:43,  1.19 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 60/111 [00:53<00:43,  1.16 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 61/111 [00:53<00:43,  1.16 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 62/111 [00:54<00:42,  1.15 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 63/111 [00:55<00:42,  1.14 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/111 [00:56<00:41,  1.13 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 65/111 [00:57<00:40,  1.13 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 66/111 [00:58<00:40,  1.12 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 67/111 [00:59<00:39,  1.13 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 68/111 [01:00<00:37,  1.15 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 69/111 [01:01<00:36,  1.15 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 70/111 [01:01<00:35,  1.15 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 71/111 [01:02<00:34,  1.15 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 72/111 [01:03<00:33,  1.16 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 73/111 [01:04<00:33,  1.14 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 74/111 [01:05<00:32,  1.15 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 75/111 [01:06<00:32,  1.12 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 76/111 [01:07<00:30,  1.13 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 77/111 [01:08<00:30,  1.13 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 78/111 [01:08<00:29,  1.14 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 79/111 [01:09<00:28,  1.12 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 80/111 [01:10<00:27,  1.13 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 81/111 [01:11<00:26,  1.13 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 82/111 [01:12<00:25,  1.14 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 83/111 [01:13<00:24,  1.15 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 84/111 [01:14<00:23,  1.14 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 85/111 [01:15<00:23,  1.13 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 86/111 [01:16<00:22,  1.13 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 87/111 [01:16<00:21,  1.12 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 88/111 [01:17<00:20,  1.13 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 89/111 [01:18<00:19,  1.14 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 90/111 [01:19<00:18,  1.15 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 91/111 [01:20<00:17,  1.15 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 92/111 [01:21<00:17,  1.11 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 93/111 [01:22<00:16,  1.10 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 94/111 [01:23<00:15,  1.09 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 95/111 [01:24<00:14,  1.10 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 96/111 [01:25<00:13,  1.09 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 97/111 [01:25<00:12,  1.11 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 98/111 [01:26<00:11,  1.09 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 99/111 [01:27<00:10,  1.09 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 100/111 [01:28<00:09,  1.10 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 101/111 [01:29<00:08,  1.11 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 102/111 [01:30<00:08,  1.11 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 103/111 [01:31<00:07,  1.08 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 104/111 [01:32<00:06,  1.06 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 105/111 [01:33<00:05,  1.05 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 106/111 [01:34<00:04,  1.03 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 107/111 [01:35<00:03,  1.04 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 108/111 [01:36<00:02,  1.01 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 109/111 [01:37<00:02,  1.00s/ examples]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 110/111 [01:38<00:00,  1.00 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [01:39<00:00,  1.01 examples/s]                                                             Saved results to: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3_finetuned_result.csv
--> Getting fine-tuned test results...
Fine-tuned Test WER Without Language Model: 0.405
Fine-tuned Test WER With Language Model: 0.350


Fine-tuned Test CER Without Language Model: 0.220
Fine-tuned Test CER With Language Model: 0.222


--> Showing some fine-tuned prediction errors...
                                         target_text  ...                                   pred_str_with_LM
0                        and then the boy felt sorry  ...                        and then the boy felt sorry
1  the green baby is holding the bottle of milk w...  ...  the green bub is holding the bubble of milk th...
2            a dinosaur jumps out and hulk is scared  ...               a dina sour jumps out and hak is get
3                    so he ran and hid behind a tree  ...                    so he ran and hid behind a tree
4                                  the baby feel sad  ...                                 the by b feel sack
5               he was sad that there was a dinosaur  ...                  he was sad that there was the doc
6                     he hears what's inside the egg  ...                             he is what's inside it
7  the green baby is holding the bottle of milk a...  ...  the green baby is holding the bubble of milk a...
8  the green baby is holding the bottle of milk w...  ...  the green baby is holding the bubble of milk w...
9                  because the dinosaur looked funny  ...                     because the dinos looked funny

[10 rows x 3 columns]
--> Taking a deeper look...
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] h [PAD] a [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] k k [PAD] | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] w w a [PAD] [PAD] s | | [PAD] [PAD] h [PAD] [PAD] [PAD] [PAD] i i [PAD] [PAD] d [PAD] i n n g | | [PAD] [PAD] [PAD] [PAD] b a [PAD] [PAD] c c c k k [PAD] w w a r r [PAD] d s | | [PAD] o [PAD] n [PAD] | [PAD] t h [PAD] e | | [PAD] s [PAD] [PAD] c a a [PAD] [PAD] [PAD] y [PAD] e [PAD] [PAD] [PAD] b o a [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] t [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] a a n n d | | h e e [PAD] | | [PAD] h h i i [PAD] [PAD] t t [PAD] [PAD] [PAD] s | | [PAD] [PAD] [PAD] [PAD] [PAD] a n n [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] g [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 20/09/2023 04:22:28
Wed Sep 20 04:59:10 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py
Started: 20/09/2023 04:59:10

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: AusKidTalk
experiment_id: eval_AusKidTalk_spontaneous_20230920_3
cache_name: AusKidTalk-eval
Language model: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914_with_lm_AusKidTalk_LM_combined_lowercase_v1
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914
use_pretrained_tokenizer: True
pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/full/full_model_OGI_American_20230702
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/spontaneous/AusKidTalk_spontaneous_test_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/spontaneous/AusKidTalk_spontaneous_test_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/finetune_MyST_lowercase_20230914
--> pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/full/full_model_OGI_American_20230702

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 506.62it/s]
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-bf8e66dca7cd02c2.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a9cb62d9def94e4f.arrow
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c6683a2452038c26_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-2901cfd0e4a01021_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-8e51be239727254e_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5d719c18b5641d05_*_of_00004.arrow
/srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py:565: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 24
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 24
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                    the dinosaur poked out his head
1  /srv/scratch/chacmod/auskidtalk_spontaneous/81...  hulk was riding backwards on a skateboard and ...
2  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                  and he feed the dinosaur his milk
3  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                 and he was looking at the dinosaur
4  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                       he was touching the dinosaur
5  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                               there was a dinosaur
6  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                           he was on the skateboard
7  /srv/scratch/chacmod/auskidtalk_spontaneous/81...  hulk looks at the dinosaur and feels bad about...
8  /srv/scratch/chacmod/auskidtalk_spontaneous/81...  he's curious to see what's inside the egg and ...
9  /srv/scratch/chacmod/auskidtalk_spontaneous/81...  hulk tries to hide from the dinosaur and is st...
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

--> Verifying data with a random sample...
Target text: hulk feeds the dinosaur the milk
Input array shape: (41875,)
Sampling rate: 16000
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.

------> EVALUATING MODEL... ------------------------------------------ 

Map:   0%|          | 0/24 [00:00<?, ? examples/s]Map:   4%|â–         | 1/24 [00:01<00:29,  1.29s/ examples]Map:   8%|â–Š         | 2/24 [00:02<00:22,  1.00s/ examples]Map:  12%|â–ˆâ–Ž        | 3/24 [00:02<00:18,  1.12 examples/s]Map:  17%|â–ˆâ–‹        | 4/24 [00:03<00:17,  1.17 examples/s]Map:  21%|â–ˆâ–ˆ        | 5/24 [00:04<00:15,  1.20 examples/s]Map:  25%|â–ˆâ–ˆâ–Œ       | 6/24 [00:05<00:14,  1.23 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 7/24 [00:06<00:13,  1.22 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 8/24 [00:06<00:12,  1.23 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9/24 [00:07<00:12,  1.24 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10/24 [00:08<00:11,  1.23 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 11/24 [00:09<00:10,  1.26 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12/24 [00:10<00:09,  1.24 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/24 [00:10<00:08,  1.23 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 14/24 [00:11<00:08,  1.21 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15/24 [00:12<00:07,  1.24 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16/24 [00:13<00:06,  1.25 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 17/24 [00:14<00:05,  1.22 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18/24 [00:14<00:04,  1.22 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19/24 [00:15<00:04,  1.21 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 20/24 [00:16<00:03,  1.21 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21/24 [00:17<00:02,  1.21 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22/24 [00:18<00:01,  1.22 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 23/24 [00:19<00:00,  1.22 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:19<00:00,  1.20 examples/s]                                                           Saved results to: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_3_finetuned_result.csv
--> Getting fine-tuned test results...
Fine-tuned Test WER Without Language Model: 0.472
Fine-tuned Test WER With Language Model: 0.455


Fine-tuned Test CER Without Language Model: 0.259
Fine-tuned Test CER With Language Model: 0.274


--> Showing some fine-tuned prediction errors...
                                         target_text  ...                                   pred_str_with_LM
0                    and the dinosaur was lying down  ...                 and the done cell was fraying down
1                               there was a dinosaur  ...                                     the he was a d
2                           he was on the skateboard  ...                             i was on the sky board
3  hulk was riding backwards on a skateboard and ...  ...  he was hiding backwards on the scale boat and ...
4                 and he was looking at the dinosaur  ...                         and he was poking at the d
5                  and he feed the dinosaur his milk  ...                      um a fat the doncell is smoke
6   laughing while the dinosuar is drinking the milk  ...    thing while the diana soul is drinking the milk
7                 the dinosaur was about to lick him  ...                     the dice was about to lick him
8                   hulk feeds the dinosaur the milk  ...                         a feeds the dinar the milk
9            a dinosaur jumps out and hulk is scared  ...               a dina sour jumps out and hak is get

[10 rows x 3 columns]
--> Taking a deeper look...
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] t [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] w [PAD] a [PAD] [PAD] [PAD] [PAD] [PAD] s s | | [PAD] [PAD] [PAD] [PAD] [PAD] o [PAD] [PAD] [PAD] [PAD] n n [PAD] | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] t t h h e [PAD] [PAD] [PAD] | | [PAD] [PAD] [PAD] s s [PAD] [PAD] [PAD] [PAD] k [PAD] u [PAD] [PAD] y y [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] b [PAD] o a a [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] r r [PAD] d d [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 20/09/2023 04:59:43
Wed Sep 20 05:03:19 AEST 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py
Started: 20/09/2023 05:03:19

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: AusKidTalk
experiment_id: eval_AusKidTalk_spontaneous_20230920_4
cache_name: AusKidTalk-eval
Language model: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/progressive_finetune_CU_MyST_lowercase_20230916_with_lm_AusKidTalk_LM_combined_lowercase_v1/language_model/AusKidTalk_LM_combined_lowercase_v1.arpa
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/progressive_finetune_CU_MyST_lowercase_20230916
use_pretrained_tokenizer: True
pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/full/full_model_OGI_American_20230702
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/progressive_finetune_CU_MyST_lowercase_20230916
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/spontaneous/AusKidTalk_spontaneous_test_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/spontaneous/AusKidTalk_spontaneous_test_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_4_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_4
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_4_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_20230920_4_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/MyST/progressive_finetune_CU_MyST_lowercase_20230916
--> pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/full/full_model_OGI_American_20230702

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 540.43it/s]
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-bf8e66dca7cd02c2.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a9cb62d9def94e4f.arrow
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c6683a2452038c26_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-2901cfd0e4a01021_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-8e51be239727254e_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-83a208ef1ad29d5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5d719c18b5641d05_*_of_00004.arrow
/srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py:565: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 24
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 24
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/auskidtalk_spontaneous/81...  he's curious to see what's inside the egg and ...
1  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                    and the dinosaur was lying down
2  /srv/scratch/chacmod/auskidtalk_spontaneous/81...  hulk looks at the dinosaur and feels bad about...
3  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                           he was on the skateboard
4  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                 and he was looking at the dinosaur
5  /srv/scratch/chacmod/auskidtalk_spontaneous/81...                   hulk feeds the dinosaur the milk
6  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                        he was listening to the egg
7  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                 the dinosaur was about to lick him
8  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                                  and then he laugh
9  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                                      he was hiding
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

--> Verifying data with a random sample...
Target text: there was a dinosaur
Input array shape: (42834,)
Sampling rate: 16000
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.

------> EVALUATING MODEL... ------------------------------------------ 

Traceback (most recent call last):
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py", line 430, in get_feature_extractor_dict
    resolved_feature_extractor_file = cached_file(
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/utils/hub.py", line 417, in cached_file
    resolved_file = hf_hub_download(
  File "/home/z5313567/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 112, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/z5313567/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 160, in validate_repo_id
    raise HFValidationError(
huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/srv/scratch/z5313567/thesis/wav2vec2/model/MyST/progressive_finetune_CU_MyST_lowercase_20230916_with_lm_AusKidTalk_LM_combined_lowercase_v1/language_model/AusKidTalk_LM_combined_lowercase_v1.arpa'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/srv/scratch/z5313567/thesis/wav2vec2/code/eval_LM_lowercase.py", line 692, in <module>
    processor_LM = Wav2Vec2ProcessorWithLM.from_pretrained(eval_lm, cache_dir = data_cache_fp)
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py", line 146, in from_pretrained
    feature_extractor, tokenizer = super()._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/processing_utils.py", line 228, in _get_arguments_from_pretrained
    args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py", line 330, in from_pretrained
    feature_extractor_dict, kwargs = cls.get_feature_extractor_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py", line 448, in get_feature_extractor_dict
    raise EnvironmentError(
OSError: Can't load feature extractor for '/srv/scratch/z5313567/thesis/wav2vec2/model/MyST/progressive_finetune_CU_MyST_lowercase_20230916_with_lm_AusKidTalk_LM_combined_lowercase_v1/language_model/AusKidTalk_LM_combined_lowercase_v1.arpa'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/srv/scratch/z5313567/thesis/wav2vec2/model/MyST/progressive_finetune_CU_MyST_lowercase_20230916_with_lm_AusKidTalk_LM_combined_lowercase_v1/language_model/AusKidTalk_LM_combined_lowercase_v1.arpa' is the correct path to a directory containing a preprocessor_config.json file
