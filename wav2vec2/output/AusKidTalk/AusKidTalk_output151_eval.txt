Mon Oct 2 14:20:23 AEDT 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval/csv/default-740726f212ce8d2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
------------------------------------------------------------------------
                 run_finetune_kids.py                                   
------------------------------------------------------------------------
Running:  /srv/scratch/z5313567/thesis/wav2vec2/code/eval_AusKidTalk_spontaneous_lowercase.py
Started: 02/10/2023 14:20:23

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Wav2VecCTC...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: wav2vec2
dataset_name: AusKidTalk
experiment_id: eval_AusKidTalk_spontaneous_lowercase_20231002
cache_name: AusKidTalk-eval
training: False
use_checkpoint: True
checkpoint: /srv/scratch/z5313567/thesis/wav2vec2/model/AusTalk/finetune_AusTalk_lowercase_20231001
use_pretrained_tokenizer: True
pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/full/full_model_OGI_American_20230702
eval_pretrained: True
eval_model: /srv/scratch/z5313567/thesis/wav2vec2/model/AusTalk/finetune_AusTalk_lowercase_20231001
baseline_model: facebook/wav2vec2-base-960h
eval_baseline: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.05
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: True
gradient_checkpointing: True

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: steps
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 14
max_steps: 60000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 1000
save_strategy: steps
save_steps: 1000
save_total_limit: 3
fp16: True
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
group_by_length: True

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_spontaneous_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-eval
--> vocab_fp: /srv/scratch/z5313567/thesis/wav2vec2/vocab/AusKidTalk/eval_AusKidTalk_spontaneous_lowercase_20231002_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/wav2vec2/model/AusKidTalk/eval_AusKidTalk_spontaneous_lowercase_20231002
--> baseline_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/baseline_result/AusKidTalk/eval_AusKidTalk_spontaneous_lowercase_20231002_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_lowercase_20231002_finetuned_result.csv
--> pretrained_mod: /srv/scratch/z5313567/thesis/wav2vec2/model/AusTalk/finetune_AusTalk_lowercase_20231001
--> pretrained_tokenizer: /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/full/full_model_OGI_American_20230702

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 312.97it/s]
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 111
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/auskidtalk_spontaneous/22...                              and then he got upset
1  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                 the dinosaur was about to lick him
2  /srv/scratch/chacmod/auskidtalk_spontaneous/10...  the green skined baby is bouncing off the dino...
3  /srv/scratch/chacmod/auskidtalk_spontaneous/65...                          he's hiding behind a tree
4  /srv/scratch/chacmod/auskidtalk_spontaneous/51...                 and he was looking at the dinosaur
5  /srv/scratch/chacmod/auskidtalk_spontaneous/11...                                    he hurt the egg
6  /srv/scratch/chacmod/auskidtalk_spontaneous/10...                the boy's touching the dinosaur egg
7  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                                the baby feel happy
8  /srv/scratch/chacmod/auskidtalk_spontaneous/25...                           and then he finds an egg
9  /srv/scratch/chacmod/auskidtalk_spontaneous/81...   laughing while the dinosuar is drinking the milk
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------

Map:   0%|          | 0/111 [00:00<?, ? examples/s]                                                   Map:   0%|          | 0/111 [00:00<?, ? examples/s]                                                   Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]Map (num_proc=4):   1%|          | 1/111 [00:01<02:55,  1.59s/ examples]Map (num_proc=4):   9%|â–‰         | 10/111 [00:01<00:13,  7.74 examples/s]Map (num_proc=4):  23%|â–ˆâ–ˆâ–Ž       | 25/111 [00:01<00:03, 22.05 examples/s]Map (num_proc=4):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 37/111 [00:01<00:02, 34.18 examples/s]Map (num_proc=4):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 50/111 [00:02<00:01, 45.87 examples/s]Map (num_proc=4):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 72/111 [00:02<00:00, 74.42 examples/s]Map (num_proc=4):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 95/111 [00:02<00:00, 97.67 examples/s]Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:02<00:00, 105.36 examples/s]                                                                           Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]Map (num_proc=4):   1%|          | 1/111 [00:00<01:38,  1.12 examples/s]Map (num_proc=4):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 94/111 [00:01<00:00, 127.22 examples/s]                                                                          --> Verifying data with a random sample...
Target text: he's hiding behind a tree
Input array shape: (35932,)
Sampling rate: 16000
Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
Map (num_proc=4):   7%|â–‹         | 8/111 [00:00<00:02, 50.22 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 40/111 [00:00<00:00, 151.12 examples/s]Map (num_proc=4):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/111 [00:00<00:00, 174.30 examples/s]Map (num_proc=4):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 84/111 [00:00<00:00, 172.79 examples/s]Map (num_proc=4):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 107/111 [00:00<00:00, 136.13 examples/s]                                                                           Map (num_proc=4):   0%|          | 0/111 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):   7%|â–‹         | 8/111 [00:00<00:02, 50.08 examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/z5313567/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:166: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tensor = as_tensor(value)
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Map (num_proc=4):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 40/111 [00:00<00:00, 149.43 examples/s]Map (num_proc=4):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/111 [00:00<00:00, 172.10 examples/s]Map (num_proc=4):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 84/111 [00:00<00:00, 176.43 examples/s]Map (num_proc=4):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 107/111 [00:00<00:00, 135.34 examples/s]                                                                           /srv/scratch/z5313567/thesis/wav2vec2/code/eval_AusKidTalk_spontaneous_lowercase.py:562: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.

------> EVALUATING MODEL... ------------------------------------------ 

Map:   0%|          | 0/111 [00:00<?, ? examples/s]Map:   1%|          | 1/111 [00:02<04:44,  2.59s/ examples]Map:   3%|â–Ž         | 3/111 [00:02<01:16,  1.40 examples/s]Map:   5%|â–         | 5/111 [00:02<00:39,  2.69 examples/s]Map:   7%|â–‹         | 8/111 [00:02<00:20,  5.05 examples/s]Map:  11%|â–ˆ         | 12/111 [00:03<00:12,  8.14 examples/s]Map:  13%|â–ˆâ–Ž        | 14/111 [00:03<00:10,  9.03 examples/s]Map:  14%|â–ˆâ–        | 16/111 [00:03<00:09, 10.21 examples/s]Map:  17%|â–ˆâ–‹        | 19/111 [00:03<00:07, 12.37 examples/s]Map:  21%|â–ˆâ–ˆ        | 23/111 [00:03<00:05, 15.01 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 27/111 [00:03<00:04, 18.13 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 31/111 [00:04<00:03, 20.71 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 34/111 [00:04<00:03, 21.49 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 38/111 [00:04<00:03, 23.70 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 42/111 [00:04<00:02, 25.98 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/111 [00:04<00:02, 27.38 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 51/111 [00:04<00:02, 27.36 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 54/111 [00:04<00:02, 26.80 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 58/111 [00:04<00:01, 26.62 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 62/111 [00:05<00:01, 26.33 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 66/111 [00:05<00:01, 26.95 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 70/111 [00:05<00:01, 26.96 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 73/111 [00:05<00:01, 27.28 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 77/111 [00:05<00:01, 24.66 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 80/111 [00:05<00:01, 23.94 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 84/111 [00:05<00:01, 26.24 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 88/111 [00:06<00:00, 26.24 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 92/111 [00:06<00:00, 27.30 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 96/111 [00:06<00:00, 26.39 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 100/111 [00:06<00:00, 26.08 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 104/111 [00:06<00:00, 25.75 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 108/111 [00:06<00:00, 22.80 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:07<00:00, 22.50 examples/s]                                                             Saved results to: /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/AusKidTalk/eval_AusKidTalk_spontaneous_lowercase_20231002_finetuned_result.csv
--> Getting fine-tuned test results...
Fine-tuned Test WER: 0.737
Fine-tuned Test CER: 0.347


--> Showing some fine-tuned prediction errors...
                                         target_text                                           pred_str
0                        and then the boy felt sorry                         and then the boy felt ssoy
1  the green baby is holding the bottle of milk w...  the groun baebebers holding tha boblofe mlk wh...
2            a dinosaur jumps out and hulk is scared              i dindas sord grompsotand hk is scard
3                    so he ran and hid behind a tree                  sow he ran and hiaed behind a tre
4                                  the baby feel sad                               the babey ffis ssapp
5               he was sad that there was a dinosaur               he was sad that tharlie the dime sol
6                     he hears what's inside the egg                                  hhars winsid bead
7  the green baby is holding the bottle of milk a...  the gran babebersholdin the bol ofe mlk and hi...
8  the green baby is holding the bottle of milk w...  the groun baebe is hoedning the bolof mlk wh l...
9                  because the dinosaur looked funny                   bekasar din the soe lok to funny
--> Taking a deeper look...
[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] h h h [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] l l l [PAD] k k [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] w w a a a s | | | h [PAD] [PAD] [PAD] [PAD] [PAD] i i i d d t i i n g g | | | | b b [PAD] [PAD] [PAD] a a c c c [PAD] w w w w e d d s | | | o o n | | t t h e | | s s [PAD] [PAD] c c [PAD] [PAD] [PAD] [PAD] a [PAD] p [PAD] | | b b [PAD] [PAD] [PAD] o o o a a r t t t [PAD] | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] a n n d | h h e e | | h h h [PAD] i i [PAD] p p p p [PAD] [PAD] [PAD] s s [PAD] | | [PAD] a a n d | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] a g g g g [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 02/10/2023 14:20:57
