{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a681b-3c9b-4389-bc00-37cfdc7999f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "cache = '/srv/scratch/z5313567/thesis/cache'\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", cache_dir = cache)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", cache_dir = cache)\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\", cache_dir = cache)\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc289a6-4f4b-490c-b991-4b9dc5752406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 14/10/2023 16:54:43\n",
      "\n",
      "------> IMPORTING PACKAGES.... ---------------------------------------\n",
      "\n",
      "-->Importing datasets...\n",
      "-->Importing jiwer...\n",
      "-->Importing random...\n",
      "-->Importing pandas & numpy...\n",
      "-->Importing re...\n",
      "-->Importing json...\n",
      "-->Importing Wav2VecCTC...\n",
      "-->Importing soundfile...\n",
      "-->Importing librosa...\n",
      "-->Importing torch, dataclasses & typing...\n",
      "-->Importing from transformers for training...\n",
      "-->Importing pyarrow for loading dataset...\n",
      "-->SUCCESS! All packages imported.\n"
     ]
    }
   ],
   "source": [
    "# For accessing date and time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "# Print out dd/mm/YY H:M:S\n",
    "# ------------------------------------------\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started:\", dt_string)\n",
    "# ------------------------------------------ \n",
    "print(\"\\n------> IMPORTING PACKAGES.... ---------------------------------------\\n\")\n",
    "print(\"-->Importing datasets...\")\n",
    "# Import datasets and evaluation metric\n",
    "from datasets import load_dataset, load_metric, ClassLabel\n",
    "# Convert pandas dataframe to DatasetDict\n",
    "from datasets import Dataset\n",
    "# Generate word alignment files for OOV checking\n",
    "print(\"-->Importing jiwer...\")\n",
    "import jiwer\n",
    "# Generate random numbers\n",
    "print(\"-->Importing random...\")\n",
    "import random\n",
    "# Manipulate dataframes and numbers\n",
    "print(\"-->Importing pandas & numpy...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Use regex\n",
    "print(\"-->Importing re...\")\n",
    "import re\n",
    "# Read, Write, Open json files\n",
    "print(\"-->Importing json...\")\n",
    "import json\n",
    "# Use models and tokenizers\n",
    "print(\"-->Importing Wav2VecCTC...\")\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "#from transformers import AutoTokenizer\n",
    "from transformers import HubertForCTC\n",
    "#from transformers import AutoModelForCTC\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "#from transformers import AutoFeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "#from transformers import AutoProcessor\n",
    "# Loading audio files\n",
    "print(\"-->Importing soundfile...\")\n",
    "import soundfile as sf\n",
    "print(\"-->Importing librosa...\")\n",
    "import librosa\n",
    "# For training\n",
    "print(\"-->Importing torch, dataclasses & typing...\")\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "print(\"-->Importing from transformers for training...\")\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "print(\"-->Importing pyarrow for loading dataset...\")\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "print(\"-->SUCCESS! All packages imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d903c127-b290-4fa2-aa71-8847d17155a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 26/09/2023 15:53:52\n",
      "\n",
      "------> IMPORTING PACKAGES.... ---------------------------------------\n",
      "\n",
      "-->Importing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5313567/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->Importing random...\n",
      "-->Importing pandas & numpy...\n",
      "-->Importing re...\n",
      "-->Importing json...\n",
      "-->Importing Wav2VecCTC...\n",
      "-->Importing soundfile...\n",
      "-->Importing librosa...\n",
      "-->Importing torch, dataclasses & typing...\n",
      "-->Importing from transformers for training...\n",
      "-->Importing pyarrow for loading dataset...\n",
      "-->SUCCESS! All packages imported.\n",
      "\n",
      "------> EXPERIMENT ARGUMENTS ----------------------------------------- \n",
      "\n",
      "base_fp: /srv/scratch/z5313567/thesis/\n",
      "model: whisper\n",
      "dataset_name: AusKidTalk\n",
      "experiment_id: AusKidTalk_scripted_spontaneous_combined_finetune_20230925\n",
      "cache_name: AusKidTalk-finetune\n",
      "training: True\n",
      "use_checkpoint: False\n",
      "use_pretrained_tokenizer: False\n",
      "eval_pretrained: False\n",
      "baseline_model: openai/whisper-small\n",
      "eval_baseline: False\n",
      "\n",
      "------> MODEL ARGUMENTS... -------------------------------------------\n",
      "\n",
      "hidden_dropout: 0.1\n",
      "activation_dropout: 0.1\n",
      "attention_dropoutput: 0.1\n",
      "feat_proj_dropout: 0.0\n",
      "layerdrop: 0.01\n",
      "mask_time_prob: 0.075\n",
      "mask_time_length: 10\n",
      "ctc_loss_reduction: mean\n",
      "ctc_zero_infinity: True\n",
      "gradient_checkpointing: True\n",
      "\n",
      "------> TRAINING ARGUMENTS... ----------------------------------------\n",
      "\n",
      "evaluation strategy: steps\n",
      "per_device_train_batch_size: 8\n",
      "gradient_accumulation_steps: 1\n",
      "learning_rate: 5e-05\n",
      "weight_decay: 0.01\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.98\n",
      "adam_epsilon: 1e-08\n",
      "num_train_epochs: 590\n",
      "max_steps: 13000\n",
      "lr_scheduler_type: linear\n",
      "warmup_ratio: 0.1\n",
      "logging_strategy: steps\n",
      "logging_steps: 1000\n",
      "save_strategy: steps\n",
      "save_steps: 1000\n",
      "save_total_limit: 2\n",
      "fp16: True\n",
      "eval_steps: 1000\n",
      "load_best_model_at_end: True\n",
      "metric_for_best_model: wer\n",
      "greater_is_better: False\n",
      "group_by_length: True\n",
      "\n",
      "------> GENERATING FILEPATHS... --------------------------------------\n",
      "\n",
      "--> data_train_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_train_only_transcription_filepath.csv\n",
      "--> data_dev_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_dev_only_transcription_filepath.csv\n",
      "--> data_test_fp: /srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_test_only_transcription_filepath.csv\n",
      "--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune\n",
      "--> vocab_fp: /srv/scratch/z5313567/thesis/whisper/vocab/AusKidTalk/AusKidTalk_scripted_spontaneous_combined_finetune_20230925_vocab.json\n",
      "--> model_fp: /srv/scratch/z5313567/thesis/whisper/model/AusKidTalk/AusKidTalk_scripted_spontaneous_combined_finetune_20230925\n",
      "--> baseline_results_fp: /srv/scratch/z5313567/thesis/whisper/baseline_result/AusKidTalk/AusKidTalk_scripted_spontaneous_combined_finetune_20230925_baseline_result.csv\n",
      "--> finetuned_results_fp: /srv/scratch/z5313567/thesis/whisper/finetuned_result/AusKidTalk/AusKidTalk_scripted_spontaneous_combined_finetune_20230925_finetuned_result.csv\n",
      "--> pretrained_mod: openai/whisper-small\n",
      "\n",
      "------> PREPARING DATASET... ------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 521.70it/s]\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5c4139f027647cd7.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-31c051c82af50429.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c6b5bfa1a550cc22.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> dataset...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'transcription_clean'],\n",
      "        num_rows: 8093\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['filepath', 'transcription_clean'],\n",
      "        num_rows: 1744\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'transcription_clean'],\n",
      "        num_rows: 1774\n",
      "    })\n",
      "})\n",
      "--> Printing some random samples...\n",
      "                                            filepath transcription_clean\n",
      "0  /srv/scratch/chacmod/auskidtalk_audio/218_task...             breathe\n",
      "1  /srv/scratch/chacmod/auskidtalk_audio/255_task...          watermelon\n",
      "2  /srv/scratch/chacmod/auskidtalk_audio/191_task...              banana\n",
      "3  /srv/scratch/chacmod/auskidtalk_audio/203_task...               beard\n",
      "4  /srv/scratch/chacmod/auskidtalk_audio/304_task...                cart\n",
      "SUCCESS: Prepared dataset.\n",
      "\n",
      "------> PROCESSING TRANSCRIPTION... ---------------------------------------\n",
      "\n",
      "--> Creating map(...) function for vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Vocab len: 26 \n",
      " {' ': 0, 'H': 1, 'D': 2, 'M': 3, 'U': 4, 'E': 5, 'G': 6, 'L': 7, 'B': 8, 'C': 9, 'A': 10, 'P': 11, 'T': 12, 'F': 13, 'R': 14, 'J': 15, 'S': 16, 'N': 17, 'W': 18, 'O': 19, 'Y': 20, 'V': 21, 'X': 22, 'K': 23, 'I': 24, 'Z': 25}\n",
      "--> Vocab len: 28 \n",
      " {'H': 1, 'D': 2, 'M': 3, 'U': 4, 'E': 5, 'G': 6, 'L': 7, 'B': 8, 'C': 9, 'A': 10, 'P': 11, 'T': 12, 'F': 13, 'R': 14, 'J': 15, 'S': 16, 'N': 17, 'W': 18, 'O': 19, 'Y': 20, 'V': 21, 'X': 22, 'K': 23, 'I': 24, 'Z': 25, '|': 0, '[UNK]': 26, '[PAD]': 27}\n",
      "SUCCESS: Created vocabulary file at /srv/scratch/z5313567/thesis/whisper/vocab/AusKidTalk/AusKidTalk_scripted_spontaneous_combined_finetune_20230925_vocab.json\n",
      "\n",
      "------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1264cc2d36ff1043_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-96623961a2e346a0_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-3c3add9f60df9f04_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Created feature extractor.\n",
      "\n",
      "------> PRE-PROCESSING DATA... ----------------------------------------- \n",
      "\n",
      "--> Verifying data with a random sample...\n",
      "Target text: DUCK\n",
      "Input array shape: (7200,)\n",
      "Sampling rate: 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fd604e6d65394860_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-4095edb5699b8b71_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/AusKidTalk-finetune/csv/default-14c1ba9ea46bfb6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5b74132e5d537859_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Data ready for training and evaluation.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "#      Install packages if needed\n",
    "# ------------------------------------------\n",
    "#pip install datasets==1.8.0\n",
    "#pip install transformers\n",
    "#pip install soundfile\n",
    "#pip install jiwer\n",
    "\n",
    "# ------------------------------------------\n",
    "#       Import required packages\n",
    "# ------------------------------------------\n",
    "# For printing filepath\n",
    "import os\n",
    "# ------------------------------------------\n",
    "# ------------------------------------------\n",
    "# For accessing date and time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "# Print out dd/mm/YY H:M:S\n",
    "# ------------------------------------------\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started:\", dt_string)\n",
    "# ------------------------------------------ \n",
    "print(\"\\n------> IMPORTING PACKAGES.... ---------------------------------------\\n\")\n",
    "print(\"-->Importing datasets...\")\n",
    "# Import datasets and evaluation metric\n",
    "from datasets import load_dataset, load_metric, ClassLabel\n",
    "# Convert pandas dataframe to DatasetDict\n",
    "from datasets import Dataset\n",
    "# Generate random numbers\n",
    "print(\"-->Importing random...\")\n",
    "import random\n",
    "# Manipulate dataframes and numbers\n",
    "print(\"-->Importing pandas & numpy...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Use regex\n",
    "print(\"-->Importing re...\")\n",
    "import re\n",
    "# Read, Write, Open json files\n",
    "print(\"-->Importing json...\")\n",
    "import json\n",
    "# Use models and tokenizers\n",
    "print(\"-->Importing Wav2VecCTC...\")\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperProcessor\n",
    "# Loading audio files\n",
    "print(\"-->Importing soundfile...\")\n",
    "import soundfile as sf\n",
    "print(\"-->Importing librosa...\")\n",
    "import librosa\n",
    "# For training\n",
    "print(\"-->Importing torch, dataclasses & typing...\")\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "print(\"-->Importing from transformers for training...\")\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "print(\"-->Importing pyarrow for loading dataset...\")\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "print(\"-->SUCCESS! All packages imported.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "#      Setting experiment arguments\n",
    "# ------------------------------------------\n",
    "print(\"\\n------> EXPERIMENT ARGUMENTS ----------------------------------------- \\n\")\n",
    "\n",
    "base_fp = '/srv/scratch/z5313567/thesis/'\n",
    "print('base_fp:', base_fp)\n",
    "\n",
    "model = 'whisper'\n",
    "print('model:', model)\n",
    "\n",
    "dataset_name = 'AusKidTalk'\n",
    "print('dataset_name:', dataset_name)\n",
    "\n",
    "experiment_id = 'AusKidTalk_scripted_spontaneous_combined_finetune_20230925'\n",
    "print('experiment_id:', experiment_id)\n",
    "\n",
    "cache_name = 'AusKidTalk-finetune'\n",
    "print('cache_name:', cache_name)\n",
    "\n",
    "\n",
    "# Perform Training (True/False)\n",
    "# If false, this will go straight to model evaluation \n",
    "training = True\n",
    "print(\"training:\", training)\n",
    "\n",
    "# Resume training from/ use checkpoint (True/False)\n",
    "# Set to True for:\n",
    "# 1) resuming from a saved checkpoint if training stopped midway through\n",
    "# 2) for using an existing finetuned model for evaluation \n",
    "# If 2), then must also set eval_pretrained = True\n",
    "use_checkpoint = False\n",
    "print(\"use_checkpoint:\", use_checkpoint)\n",
    "\n",
    "# Set checkpoint if resuming from/using checkpoint\n",
    "#checkpoint = \"/srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/s5/myST-OGI_local/20210819-OGI-myST-120h\"\n",
    "checkpoint = \"/srv/scratch/z5313567/thesis/wav2vec2/model/Renee_myST_OGI_TLT/20211016-base-myST-OGI-TLT17/checkpoint-20000\"\n",
    "if use_checkpoint:\n",
    "    print(\"checkpoint:\", checkpoint)\n",
    "\n",
    "# Use a pretrained tokenizer (True/False)\n",
    "#     True: Use existing tokenizer (if custom dataset has same vocab)\n",
    "#     False: Use custom tokenizer (if custom dataset has different vocab)\n",
    "use_pretrained_tokenizer = False\n",
    "print(\"use_pretrained_tokenizer:\", use_pretrained_tokenizer)\n",
    "\n",
    "# Set tokenizer\n",
    "pretrained_tokenizer = \"facebook/wav2vec2-base-960h\"\n",
    "if use_pretrained_tokenizer:\n",
    "    print(\"pretrained_tokenizer:\", pretrained_tokenizer)\n",
    "\n",
    "# Evaluate existing model instead of newly trained model (True/False)\n",
    "#     True: use the model in the filepath set by 'eval_model' for eval\n",
    "#     False: use the model trained from this script for eval\n",
    "eval_pretrained = False\n",
    "print(\"eval_pretrained:\", eval_pretrained)\n",
    "\n",
    "# Set existing model to evaluate, if evaluating on existing model\n",
    "eval_model = checkpoint\n",
    "if eval_pretrained:\n",
    "    print(\"eval_model:\", eval_model)\n",
    "\n",
    "# Baseline model for evaluating baseline metric\n",
    "# This model will be evaluated at the end for the baseline WER\n",
    "baseline_model = \"openai/whisper-small\"\n",
    "print(\"baseline_model:\", baseline_model)\n",
    "\n",
    "# Evalulate the baseline model or not (True/False)\n",
    "#   True: evaluate baseline model on test set\n",
    "#   False: do not evaluate baseline model on test set\n",
    "eval_baseline = False\n",
    "print(\"eval_baseline:\", eval_baseline)\n",
    "\n",
    "\n",
    "print(\"\\n------> MODEL ARGUMENTS... -------------------------------------------\\n\")\n",
    "# For setting model = Wav2Vec2ForCTC.from_pretrained()\n",
    "\n",
    "set_hidden_dropout = 0.1                    # Default = 0.1\n",
    "print(\"hidden_dropout:\", set_hidden_dropout)\n",
    "set_activation_dropout = 0.1                # Default = 0.1\n",
    "print(\"activation_dropout:\", set_activation_dropout)\n",
    "set_attention_dropout = 0.1                 # Default = 0.1\n",
    "print(\"attention_dropoutput:\", set_attention_dropout)\n",
    "set_feat_proj_dropout = 0.0                 # Default = 0.1\n",
    "print(\"feat_proj_dropout:\", set_feat_proj_dropout)\n",
    "set_layerdrop = 0.01                         # Default = 0.1\n",
    "print(\"layerdrop:\", set_layerdrop)\n",
    "set_mask_time_prob = 0.075                  # Default = 0.05\n",
    "print(\"mask_time_prob:\", set_mask_time_prob)\n",
    "set_mask_time_length = 10                   # Default = 10\n",
    "print(\"mask_time_length:\", set_mask_time_length)\n",
    "set_ctc_loss_reduction = \"mean\"             # Default = \"sum\"\n",
    "print(\"ctc_loss_reduction:\", set_ctc_loss_reduction)\n",
    "set_ctc_zero_infinity = True               # Default = False\n",
    "print(\"ctc_zero_infinity:\", set_ctc_zero_infinity)\n",
    "set_gradient_checkpointing = True           # Default = False\n",
    "print(\"gradient_checkpointing:\", set_gradient_checkpointing)\n",
    "\n",
    "print(\"\\n------> TRAINING ARGUMENTS... ----------------------------------------\\n\")\n",
    "# For setting training_args = TrainingArguments()\n",
    "\n",
    "set_evaluation_strategy = \"steps\"           # Default = \"no\"\n",
    "print(\"evaluation strategy:\", set_evaluation_strategy)\n",
    "set_per_device_train_batch_size = 8         # Default = 8\n",
    "print(\"per_device_train_batch_size:\", set_per_device_train_batch_size)\n",
    "set_gradient_accumulation_steps = 1         # Default = 1\n",
    "print(\"gradient_accumulation_steps:\", set_gradient_accumulation_steps)\n",
    "set_learning_rate = 0.00005                 # Default = 0.00005\n",
    "print(\"learning_rate:\", set_learning_rate)\n",
    "set_weight_decay = 0.01                     # Default = 0\n",
    "print(\"weight_decay:\", set_weight_decay)\n",
    "set_adam_beta1 = 0.9                        # Default = 0.9\n",
    "print(\"adam_beta1:\", set_adam_beta1)\n",
    "set_adam_beta2 = 0.98                       # Default = 0.999\n",
    "print(\"adam_beta2:\", set_adam_beta2)\n",
    "set_adam_epsilon = 0.00000001               # Default = 0.00000001\n",
    "print(\"adam_epsilon:\", set_adam_epsilon)\n",
    "set_num_train_epochs = 590                   # Default = 3.0\n",
    "print(\"num_train_epochs:\", set_num_train_epochs)\n",
    "set_max_steps = 13000                          # Default = -1, overrides epochs\n",
    "print(\"max_steps:\", set_max_steps)\n",
    "set_lr_scheduler_type = \"linear\"            # Default = \"linear\"\n",
    "print(\"lr_scheduler_type:\", set_lr_scheduler_type )\n",
    "set_warmup_ratio = 0.1                      # Default = 0.0\n",
    "print(\"warmup_ratio:\", set_warmup_ratio)\n",
    "set_logging_strategy = \"steps\"              # Default = \"steps\"\n",
    "print(\"logging_strategy:\", set_logging_strategy)\n",
    "set_logging_steps = 1000                      # Default = 500\n",
    "print(\"logging_steps:\", set_logging_steps)\n",
    "set_save_strategy = \"steps\"                 # Default = \"steps\"\n",
    "print(\"save_strategy:\", set_save_strategy)\n",
    "set_save_steps = 1000                         # Default = 500\n",
    "print(\"save_steps:\", set_save_steps)\n",
    "set_save_total_limit = 2                   # Optional                 \n",
    "print(\"save_total_limit:\", set_save_total_limit)\n",
    "set_fp16 = True                             # Default = False\n",
    "print(\"fp16:\", set_fp16)\n",
    "set_eval_steps = 1000                         # Optional\n",
    "print(\"eval_steps:\", set_eval_steps)\n",
    "set_load_best_model_at_end = True           # Default = False\n",
    "print(\"load_best_model_at_end:\", set_load_best_model_at_end)\n",
    "set_metric_for_best_model = \"wer\"           # Optional\n",
    "print(\"metric_for_best_model:\", set_metric_for_best_model)\n",
    "set_greater_is_better = False               # Optional\n",
    "print(\"greater_is_better:\", set_greater_is_better)\n",
    "set_group_by_length = True                  # Default = False\n",
    "print(\"group_by_length:\", set_group_by_length)\n",
    "\n",
    "# ------------------------------------------\n",
    "#        Generating file paths\n",
    "# ------------------------------------------\n",
    "print(\"\\n------> GENERATING FILEPATHS... --------------------------------------\\n\")\n",
    "# Path to dataframe csv for train dataset\n",
    "# data_train_fp = base_fp + train_name + \"_local/\" + train_filename + \".csv\"\n",
    "data_train_fp = '/srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_train_only_transcription_filepath.csv'\n",
    "print(\"--> data_train_fp:\", data_train_fp)\n",
    "\n",
    "# Path to dataframe csv for test dataset\n",
    "data_dev_fp = '/srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_dev_only_transcription_filepath.csv'\n",
    "print(\"--> data_dev_fp:\", data_dev_fp)\n",
    "\n",
    "# Path to dataframe csv for test dataset\n",
    "#data_test_fp = base_fp + evaluation_name + \"_local/\" + evaluation_filename + \".csv\"\n",
    "data_test_fp = '/srv/scratch/z5313567/thesis/AusKidTalk_local/AusKidTalk_test_only_transcription_filepath.csv'\n",
    "print(\"--> data_test_fp:\", data_test_fp)\n",
    "\n",
    "# Dataframe file \n",
    "# |-----------|---------------------|----------|---------|\n",
    "# | file path | transcription_clean | duration | spkr_id |\n",
    "# |-----------|---------------------|----------|---------|\n",
    "# |   ...     |      ...            |  ..secs  | ......  |\n",
    "# |-----------|---------------------|----------|---------|\n",
    "# NOTE: The spkr_id column may need to be removed beforehand if\n",
    "#       there appears to be a mixture between numerical and string ID's\n",
    "#       due to this issue: https://github.com/apache/arrow/issues/4168\n",
    "#       when calling load_dataset()\n",
    "\n",
    "# Path to datasets cache\n",
    "# data_cache_fp = base_cache_fp + datasetdict_id\n",
    "data_cache_fp = '/srv/scratch/chacmod/.cache/huggingface/datasets/' + cache_name\n",
    "print(\"--> data_cache_fp:\", data_cache_fp)\n",
    "\n",
    "# Path to save vocab.json\n",
    "# vocab_fp = base_fp + train_name + \"_local/vocab_\" + experiment_id + \".json\"\n",
    "vocab_fp =  base_fp + model + '/vocab/' + dataset_name + '/' + experiment_id + '_vocab.json'\n",
    "print(\"--> vocab_fp:\", vocab_fp)\n",
    "\n",
    "# Path to save model output\n",
    "#model_fp = base_fp + train_name + \"_local/\" + experiment_id\n",
    "model_fp = base_fp + model + '/model/' + dataset_name + '/' + experiment_id\n",
    "print(\"--> model_fp:\", model_fp)\n",
    "\n",
    "# Path to save results output\n",
    "# baseline_results_fp = base_fp + train_name + \"_local/\" + experiment_id + \"_baseline_results.csv\" \n",
    "baseline_results_fp = base_fp + model + '/baseline_result/' + dataset_name + '/'  + experiment_id + '_baseline_result.csv'\n",
    "print(\"--> baseline_results_fp:\", baseline_results_fp)\n",
    "\n",
    "# finetuned_results_fp = base_fp + train_name + \"_local/\" + experiment_id + \"_finetuned_results.csv\"\n",
    "finetuned_results_fp = base_fp + model + '/finetuned_result/' + dataset_name + '/'  + experiment_id + '_finetuned_result.csv'\n",
    "print(\"--> finetuned_results_fp:\", finetuned_results_fp)\n",
    "\n",
    "# Pre-trained checkpoint model\n",
    "# For 1) Fine-tuning or\n",
    "#     2) resuming training from pre-trained model\n",
    "# If 1) must set use_checkpoint = False\n",
    "# If 2)must set use_checkpoint = True\n",
    "# Default model to fine-tune is facebook's model\n",
    "pretrained_mod = \"openai/whisper-small\"\n",
    "if use_checkpoint:\n",
    "    pretrained_mod = checkpoint\n",
    "print(\"--> pretrained_mod:\", pretrained_mod)\n",
    "# Path to pre-trained tokenizer\n",
    "# If use_pretrained_tokenizer = True\n",
    "if use_pretrained_tokenizer:\n",
    "    print(\"--> pretrained_tokenizer:\", pretrained_tokenizer)\n",
    "\n",
    "# ------------------------------------------\n",
    "#         Preparing dataset\n",
    "# ------------------------------------------\n",
    "# Run the following scripts to prepare data\n",
    "# 1) Prepare data from kaldi file: \n",
    "# /srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/s5/wav2vec_exp/data_prep.py\n",
    "# 3) [Optional] Limit the files to certain duration:\n",
    "# /srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/s5/wav2vec_projects/data_getShortWavs.py\n",
    "# 2) Split data into train and test:\n",
    "# /srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/s5/wav2vec_projects/data_split.py\n",
    "\n",
    "print(\"\\n------> PREPARING DATASET... ------------------------------------\\n\")\n",
    "# Read the existing csv saved dataframes and\n",
    "# load as a DatasetDict \n",
    "data = load_dataset('csv', \n",
    "                    data_files={'train': data_train_fp,\n",
    "                                'dev' : data_dev_fp,\n",
    "                                'test': data_test_fp},\n",
    "                    cache_dir=data_cache_fp)\n",
    "# Remove the \"duration\" and \"spkr_id\" column\n",
    "#data = data.remove_columns([\"duration\", \"spkr_id\"])\n",
    "#data = data.remove_columns([\"duration\"])\n",
    "print(\"--> dataset...\")\n",
    "print(data)\n",
    "# Display some random samples of the dataset\n",
    "print(\"--> Printing some random samples...\")\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Picking more elements than in dataset\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    print(df)\n",
    "show_random_elements(data[\"train\"], num_examples=5)\n",
    "print(\"SUCCESS: Prepared dataset.\")\n",
    "# ------------------------------------------\n",
    "#       Processing transcription\n",
    "# ------------------------------------------\n",
    "# Create vocab.json\n",
    "# Extracting all distinct letters of train and test set\n",
    "# and building vocab from this set of letters\n",
    "print(\"\\n------> PROCESSING TRANSCRIPTION... ---------------------------------------\\n\")\n",
    "# Mapping function that concatenates all transcriptions\n",
    "# into one long transcription and then transforms the\n",
    "# string into a set of chars. Set batched=True to the \n",
    "# map(...) function so that the mapping function has access\n",
    "# to all transcriptions at once.\n",
    "\n",
    "#chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def process_transcription(batch):\n",
    "    #batch[\"transcription_clean\"] = re.sub(chars_to_ignore_regex, '', batch[\"transcription_clean\"]).upper()\n",
    "    batch[\"transcription_clean\"] = batch[\"transcription_clean\"].upper()\n",
    "    batch[\"transcription_clean\"] = batch[\"transcription_clean\"].replace(\"<UNK>\", \"<unk>\")\n",
    "    return batch\n",
    "\n",
    "data = data.map(process_transcription)\n",
    "\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"transcription_clean\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "    \n",
    "if not use_pretrained_tokenizer:\n",
    "    print(\"--> Creating map(...) function for vocab...\")\n",
    "    vocabs = data.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data.column_names[\"train\"])\n",
    "    # Create union of all distinct letters in train and test set\n",
    "    # and convert resulting list into enumerated dictionary\n",
    "    # Vocab includes a-z, ' , space, UNK, PAD\n",
    "    vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "    print(\"--> Vocab len:\", len(vocab_dict), \"\\n\", vocab_dict)\n",
    "    # Give space \" \" a visible character \" | \"\n",
    "    # Include \"unknown\" [UNK] token for dealing with characters\n",
    "    # not encountered in training.\n",
    "    # Add padding token to corresponds to CTC's \"blank token\".\n",
    "    vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "    del vocab_dict[\" \"]\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    print(\"--> Vocab len:\", len(vocab_dict), \"\\n\", vocab_dict)\n",
    "    # Save vocab as a json file\n",
    "    with open(vocab_fp, 'w') as vocab_file:\n",
    "        json.dump(vocab_dict, vocab_file)\n",
    "    print(\"SUCCESS: Created vocabulary file at\", vocab_fp)\n",
    "# Use json file to instantiate an object of the \n",
    "# Wav2VecCTCTokenziser class if not using pretrained tokenizer\n",
    "if use_pretrained_tokenizer:\n",
    "    #tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(pretrained_tokenizer)\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(pretrained_tokenizer)\n",
    "else:\n",
    "    #tokenizer = Wav2Vec2CTCTokenizer(vocab_fp, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"english\", task=\"transcribe\")\n",
    "#tokenizer = save_pretrained(model_fp)\n",
    "# ------------------------------------------\n",
    "#    Create Wav2Vec2 Feature Extractor\n",
    "# ------------------------------------------\n",
    "print(\"\\n------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------\\n\")\n",
    "# Instantiate a Wav2Vec2 feature extractor:\n",
    "# - feature_size: set to 1 because model was trained on raw speech signal\n",
    "# - sampling_rate: sampling rate the model is trained on\n",
    "# - padding_value: for batched inference, shorter inputs are padded\n",
    "# - do_normalize: whether input should be zero-mean-unit-variance\n",
    "#   normalised or not. Usually, speech models perform better when true.\n",
    "# - return_attention_mask: set to false for Wav2Vec2, but true for\n",
    "#   fine-tuning large-lv60\n",
    "#feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "# Feature extractor and tokenizer wrapped into a single\n",
    "# Wav2Vec2Processor class so we only need a model and processor object\n",
    "# processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"english\", task=\"transcribe\")\n",
    "# Save to re-use the just created processor and the fine-tuned model\n",
    "processor.save_pretrained(model_fp)\n",
    "print(\"SUCCESS: Created feature extractor.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "#             Pre-process Data\n",
    "# ------------------------------------------\n",
    "print(\"\\n------> PRE-PROCESSING DATA... ----------------------------------------- \\n\")\n",
    "# Audio files are stored as .wav format\n",
    "# We want to store both audio values and sampling rate\n",
    "# in the dataset. \n",
    "# We write a map(...) function accordingly.\n",
    "\n",
    "# def speech_file_to_array_fn(batch):\n",
    "#    speech_array, sampling_rate = sf.read(batch[\"filepath\"])\n",
    "#    batch[\"speech\"] = speech_array\n",
    "#    batch[\"sampling_rate\"] = sampling_rate\n",
    "#    batch[\"target_text\"] = batch[\"transcription_clean\"]\n",
    "#    return batch\n",
    "def speech_file_to_array_fn(batch):\n",
    "    #speech_array, sampling_rate = sf.read(batch[\"filepath\"])\n",
    "    speech_array, sampling_rate = librosa.load(batch['filepath'], sr=feature_extractor.sampling_rate)\n",
    "    batch[\"speech\"] = speech_array\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"transcription_clean\"]\n",
    "    return batch\n",
    "\n",
    "data = data.map(speech_file_to_array_fn, remove_columns=data.column_names[\"train\"], num_proc=4)\n",
    "# Check a few rows of data to verify data properly loaded\n",
    "print(\"--> Verifying data with a random sample...\")\n",
    "rand_int = random.randint(0, len(data[\"train\"])-1)\n",
    "print(\"Target text:\", data[\"train\"][rand_int][\"target_text\"])\n",
    "print(\"Input array shape:\", np.asarray(data[\"train\"][rand_int][\"speech\"]).shape)\n",
    "print(\"Sampling rate:\", data[\"train\"][rand_int][\"sampling_rate\"])\n",
    "# Process dataset to the format expected by model for training\n",
    "# Using map(...)\n",
    "# 1) Check all data samples have same sampling rate (16kHz)\n",
    "# 2) Extract input_values from loaded audio file.\n",
    "#    This only involves normalisation but could also correspond\n",
    "#    to extracting log-mel features\n",
    "# 3) Encode the transcriptions to label ids\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "    # # check that all files have the correct sampling rate\n",
    "    # assert (\n",
    "        # len(set(batch[\"sampling_rate\"])) == 1\n",
    "    # ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    # batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "\n",
    "    # with processor.as_target_processor():\n",
    "        # batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    # return batch\n",
    "def prepare_dataset(batch):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "    \n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_features\n",
    "    \n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"target_text\"]).input_ids\n",
    "    return batch\n",
    "data_prepared = data.map(prepare_dataset, remove_columns=data.column_names[\"train\"], batch_size=8, num_proc=4, batched=True)\n",
    "\n",
    "print(\"SUCCESS: Data ready for training and evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f736baa2-5a71-4eed-833b-9526c72988bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8093"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_prepared['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043fbc25-9544-4fab-9af0-63a05f7da6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepared['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c77ea15-1fec-458e-98cf-808257b493e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5313567/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Pre-trained checkpoint loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperModel(\n",
       "  (encoder): WhisperEncoder(\n",
       "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (embed_positions): Embedding(1500, 384)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x WhisperEncoderLayer(\n",
       "        (self_attn): WhisperAttention(\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): WhisperDecoder(\n",
       "    (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "    (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x WhisperDecoderLayer(\n",
       "        (self_attn): WhisperAttention(\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): WhisperAttention(\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "pretrained_mod = \"openai/whisper-tiny\"\n",
    "model_cache_fp = '/srv/scratch/z5313567/thesis/cache'\n",
    "model = WhisperForConditionalGeneration.from_pretrained(pretrained_mod, cache_dir=model_cache_fp)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"SUCCESS: Pre-trained checkpoint loaded.\")\n",
    "\n",
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84f39f2-f3a5-4e53-bd24-7554f9a3bd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = model.base_model.encoder\n",
    "len(encoder.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef048dde-88a4-4c37-9517-9130beaa590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Pre-trained checkpoint loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "pretrained_mod = \"facebook/wav2vec2-base-960h\"\n",
    "model_cache_fp = '/srv/scratch/z5313567/thesis/cache'\n",
    "model = Wav2Vec2ForCTC.from_pretrained(pretrained_mod, cache_dir=model_cache_fp)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"SUCCESS: Pre-trained checkpoint loaded.\")\n",
    "\n",
    "model.base_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d83ec9c-268d-40e0-b95a-66ff9a43afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5313567/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing lower 3 transformer layers\n",
      "SUCCESS: Pre-trained checkpoint loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import HubertForCTC\n",
    "pretrained_mod = \"facebook/hubert-large-ls960-ft\"\n",
    "model_cache_fp = '/srv/scratch/z5313567/thesis/cache'\n",
    "model = HubertForCTC.from_pretrained(pretrained_mod, cache_dir=model_cache_fp)\n",
    "\n",
    "#for param in model.base_model.parameters():\n",
    "#    param.requires_grad = False\n",
    "#print(\"SUCCESS: Pre-trained checkpoint loaded.\")\n",
    "num_freeze_lower_layer = 3\n",
    "print(f\"Freezing lower {num_freeze_lower_layer} transformer layers\")\n",
    "for n in range(num_freeze_lower_layer):\n",
    "    for param in model.base_model.encoder.layers[n].parameters():\n",
    "         param.requires_grad = False\n",
    "print(\"SUCCESS: Pre-trained checkpoint loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f72e2204-4735-4bf2-8944-92bb8537018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache_fp = '/srv/scratch/z5313567/thesis/cache'\n",
    "from transformers import WhisperProcessor\n",
    "pretrained_tokenizer = \"openai/whisper-medium\"\n",
    "#pretrained_tokenizer = '/srv/scratch/z5313567/thesis/whisper/model/CU/whisper_medium_finetune_CU_lowercase_20231017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15c20c52-da93-48ae-a294-b64dc187635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(pretrained_tokenizer, language=\"english\", task=\"transcribe\", cache_dir=model_cache_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da78688-e355-4737-b826-e303ee2df90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebccfd7c-f1f9-48b0-9ec6-4269ca69d5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperProcessor:\n",
       "- feature_extractor: WhisperFeatureExtractor {\n",
       "  \"chunk_length\": 30,\n",
       "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
       "  \"feature_size\": 80,\n",
       "  \"hop_length\": 160,\n",
       "  \"n_fft\": 400,\n",
       "  \"n_samples\": 480000,\n",
       "  \"nb_max_frames\": 3000,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"WhisperProcessor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: WhisperTokenizer(name_or_path='openai/whisper-medium', vocab_size=50258, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|startoftranscript|>', '<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>', '<|pl|>', '<|ca|>', '<|nl|>', '<|ar|>', '<|sv|>', '<|it|>', '<|id|>', '<|hi|>', '<|fi|>', '<|vi|>', '<|he|>', '<|uk|>', '<|el|>', '<|ms|>', '<|cs|>', '<|ro|>', '<|da|>', '<|hu|>', '<|ta|>', '<|no|>', '<|th|>', '<|ur|>', '<|hr|>', '<|bg|>', '<|lt|>', '<|la|>', '<|mi|>', '<|ml|>', '<|cy|>', '<|sk|>', '<|te|>', '<|fa|>', '<|lv|>', '<|bn|>', '<|sr|>', '<|az|>', '<|sl|>', '<|kn|>', '<|et|>', '<|mk|>', '<|br|>', '<|eu|>', '<|is|>', '<|hy|>', '<|ne|>', '<|mn|>', '<|bs|>', '<|kk|>', '<|sq|>', '<|sw|>', '<|gl|>', '<|mr|>', '<|pa|>', '<|si|>', '<|km|>', '<|sn|>', '<|yo|>', '<|so|>', '<|af|>', '<|oc|>', '<|ka|>', '<|be|>', '<|tg|>', '<|sd|>', '<|gu|>', '<|am|>', '<|yi|>', '<|lo|>', '<|uz|>', '<|fo|>', '<|ht|>', '<|ps|>', '<|tk|>', '<|nn|>', '<|mt|>', '<|sa|>', '<|lb|>', '<|my|>', '<|bo|>', '<|tl|>', '<|mg|>', '<|as|>', '<|tt|>', '<|haw|>', '<|ln|>', '<|ha|>', '<|ba|>', '<|jw|>', '<|su|>', '<|translate|>', '<|transcribe|>', '<|startoflm|>', '<|startofprev|>', '<|nocaptions|>', '<|notimestamps|>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2b951ff-b846-432a-85c4-13cec0e5dd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperProcessor:\n",
       "- feature_extractor: WhisperFeatureExtractor {\n",
       "  \"chunk_length\": 30,\n",
       "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
       "  \"feature_size\": 80,\n",
       "  \"hop_length\": 160,\n",
       "  \"n_fft\": 400,\n",
       "  \"n_samples\": 480000,\n",
       "  \"nb_max_frames\": 3000,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"WhisperProcessor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: WhisperTokenizer(name_or_path='/srv/scratch/z5313567/thesis/whisper/model/CU/whisper_medium_finetune_CU_lowercase_20231017', vocab_size=50258, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|startoftranscript|>', '<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>', '<|pl|>', '<|ca|>', '<|nl|>', '<|ar|>', '<|sv|>', '<|it|>', '<|id|>', '<|hi|>', '<|fi|>', '<|vi|>', '<|he|>', '<|uk|>', '<|el|>', '<|ms|>', '<|cs|>', '<|ro|>', '<|da|>', '<|hu|>', '<|ta|>', '<|no|>', '<|th|>', '<|ur|>', '<|hr|>', '<|bg|>', '<|lt|>', '<|la|>', '<|mi|>', '<|ml|>', '<|cy|>', '<|sk|>', '<|te|>', '<|fa|>', '<|lv|>', '<|bn|>', '<|sr|>', '<|az|>', '<|sl|>', '<|kn|>', '<|et|>', '<|mk|>', '<|br|>', '<|eu|>', '<|is|>', '<|hy|>', '<|ne|>', '<|mn|>', '<|bs|>', '<|kk|>', '<|sq|>', '<|sw|>', '<|gl|>', '<|mr|>', '<|pa|>', '<|si|>', '<|km|>', '<|sn|>', '<|yo|>', '<|so|>', '<|af|>', '<|oc|>', '<|ka|>', '<|be|>', '<|tg|>', '<|sd|>', '<|gu|>', '<|am|>', '<|yi|>', '<|lo|>', '<|uz|>', '<|fo|>', '<|ht|>', '<|ps|>', '<|tk|>', '<|nn|>', '<|mt|>', '<|sa|>', '<|lb|>', '<|my|>', '<|bo|>', '<|tl|>', '<|mg|>', '<|as|>', '<|tt|>', '<|haw|>', '<|ln|>', '<|ha|>', '<|ba|>', '<|jw|>', '<|su|>', '<|translate|>', '<|transcribe|>', '<|startoflm|>', '<|startofprev|>', '<|nocaptions|>', '<|notimestamps|>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cache_fp = '/srv/scratch/z5313567/thesis/cache'\n",
    "from transformers import WhisperProcessor\n",
    "#pretrained_tokenizer = \"openai/whisper-tiny\"\n",
    "pretrained_tokenizer = '/srv/scratch/z5313567/thesis/whisper/model/CU/whisper_medium_finetune_CU_lowercase_20231017'\n",
    "processor = WhisperProcessor.from_pretrained(pretrained_tokenizer, language=\"english\", task=\"transcribe\", cache_dir=model_cache_fp)\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddb45f80-ff4d-4f22-81b5-cb19fec34e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperTokenizer(name_or_path='openai/whisper-medium', vocab_size=50258, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|startoftranscript|>', '<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>', '<|pl|>', '<|ca|>', '<|nl|>', '<|ar|>', '<|sv|>', '<|it|>', '<|id|>', '<|hi|>', '<|fi|>', '<|vi|>', '<|he|>', '<|uk|>', '<|el|>', '<|ms|>', '<|cs|>', '<|ro|>', '<|da|>', '<|hu|>', '<|ta|>', '<|no|>', '<|th|>', '<|ur|>', '<|hr|>', '<|bg|>', '<|lt|>', '<|la|>', '<|mi|>', '<|ml|>', '<|cy|>', '<|sk|>', '<|te|>', '<|fa|>', '<|lv|>', '<|bn|>', '<|sr|>', '<|az|>', '<|sl|>', '<|kn|>', '<|et|>', '<|mk|>', '<|br|>', '<|eu|>', '<|is|>', '<|hy|>', '<|ne|>', '<|mn|>', '<|bs|>', '<|kk|>', '<|sq|>', '<|sw|>', '<|gl|>', '<|mr|>', '<|pa|>', '<|si|>', '<|km|>', '<|sn|>', '<|yo|>', '<|so|>', '<|af|>', '<|oc|>', '<|ka|>', '<|be|>', '<|tg|>', '<|sd|>', '<|gu|>', '<|am|>', '<|yi|>', '<|lo|>', '<|uz|>', '<|fo|>', '<|ht|>', '<|ps|>', '<|tk|>', '<|nn|>', '<|mt|>', '<|sa|>', '<|lb|>', '<|my|>', '<|bo|>', '<|tl|>', '<|mg|>', '<|as|>', '<|tt|>', '<|haw|>', '<|ln|>', '<|ha|>', '<|ba|>', '<|jw|>', '<|su|>', '<|translate|>', '<|transcribe|>', '<|startoflm|>', '<|startofprev|>', '<|nocaptions|>', '<|notimestamps|>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "pretrained_tokenizer = \"openai/whisper-medium\"\n",
    "tokenizer = WhisperTokenizer.from_pretrained(pretrained_tokenizer, language=\"english\", task=\"transcribe\", cache_dir=model_cache_fp)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e62f4c63-74a1-45c8-ba3f-6afb541e60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "#pretrained_tokenizer = \"openai/whisper-medium\"\n",
    "pretrained_tokenizer = '/srv/scratch/z5313567/thesis/whisper/model/CU/whisper_medium_finetune_CU_lowercase_20231017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc09a577-65ad-4a44-b8ce-1815e9e3477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(pretrained_tokenizer, language=\"english\", task=\"transcribe\", cache_dir=model_cache_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "873d46d9-9608-4c89-b5ca-15eae84ee20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperTokenizer(name_or_path='/srv/scratch/z5313567/thesis/whisper/model/CU/whisper_medium_finetune_CU_lowercase_20231017', vocab_size=50258, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|startoftranscript|>', '<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>', '<|pl|>', '<|ca|>', '<|nl|>', '<|ar|>', '<|sv|>', '<|it|>', '<|id|>', '<|hi|>', '<|fi|>', '<|vi|>', '<|he|>', '<|uk|>', '<|el|>', '<|ms|>', '<|cs|>', '<|ro|>', '<|da|>', '<|hu|>', '<|ta|>', '<|no|>', '<|th|>', '<|ur|>', '<|hr|>', '<|bg|>', '<|lt|>', '<|la|>', '<|mi|>', '<|ml|>', '<|cy|>', '<|sk|>', '<|te|>', '<|fa|>', '<|lv|>', '<|bn|>', '<|sr|>', '<|az|>', '<|sl|>', '<|kn|>', '<|et|>', '<|mk|>', '<|br|>', '<|eu|>', '<|is|>', '<|hy|>', '<|ne|>', '<|mn|>', '<|bs|>', '<|kk|>', '<|sq|>', '<|sw|>', '<|gl|>', '<|mr|>', '<|pa|>', '<|si|>', '<|km|>', '<|sn|>', '<|yo|>', '<|so|>', '<|af|>', '<|oc|>', '<|ka|>', '<|be|>', '<|tg|>', '<|sd|>', '<|gu|>', '<|am|>', '<|yi|>', '<|lo|>', '<|uz|>', '<|fo|>', '<|ht|>', '<|ps|>', '<|tk|>', '<|nn|>', '<|mt|>', '<|sa|>', '<|lb|>', '<|my|>', '<|bo|>', '<|tl|>', '<|mg|>', '<|as|>', '<|tt|>', '<|haw|>', '<|ln|>', '<|ha|>', '<|ba|>', '<|jw|>', '<|su|>', '<|translate|>', '<|transcribe|>', '<|startoflm|>', '<|startofprev|>', '<|nocaptions|>', '<|notimestamps|>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0429f-6b6c-48ca-ba44-ce5783fc2fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaEnv3",
   "language": "python",
   "name": "condaenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
