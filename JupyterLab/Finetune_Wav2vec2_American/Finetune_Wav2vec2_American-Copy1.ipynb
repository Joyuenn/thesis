{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d319d-f9bb-4214-9066-bcad64e28313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a968b3a5-779c-4d65-91e1-f68871dfca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.29.2 in /home/z5313567/.local/lib/python3.10/site-packages (4.29.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (23.0)\n",
      "Requirement already satisfied: requests in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (4.64.1)\n",
      "Requirement already satisfied: filelock in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/python-3.10.8-pmtwsrrmcmrs6olvgx5xhepgh7gl5vro/lib/python3.10/site-packages (from transformers==4.29.2) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (1.24.2)\n",
      "Requirement already satisfied: fsspec in /home/z5313567/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/ood-jupyterlab-3.4.8-3qopzgrtpuwifxuotyxofkeze52d6ra7/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers==4.29.2\n",
    "pip install transformers==4.29.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f48c9b-d836-4ec8-b06f-4fdb5e9480c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers==4.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518ac92-56cb-4b5f-8594-f6ffcec7862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407fda3-095e-4e59-9863-e04b33673d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5030e-27c6-4906-b7d2-d49058f60fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c977ee-0319-421b-a2ef-322e31e01d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1032c-4525-4d85-b58d-8807588a809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/22816\n",
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb0c31-2f79-47dc-9fbe-76554b32e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/22816\n",
    "# please do pip install git+https://github.com/huggingface/accelerate to install the dev version, or pip install accelerate -U if you are not using multi-GPUs (such as in Colab).\n",
    "pip install git+https://github.com/huggingface/accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642919d-bbb5-4ee1-a23a-7b9d97cd1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/22816\n",
    "# please do pip install git+https://github.com/huggingface/accelerate to install the dev version, or pip install accelerate -U if you are not using multi-GPUs (such as in Colab).\n",
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ccbbc-8641-47b1-a115-bd98af0171ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: \n",
    "#     https://huggingface.co/blog/fine-tune-wav2vec2-english\n",
    "#     https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb#scrollTo=e7cqAWIayn6w\n",
    "#     https://github.com/monomest/Thesis/blob/3a15f747dfd934535ffb7a02bf3fee97d9c546cb/s5/wav2vec_projects/THESIS_C/run_finetune_kids_OGI.py#L239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd68f7e2-edb5-4a3f-9213-75a2435a63de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 14/06/2023 22:14:09\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "# Print out dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started:\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56590245-4ce2-4076-9b50-a8b34592434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------ Importing libraries... ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------ Importing libraries... ------------\\n\")\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa2990e-1a36-45d6-bb06-a6ab0da555bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Wav2Vec2ForCTC.from_pretrained('/srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/model_OGI_American_20230528')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d6f080-0799-4eda-8e28-7205e55f431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.0.1\n",
      "json version: 2.0.9\n",
      "librosa version: 0.10.0.post2\n",
      "Numpy version: 1.24.2\n",
      "Transformers version: 4.29.2\n",
      "Torch version: 1.13.1+cu117\n",
      "Test cuda_device_count 0\n",
      "Test cuda_is_available False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest cuda_device_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest cuda_is_available\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest get_device_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib/python3.10/site-packages/torch/cuda/__init__.py:341\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib/python3.10/site-packages/torch/cuda/__init__.py:371\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib/python3.10/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"json version:\", json.__version__)\n",
    "print(\"librosa version:\", librosa.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Test cuda_device_count\", torch.cuda.device_count())\n",
    "print(\"Test cuda_is_available\", torch.cuda.is_available())\n",
    "print(\"Test get_device_name\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1e2365-06a2-43b8-910f-43cf84490f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Model arguments... ------------------\n",
      "\n",
      "hidden_dropout: 0.1\n",
      "activation_dropout: 0.1\n",
      "attention_dropout: 0.1\n",
      "feat_proj_dropout: 0.0\n",
      "layerdrop: 0.05\n",
      "mask_time_prob: 0.065\n",
      "mask_time_length: 10\n",
      "ctc_loss_reduction: mean\n",
      "ctc_zero_infinity: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Model arguments... ------------------\\n\")\n",
    "# For setting model = Wav2Vec2ForCTC.from_pretrained()\n",
    "\n",
    "set_hidden_dropout = 0.1                    # Default = 0.1\n",
    "print(\"hidden_dropout:\", set_hidden_dropout)\n",
    "set_activation_dropout = 0.1                # Default = 0.1\n",
    "print(\"activation_dropout:\", set_activation_dropout)\n",
    "set_attention_dropout = 0.1                 # Default = 0.1\n",
    "print(\"attention_dropout:\", set_attention_dropout)\n",
    "set_feat_proj_dropout = 0.0                 # Default = 0.1\n",
    "print(\"feat_proj_dropout:\", set_feat_proj_dropout)\n",
    "set_layerdrop = 0.05                        # Default = 0.1\n",
    "print(\"layerdrop:\", set_layerdrop)\n",
    "set_mask_time_prob = 0.065                  # Default = 0.05\n",
    "print(\"mask_time_prob:\", set_mask_time_prob)\n",
    "set_mask_time_length = 10                   # Default = 10\n",
    "print(\"mask_time_length:\", set_mask_time_length)\n",
    "set_ctc_loss_reduction = \"mean\"             # Default = \"sum\"\n",
    "print(\"ctc_loss_reduction:\", set_ctc_loss_reduction)\n",
    "set_ctc_zero_infinity = True               # Default = False\n",
    "print(\"ctc_zero_infinity:\", set_ctc_zero_infinity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7208b81-c363-4d3f-8689-c141adadae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Training arguments... ------------------\n",
      "\n",
      "per_device_train_batch_size: 8\n",
      "group_by_length: True\n",
      "gradient_accumulation_steps: 1\n",
      "gradient_checkpointing: True\n",
      "weight_decay: 0.01\n",
      "fp16: True\n",
      "learning_rate: 4e-05\n",
      "lr_scheduler_type: linear\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.98\n",
      "adam_epsilon: 1e-08\n",
      "warmup_ratio: 0.1\n",
      "num_train_epochs: 22\n",
      "max_steps: 35000\n",
      "logging_strategy: steps\n",
      "logging_steps: 1000\n",
      "save_strategy: steps\n",
      "save_steps: 1000\n",
      "evaluation_strategy: steps\n",
      "eval_steps: 1000\n",
      "save_total_limit: 40\n",
      "load_best_model_at_end: True\n",
      "metric_for_best_model: wer\n",
      "greater_is_better: False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Training arguments... ------------------\\n\")\n",
    "# For setting training_args = TrainingArguments()\n",
    "\n",
    "set_per_device_train_batch_size = 8         # Default = 8\n",
    "print(\"per_device_train_batch_size:\", set_per_device_train_batch_size)\n",
    "set_group_by_length = True                  # Default = False\n",
    "print(\"group_by_length:\", set_group_by_length)\n",
    "set_gradient_accumulation_steps = 1         # Default = 1\n",
    "print(\"gradient_accumulation_steps:\", set_gradient_accumulation_steps)\n",
    "set_gradient_checkpointing = True           # Default = False\n",
    "print(\"gradient_checkpointing:\", set_gradient_checkpointing)\n",
    "set_weight_decay = 0.01                     # Default = 0\n",
    "print(\"weight_decay:\", set_weight_decay)\n",
    "set_fp16 = True                             # Default = False\n",
    "print(\"fp16:\", set_fp16)\n",
    "\n",
    "set_learning_rate = 0.00004                 # Default = 0.00005\n",
    "print(\"learning_rate:\", set_learning_rate)\n",
    "set_lr_scheduler_type = \"linear\"            # Default = \"linear\"\n",
    "print(\"lr_scheduler_type:\", set_lr_scheduler_type )\n",
    "set_adam_beta1 = 0.9                        # Default = 0.9\n",
    "print(\"adam_beta1:\", set_adam_beta1)\n",
    "set_adam_beta2 = 0.98                       # Default = 0.999\n",
    "print(\"adam_beta2:\", set_adam_beta2)\n",
    "set_adam_epsilon = 0.00000001               # Default = 0.00000001\n",
    "print(\"adam_epsilon:\", set_adam_epsilon)\n",
    "set_warmup_ratio = 0.1                      # Default = 0.0\n",
    "print(\"warmup_ratio:\", set_warmup_ratio)\n",
    "\n",
    "set_num_train_epochs = 22                 # Default = 3.0\n",
    "print(\"num_train_epochs:\", set_num_train_epochs)\n",
    "set_max_steps = 35000                       # Default = -1, overrides epochs\n",
    "print(\"max_steps:\", set_max_steps)\n",
    "\n",
    "set_logging_strategy = \"steps\"              # Default = \"steps\"\n",
    "print(\"logging_strategy:\", set_logging_strategy)\n",
    "set_logging_steps = 1000                      # Default = 500\n",
    "print(\"logging_steps:\", set_logging_steps)\n",
    "set_save_strategy = \"steps\"                 # Default = \"steps\"\n",
    "print(\"save_strategy:\", set_save_strategy)\n",
    "set_save_steps = 1000                         # Default = 500\n",
    "print(\"save_steps:\", set_save_steps)\n",
    "set_evaluation_strategy = \"steps\"           # Default = \"no\"\n",
    "print(\"evaluation_strategy:\", set_evaluation_strategy)\n",
    "set_eval_steps = 1000                         # Optional\n",
    "print(\"eval_steps:\", set_eval_steps)\n",
    "set_save_total_limit = 40                   # Optional                 \n",
    "print(\"save_total_limit:\", set_save_total_limit)\n",
    "\n",
    "set_load_best_model_at_end = True           # Default = False\n",
    "print(\"load_best_model_at_end:\", set_load_best_model_at_end)\n",
    "set_metric_for_best_model = \"wer\"           # Optional\n",
    "print(\"metric_for_best_model:\", set_metric_for_best_model)\n",
    "set_greater_is_better = False               # Optional\n",
    "print(\"greater_is_better:\", set_greater_is_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9d91656-f245-4ef6-a64f-b3809d0602ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Experiment arguments... ------------------\n",
      "\n",
      "use_checkpoint: True\n",
      "training: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Experiment arguments... ------------------\\n\")\n",
    "use_checkpoint = True\n",
    "print('use_checkpoint:', use_checkpoint)\n",
    "\n",
    "training = True\n",
    "print('training:', training)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29e4c4a1-5e8b-469d-8041-a9ccc4f414ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cb7bd5d-b2c2-4c66-b654-cb37ef21f30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading files... ------------------\n",
      "\n",
      "Training dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_train_dataframe.csv\n",
      "\n",
      "Development dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_dev_dataframe.csv\n",
      "\n",
      "Testing dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_test_dataframe.csv\n",
      "\n",
      "Cache filepath is /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune\n",
      "\n",
      "Model filepath is /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/model_OGI_American_20230609\n",
      "\n",
      "Vocab filepath is /srv/scratch/z5313567/thesis/wav2vec2/vocab/OGI_American/vocab_OGI_American_20230609.json\n",
      "\n",
      "Fine-tuned result filepath is /srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/OGI_American/result_OGI_American_20230609.csv\n",
      "\n",
      "Checkpoint directory is /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/model_OGI_American_20230609/checkpoint-18000\n",
      "\n",
      "Pretrained model is /srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/model_OGI_American_20230609/checkpoint-18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Loading files... ------------------\\n\")\n",
    "\n",
    "train_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_train_dataframe.csv'\n",
    "print(f'Training dataset is stored at {train_df_fp}\\n')\n",
    "dev_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_dev_dataframe.csv'\n",
    "print(f'Development dataset is stored at {dev_df_fp}\\n')\n",
    "test_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_test_dataframe.csv'\n",
    "print(f'Testing dataset is stored at {test_df_fp}\\n')\n",
    "cache_fp = '/srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune'\n",
    "# cache_fp = '/srv/scratch/z5313567/thesis/cache'\n",
    "print(f'Cache filepath is {cache_fp}\\n')\n",
    "model_fp = '/srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/model_OGI_American_20230609'\n",
    "print(f'Model filepath is {model_fp}\\n')\n",
    "vocab_fp = '/srv/scratch/z5313567/thesis/wav2vec2/vocab/OGI_American/vocab_OGI_American_20230609.json'\n",
    "print(f'Vocab filepath is {vocab_fp}\\n')\n",
    "finetuned_result_fp = '/srv/scratch/z5313567/thesis/wav2vec2/finetuned_result/OGI_American/result_OGI_American_20230609.csv'\n",
    "print(f'Fine-tuned result filepath is {finetuned_result_fp}\\n')\n",
    "\n",
    "# use_checkpoint = False -----> pretrained_mod = 'facebook/wav2vec2-base'\n",
    "# use_checkpoint = True  -----> pretrained_mod = checkpoint_dir\n",
    "pretrained_mod = 'facebook/wav2vec2-base'\n",
    "if use_checkpoint:\n",
    "    checkpoint_dir = '/srv/scratch/z5313567/thesis/wav2vec2/model/OGI_American/model_OGI_American_20230609/checkpoint-18000'\n",
    "    pretrained_mod = checkpoint_dir\n",
    "    print(f'Checkpoint directory is {checkpoint_dir}\\n')\n",
    "print(f'Pretrained model is {pretrained_mod}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71b638bf-2870-4112-af82-385b29c17d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading datasets... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-168d650f40feead3/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 28.49it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Loading datasets... ------------------\\n\")\n",
    "\n",
    "dataset = load_dataset('csv', \n",
    "                    data_files={'train': train_df_fp,\n",
    "                                'dev': dev_df_fp,\n",
    "                                'test': test_df_fp},\n",
    "                    cache_dir=cache_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80b4ba43-f8a1-4cca-a1f8-d4a3f362cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc517131-380f-4e91-adb0-5160a0636ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 50187\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 10965\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 10847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print('dataset is:', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23f37c5d-702a-4b5d-879b-3c41808fcdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Showing some random elements... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Showing some random elements... ------------------\\n\")\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), f\"Can't pick more elements than there are in the dataset {len(dataset)}.\" \n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7137dd2-7f50-4aca-b4ad-a88981955d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           transcription\n",
      "0                              advantage\n",
      "1  why is the earth inside the milky way\n",
      "2           i thought i heard new voices\n",
      "3                                engrave\n",
      "4                                   four\n",
      "5                                   sour\n",
      "6                                 spoons\n",
      "7                      take off your hat\n",
      "8                             stepfather\n",
      "9                                ragtime\n"
     ]
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"].remove_columns([\"filepath\", \"duration\", \"speaker_id\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867872fe-b31c-4d89-84d9-fff2dc0dcc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Extracting individual characters... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Extracting individual characters... ------------------\\n\")\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"transcription\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee24fa6-b0ba-403e-8d99-d767493fe302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "vocabs = dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=dataset.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f839ff12-6ba5-4b6f-a275-0fe0fe93d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabs is: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print('vocabs is:', vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ef47eca-b158-409b-81f2-555f9f5d04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.column_names[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26888863-4a52-4c96-8ece-9fa8853260ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef97c36d-1e9f-4f55-a52f-d55c666d5060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_dict is: {'h': 0, 's': 1, 'k': 2, 'c': 3, 't': 4, 'e': 5, 'l': 6, 'a': 7, 'f': 8, 'g': 9, 'r': 10, 'b': 11, 'd': 12, 'i': 13, ' ': 14, 'm': 15, 'z': 16, 'o': 17, 'w': 18, 'q': 19, 'y': 20, 'j': 21, \"'\": 22, 'x': 23, 'v': 24, 'u': 25, 'p': 26, 'n': 27}\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "print(\"vocab_dict is:\", vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28a79c90-a38c-4b62-9045-c04a38f67083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lendth of vocab_list is 30\n"
     ]
    }
   ],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "print(f'The lendth of vocab_list is {len(vocab_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00390ae8-5166-47c8-9682-cac6abf9a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created vocab.json file at: /srv/scratch/z5313567/thesis/wav2vec2/vocab/OGI_American/vocab_OGI_American_20230609.json\n"
     ]
    }
   ],
   "source": [
    "with open(vocab_fp, 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "print('Successfully created vocab.json file at:', vocab_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0662038e-c029-432d-a7b5-a352ce023842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating tokenizer... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating tokenizer... ------------------\\n\")\n",
    "tokenizer = Wav2Vec2CTCTokenizer(vocab_fp, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba52903f-e99c-49da-a41b-d234d2c8f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating feature extractor... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating feature extractor... ------------------\\n\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e1deb36-3984-4897-a874-c8a228ecfef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating processor... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating processor... ------------------\\n\")\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor.save_pretrained(model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7ec74f0-137a-4459-abf7-8bc24cf0361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(batch):\n",
    "    speech_array, sampling_rate = librosa.load(batch['filepath'], sr=feature_extractor.sampling_rate)\n",
    "    batch['speech'] = speech_array\n",
    "    batch['sampling_rate'] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"transcription\"]\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd319db3-3329-4df9-bb1b-3c2581708000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Obtaining speech arrays, sampling rates, and target texts... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Obtaining speech arrays, sampling rates, and target texts... ------------------\\n\")\n",
    "dataset = dataset.map(process_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8319a7f0-0708-436a-9377-0863684e6d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text'],\n",
      "        num_rows: 50187\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text'],\n",
      "        num_rows: 10965\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text'],\n",
      "        num_rows: 10847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print('dataset is:', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a227ea84-3a8e-408f-999a-18ff4d775350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sampling rate of the data that was used to pretrain the model should match the sampling rate of the dataset used to fine-tune the model.\n",
    "def check_sampling_rate(dataset): # say, dataset = dataset['train']\n",
    "    target_sr = feature_extractor.sampling_rate\n",
    "    if len(set(dataset['sampling_rate'])) == 1:\n",
    "        actual_sr = list(set(dataset['sampling_rate']))[0] # sampling rate = 22050 Hz for OGI corpus\n",
    "        if actual_sr != target_sr:\n",
    "            print('MISMATCH!: the sampling rate used for fine-tuning Wav2vec2.0 does not match the sampling rate used for pretraining')\n",
    "            \n",
    "            now = datetime.now()\n",
    "            # Print out dd/mm/YY H:M:S\n",
    "            # ------------------------------------------\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print('Resampling starts...', dt_string)\n",
    "            for i in range(len(dataset)):\n",
    "                dataset[i]['speech'] = librosa.resample(np.asarray(dataset[i]['speech']), orig_sr=actual_sr, target_sr=target_sr)\n",
    "            now = datetime.now()\n",
    "            # dd/mm/YY H:M:S\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print('Resampling completes', dt_string)\n",
    "        else:\n",
    "            print('MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee2c1918-c482-4491-8d30-bacbd733075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckech sampling rates of training datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckech sampling rates of training datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41ea3672-866e-41fa-8ecb-5933b0df8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckeck sampling rates of development datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckeck sampling rates of development datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7f34bdf-5011-480c-a6f2-c13041944561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckech sampling rates of testing datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckech sampling rates of testing datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce796258-7bb8-4ec9-a273-f224578808ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(set(dataset['train']['sampling_rate']))[0])\n",
    "#print(len(set(dataset['train']['sampling_rate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f561fd1-b0ba-4bca-a2ec-b52d6c6d8d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Verifying some ramdom samples... ------------------\n",
      "\n",
      "Target text: four\n",
      "Input array shape: (33223,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: throughout\n",
      "Input array shape: (25404,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: even your ears sweat in a sauna\n",
      "Input array shape: (73804,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: nine\n",
      "Input array shape: (50506,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: women\n",
      "Input array shape: (41748,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: biology\n",
      "Input array shape: (50502,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Verifying some ramdom samples... ------------------\\n\")\n",
    "# verify if the data is a 1-dimensional array, the sampling rate corresponds to 16kHz, and the target text is clean.\n",
    "for i in range(6):\n",
    "    rand_int = random.randint(0, len(dataset[\"train\"])-1)\n",
    "    #rand_int = i\n",
    "    print('Target text:', dataset['train'][rand_int]['target_text'])\n",
    "    print('Input array shape:', np.asarray(dataset['train'][rand_int]['speech']).shape)\n",
    "    print('Sampling rate:', dataset['train'][rand_int][\"sampling_rate\"])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caa8835e-60e5-451d-94fe-d311732d3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    #batch[\"input_values\"] = processor(batch['speech'], sampling_rate=batch[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_values\"] = processor(batch['speech'], sampling_rate=batch[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "977a8dd6-9d83-4234-b335-94482726a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Obtaining input values, input length, and labels... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/50187 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map (num_proc=4):   3%|▎         | 1459/50187 [02:12<28:18:42,  2.09s/ examples]Process ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/z5313567/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/z5313567/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "                                                                                  File \"/home/z5313567/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Obtaining input values, input length, and labels... ------------------\\n\")\n",
    "#prep_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)\n",
    "# dataset = dataset.map(prepare_dataset, num_proc=4)\n",
    "data_prepared = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78d28c-dcb0-412d-a1e4-93d9676642db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(prep_dataset)\n",
    "print('dataset is:', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73221cc5-8f44-4e06-b9fb-0ac4e2e451e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------Training + Evaluation--------------------\n",
    "# ** Define a padding data collator used to pad the training samples to longest sample in their batch. In contrast to most NLP models,\n",
    "#    Wav2Vec2 has a much larger input length than output length. E.g., a sample of input length 50000 has an output length of no more than 100. \n",
    "#    Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only \n",
    "#    be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning Wav2Vec2 requires a special padding data collator, \n",
    "#    which we will define below\n",
    "# ** Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a compute_metrics function accordingly\n",
    "# ** Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n",
    "# ** Define the training configuration.\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace label ids of padding tokens with -100 so that those tokens are not taken into account when computing the loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df96aeb-6a31-43e8-a96c-2c405903c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Setting up the padding data collator... ------------------\\n')\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd75e8-cff0-4c80-9577-2f9f1740aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Setting up WER metric... ------------------\\n')\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75c201-ed6e-460d-b900-732fcbf90089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96aeef41-a42d-4123-ae33-3b083ac9ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading a pretrained checkpount... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:356: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_hid.bias', 'quantizer.codevectors', 'project_q.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.weight', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Loading a pretrained checkpount... ------------------\\n')\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    pretrained_mod, \n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    hidden_dropout=set_hidden_dropout,\n",
    "    activation_dropout=set_activation_dropout,\n",
    "    attention_dropout=set_attention_dropout,\n",
    "    feat_proj_dropout=set_feat_proj_dropout,\n",
    "    layerdrop=set_layerdrop,\n",
    "    mask_time_prob=set_mask_time_prob,\n",
    "    mask_time_length=set_mask_time_length,\n",
    "    ctc_loss_reduction=set_ctc_loss_reduction,\n",
    "    ctc_zero_infinity=set_ctc_zero_infinity    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea00f36b-d3ab-4fa5-9410-95cca877b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layers of Wav2vec2.0 model is sufficiently trained, hence they do not need to be finetuned anymore\n",
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7da9374-cee2-4139-8b49-768781fb4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Setting TrainingArguments... ------------------\\n')\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_fp,\n",
    "    per_device_train_batch_size=set_per_device_train_batch_size,\n",
    "    group_by_length=set_group_by_length,\n",
    "    gradient_accumulation_steps=set_gradient_accumulation_steps,\n",
    "    gradient_checkpointing=set_gradient_checkpointing,\n",
    "    weight_decay=set_weight_decay,\n",
    "    fp16=set_fp16,\n",
    "    learning_rate=set_learning_rate,\n",
    "    lr_scheduler_type=set_lr_scheduler_type,\n",
    "    adam_beta1=set_adam_beta1,\n",
    "    adam_beta2=set_adam_beta2,\n",
    "    adam_epsilon=set_adam_epsilon,\n",
    "    warmup_ratio=set_warmup_ratio,\n",
    "    num_train_epochs=set_num_train_epochs,\n",
    "    max_steps=set_max_steps,\n",
    "    logging_strategy=set_logging_strategy,\n",
    "    logging_steps=set_logging_steps,\n",
    "    save_strategy=set_save_strategy,\n",
    "    save_steps=set_save_steps,\n",
    "    evaluation_strategy=set_evaluation_strategy,\n",
    "    eval_steps=set_eval_steps,\n",
    "    save_total_limit=set_save_total_limit,\n",
    "    load_best_model_at_end=set_load_best_model_at_end,\n",
    "    metric_for_best_model=set_metric_for_best_model,\n",
    "    greater_is_better=set_greater_is_better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6295a847-06a6-4eae-9dfa-616902fe62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Setting Trainer... ------------------\\n')\n",
    "\n",
    "'''\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "'''\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data_prepared[\"train\"],\n",
    "    eval_dataset=data_prepared[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be5999-0d0e-485f-b3fe-6fe50c3d3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 50187\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 188220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='188220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    51/188220 14:17 < 914:53:08, 0.06 it/s, Epoch 0.01/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if training:\n",
    "    print('\\n------------------ Starting training... ------------------\\n')\n",
    "    torch.cuda.empty_cache()\n",
    "    if use_checkpoint:\n",
    "        trainer.train(resume_from_checkpoint=checkpoint_dir)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    model.save_pretrained(model_fp)\n",
    "    \n",
    "       \n",
    "    now = datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"Training Finished:\", dt_string)\n",
    "    print('\\n------------------ Training finished... ------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abc409-7e4d-4210-8155-94129c8d20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Evaluation starts... ------------------\\n')\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_fp)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d434a-8c8f-4ce7-a7fa-2df30a191a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will make use of the map(...) function to predict the transcription of every test sample and to save the prediction \n",
    "# in the dataset itself. We will call the resulting dictionary \"results\".\n",
    "\n",
    "# Note: we evaluate the test data set with batch_size=1 on purpose due to this issue. Since padded inputs don't yield the \n",
    "# exact same output as non-padded inputs, a better WER can be achieved by not padding the input at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f92bcf-e836-4e0e-badf-11189d554179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Generating fine-tuned results... ------------------\\n')\n",
    "\n",
    "\n",
    "def map_to_result(batch):\n",
    "    model.to(\"cuda\")\n",
    "    input_values = processor(\n",
    "      batch[\"speech\"], \n",
    "      sampling_rate=batch[\"sampling_rate\"], \n",
    "      return_tensors=\"pt\"\n",
    "    ).input_values.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37352b8b-ddc3-4efc-b0d8-39cd1e525382",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def map_to_result(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  \n",
    "    return batch\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051bcb4-98dd-4ef8-acf4-2da65c28df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset[\"test\"].map(map_to_result)\n",
    "print('results are:', results)\n",
    "\n",
    "results_df = results.to_pandas()\n",
    "results_df = results_df.drop(columns=[\"speech\", \"sampling_rate\", \"input_values\", \"input_length\", \"labels\"])\n",
    "results_df.to_csv(finetuned_result_fp)\n",
    "print('Fine-tuned results are saved to:', finetuned_result_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5083284-01f9-476d-9b37-ca4e5c9a2165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Generating WER result on test dataset... ------------------\\n')\n",
    "print(\"Fine-tuned Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"target_text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea690dc-72fc-4179-b629-dae9c9df9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Showing some random prediction errors... ------------------\\n')\n",
    "show_random_elements(results.remove_columns([\"speech\", \"sampling_rate\", \"input_values\", \"input_length\", \"labels\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8152ad-1f37-412a-befc-83c67538e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Generating the exact output of the model... ------------------\\n')\n",
    "\n",
    "model.to(\"cuda\")\n",
    "input_values = processor(\n",
    "      dataset[\"test\"][0][\"speech\"], \n",
    "      sampling_rate=dataset[\"test\"][0][\"sampling_rate\"], \n",
    "      return_tensors=\"pt\"\n",
    "  ).input_values.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "print(\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113663c-3a7d-41b8-9087-01885f1897a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(dataset[\"test\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "print(\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist())))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45ee3b-6821-4dd5-a533-6ffc8c4096ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Evaluation Finished:\", dt_string)\n",
    "print('\\n------------------ Evaluation finished... ------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_jupyter_env",
   "language": "python",
   "name": "my_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
