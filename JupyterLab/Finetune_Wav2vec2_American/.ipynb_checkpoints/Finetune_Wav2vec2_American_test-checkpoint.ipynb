{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d319d-f9bb-4214-9066-bcad64e28313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a968b3a5-779c-4d65-91e1-f68871dfca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.29.2 in /home/z5313567/.local/lib/python3.10/site-packages (4.29.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (23.0)\n",
      "Requirement already satisfied: requests in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (4.64.1)\n",
      "Requirement already satisfied: filelock in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/python-3.10.8-pmtwsrrmcmrs6olvgx5xhepgh7gl5vro/lib/python3.10/site-packages (from transformers==4.29.2) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (1.24.2)\n",
      "Requirement already satisfied: fsspec in /home/z5313567/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/ood-jupyterlab-3.4.8-3qopzgrtpuwifxuotyxofkeze52d6ra7/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers==4.29.2\n",
    "pip install transformers==4.29.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f48c9b-d836-4ec8-b06f-4fdb5e9480c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers==4.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518ac92-56cb-4b5f-8594-f6ffcec7862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407fda3-095e-4e59-9863-e04b33673d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5030e-27c6-4906-b7d2-d49058f60fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c977ee-0319-421b-a2ef-322e31e01d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1032c-4525-4d85-b58d-8807588a809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb0c31-2f79-47dc-9fbe-76554b32e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642919d-bbb5-4ee1-a23a-7b9d97cd1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ccbbc-8641-47b1-a115-bd98af0171ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: \n",
    "#     https://huggingface.co/blog/fine-tune-wav2vec2-english\n",
    "#     https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb#scrollTo=e7cqAWIayn6w\n",
    "#     https://github.com/monomest/Thesis/blob/3a15f747dfd934535ffb7a02bf3fee97d9c546cb/s5/wav2vec_projects/THESIS_C/run_finetune_kids_OGI.py#L239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56590245-4ce2-4076-9b50-a8b34592434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------ Importing libraries... ------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-24 00:13:41.858797: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/cudnn-8.4.0.27-11.6-4tsywshvqo4jcejxttjfatv44nfj6ci4/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/cuda-11.8.0-5c2q3i32c2okq6c6xiodcedi7qdn5xc7/lib64:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/python-3.10.8-pmtwsrrmcmrs6olvgx5xhepgh7gl5vro/lib\n",
      "2023-05-24 00:13:41.858929: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/cudnn-8.4.0.27-11.6-4tsywshvqo4jcejxttjfatv44nfj6ci4/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/cuda-11.8.0-5c2q3i32c2okq6c6xiodcedi7qdn5xc7/lib64:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/python-3.10.8-pmtwsrrmcmrs6olvgx5xhepgh7gl5vro/lib\n",
      "2023-05-24 00:13:41.858940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------ Importing libraries... ------------\\n\")\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d6f080-0799-4eda-8e28-7205e55f431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n",
      "1.24.2\n",
      "0\n",
      "False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib/python3.10/site-packages/torch/cuda/__init__.py:341\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib/python3.10/site-packages/torch/cuda/__init__.py:371\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib/python3.10/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)\n",
    "print(np.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb7bd5d-b2c2-4c66-b654-cb37ef21f30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading files... ------------------\n",
      "\n",
      "Training dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_train_dataframe.csv\n",
      "\n",
      "Development dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_dev_dataframe.csv\n",
      "\n",
      "Testing dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_test_dataframe.csv\n",
      "\n",
      "Cache filepath is /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune\n",
      "\n",
      "Model filepath is /srv/scratch/z5313567/thesis/OGI_local/base-OGI-20230523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Loading files... ------------------\\n\")\n",
    "\n",
    "train_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_train_dataframe.csv'\n",
    "dev_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_dev_dataframe.csv'\n",
    "test_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/OGI_scripted_test_dataframe.csv'\n",
    "cache_fp = '/srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune'\n",
    "model_fp = '/srv/scratch/z5313567/thesis/OGI_local/base-OGI-20230523'\n",
    "\n",
    "print(f'Training dataset is stored at {train_df_fp}\\n')\n",
    "print(f'Development dataset is stored at {dev_df_fp}\\n')\n",
    "print(f'Testing dataset is stored at {test_df_fp}\\n')\n",
    "print(f'Cache filepath is {cache_fp}\\n')\n",
    "print(f'Model filepath is {model_fp}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b638bf-2870-4112-af82-385b29c17d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading datasets... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-8ee1a63a5391500a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 23.27it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Loading datasets... ------------------\\n\")\n",
    "\n",
    "dataset = load_dataset('csv', \n",
    "                    data_files={'train': train_df_fp,\n",
    "                                'dev': dev_df_fp,\n",
    "                                'test': test_df_fp},\n",
    "                    cache_dir=cache_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b4ba43-f8a1-4cca-a1f8-d4a3f362cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc517131-380f-4e91-adb0-5160a0636ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 50187\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 10965\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 10847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f37c5d-702a-4b5d-879b-3c41808fcdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Showing some random elements... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Showing some random elements... ------------------\\n\")\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), f\"Can't pick more elements than there are in the dataset {len(dataset)}.\" \n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7137dd2-7f50-4aca-b4ad-a88981955d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      transcription\n",
      "0                            yields\n",
      "1                           nuclear\n",
      "2                            easier\n",
      "3                              page\n",
      "4                           smashed\n",
      "5                         dashboard\n",
      "6                             women\n",
      "7  i like my meat on a cold platter\n",
      "8                              nine\n",
      "9                         lethargic\n"
     ]
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"].remove_columns([\"filepath\", \"duration\", \"speaker_id\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867872fe-b31c-4d89-84d9-fff2dc0dcc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Extracting individual characters... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Extracting individual characters... ------------------\\n\")\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"transcription\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee24fa6-b0ba-403e-8d99-d767493fe302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "vocabs = dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=dataset.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f839ff12-6ba5-4b6f-a275-0fe0fe93d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef47eca-b158-409b-81f2-555f9f5d04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.column_names[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26888863-4a52-4c96-8ece-9fa8853260ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef97c36d-1e9f-4f55-a52f-d55c666d5060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': 0, 'i': 1, 'p': 2, 'u': 3, 'z': 4, ' ': 5, 't': 6, 'r': 7, 'a': 8, \"'\": 9, 'h': 10, 'j': 11, 'c': 12, 's': 13, 'k': 14, 'g': 15, 'o': 16, 'v': 17, 'e': 18, 'f': 19, 'x': 20, 'q': 21, 'y': 22, 'd': 23, 'm': 24, 'n': 25, 'l': 26, 'b': 27}\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28a79c90-a38c-4b62-9045-c04a38f67083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lendth of vocab_list is 30\n"
     ]
    }
   ],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "print(f'The lendth of vocab_list is {len(vocab_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00390ae8-5166-47c8-9682-cac6abf9a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0662038e-c029-432d-a7b5-a352ce023842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating tokenizer... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating tokenizer... ------------------\\n\")\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba52903f-e99c-49da-a41b-d234d2c8f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating feature extractor... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating feature extractor... ------------------\\n\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e1deb36-3984-4897-a874-c8a228ecfef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating processor... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating processor... ------------------\\n\")\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7ec74f0-137a-4459-abf7-8bc24cf0361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(batch):\n",
    "    batch['speech'], batch['sampling rate'] = librosa.load(batch['filepath'], sr=feature_extractor.sampling_rate)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd319db3-3329-4df9-bb1b-3c2581708000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Obtaining speech arrays and sampling rates... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-8ee1a63a5391500a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9baeb70e8f9cf996_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-8ee1a63a5391500a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-703f2ef35c5ae356_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-8ee1a63a5391500a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-6b65b6ef71230708_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Obtaining speech arrays and sampling rates... ------------------\\n\")\n",
    "dataset = dataset.map(process_dataset, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a227ea84-3a8e-408f-999a-18ff4d775350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# the sampling rate of the data that was used to pretrain the model should match the sampling rate of the dataset used to fine-tune the model.\n",
    "def check_sampling_rate(dataset): # say, dataset = dataset['train']\n",
    "    target_sr = feature_extractor.sampling_rate\n",
    "    if len(set(dataset['sampling rate'])) == 1:\n",
    "        actual_sr = list(set(dataset['sampling rate']))[0] # sampling rate = 22050 Hz for OGI corpus\n",
    "        if actual_sr != target_sr:\n",
    "            print('MISMATCH!: the sampling rate used for fine-tuning Wav2vec2.0 does not match the sampling rate used for pretraining')\n",
    "            \n",
    "            now = datetime.now()\n",
    "            # Print out dd/mm/YY H:M:S\n",
    "            # ------------------------------------------\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print('Resampling starts...', dt_string)\n",
    "            for i in range(len(dataset)):\n",
    "                dataset[i]['speech'] = librosa.resample(np.asarray(dataset[i]['speech']), orig_sr=actual_sr, target_sr=target_sr)\n",
    "            now = datetime.now()\n",
    "            # dd/mm/YY H:M:S\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print('Resampling completes', dt_string)\n",
    "        else:\n",
    "            print('MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee2c1918-c482-4491-8d30-bacbd733075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckech sampling rates of training datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckech sampling rates of training datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41ea3672-866e-41fa-8ecb-5933b0df8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckeck sampling rates of development datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckeck sampling rates of development datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7f34bdf-5011-480c-a6f2-c13041944561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckech sampling rates of testing datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckech sampling rates of testing datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce796258-7bb8-4ec9-a273-f224578808ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(set(dataset['train']['sampling rate']))[0])\n",
    "#print(len(set(dataset['train']['sampling rate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f561fd1-b0ba-4bca-a2ec-b52d6c6d8d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Verifying some ramdom samples... ------------------\n",
      "\n",
      "Target text: humor\n",
      "Input array shape: (25721,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: behind\n",
      "Input array shape: (73791,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: picture\n",
      "Input array shape: (57770,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: i don't clean windows\n",
      "Input array shape: (89815,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: bigfoot\n",
      "Input array shape: (44751,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: yankee\n",
      "Input array shape: (31497,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Verifying some ramdom samples... ------------------\\n\")\n",
    "# verify if the data is a 1-dimensional array, the sampling rate corresponds to 16kHz, and the target text is clean.\n",
    "for i in range(6):\n",
    "    rand_int = random.randint(0, len(dataset[\"train\"]))\n",
    "    #rand_int = i\n",
    "    print('Target text:', dataset['train'][rand_int]['transcription'])\n",
    "    print('Input array shape:', np.asarray(dataset['train'][rand_int]['speech']).shape)\n",
    "    print('Sampling rate:', dataset['train'][rand_int][\"sampling rate\"])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8835e-60e5-451d-94fe-d311732d3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch['speech'], sampling_rate=batch[\"sampling rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "977a8dd6-9d83-4234-b335-94482726a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/50187 [00:00<?, ? examples/s]/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                            Process ForkPoolWorker-8:\n",
      "\r"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:1373\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1373\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/multiprocess/managers.py:818\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m conn\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id, methodname, args, kwds))\n\u001b[0;32m--> 818\u001b[0m kind, result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/multiprocess/connection.py:258\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 258\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/multiprocess/connection.py:422\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 422\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/multiprocess/connection.py:387\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#prep_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:851\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 851\u001b[0m     {\n\u001b[1;32m    852\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:852\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    851\u001b[0m     {\n\u001b[0;32m--> 852\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3166\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3158\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3160\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3161\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3164\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3165\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3166\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3167\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3168\u001b[0m     ):\n\u001b[1;32m   3169\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3170\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:1379\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:1379\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/multiprocess/pool.py:770\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_success:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#prep_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd78d28c-dcb0-412d-a1e4-93d9676642db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'input_length', 'labels'],\n",
      "        num_rows: 50187\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['input_values', 'input_length', 'labels'],\n",
      "        num_rows: 10965\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'input_length', 'labels'],\n",
      "        num_rows: 10847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#print(prep_dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "73221cc5-8f44-4e06-b9fb-0ac4e2e451e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------Training + Evaluation--------------------\n",
    "# ** Define a padding data collator used to pad the training samples to longest sample in their batch. In contrast to most NLP models,\n",
    "#    Wav2Vec2 has a much larger input length than output length. E.g., a sample of input length 50000 has an output length of no more than 100. \n",
    "#    Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only \n",
    "#    be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning Wav2Vec2 requires a special padding data collator, \n",
    "#    which we will define below\n",
    "# ** Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a compute_metrics function accordingly\n",
    "# ** Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n",
    "# ** Define the training configuration.\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace label ids of padding tokens with -100 so that those tokens are not taken into account when computing the loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9df96aeb-6a31-43e8-a96c-2c405903c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Setting up the padding data collator... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Setting up the padding data collator... ------------------\\n')\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25dd75e8-cff0-4c80-9577-2f9f1740aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Setting up WER metric... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Setting up WER metric... ------------------\\n')\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af75c201-ed6e-460d-b900-732fcbf90089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96aeef41-a42d-4123-ae33-3b083ac9ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading a pretrained checkpount... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:356: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_hid.bias', 'quantizer.codevectors', 'project_q.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.weight', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Loading a pretrained checkpount... ------------------\\n')\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\",\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea00f36b-d3ab-4fa5-9410-95cca877b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layers of Wav2vec2.0 model is sufficiently trained, hence they do not need to be finetuned anymore\n",
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7da9374-cee2-4139-8b49-768781fb4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=model_fp,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  #fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6295a847-06a6-4eae-9dfa-616902fe62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be5999-0d0e-485f-b3fe-6fe50c3d3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 50187\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 188220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='188220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    51/188220 14:17 < 914:53:08, 0.06 it/s, Epoch 0.01/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.bool = np.bool_\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abc409-7e4d-4210-8155-94129c8d20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Evaluation starts... ------------------\\n')\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3ed95-8fc7-4deb-bad2-30e8b024becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-timit-demo-google-colab\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f92bcf-e836-4e0e-badf-11189d554179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "  model.to(\"cuda\")\n",
    "  input_values = processor(\n",
    "      batch[\"speech\"], \n",
    "      sampling_rate=batch[\"sampling_rate\"], \n",
    "      return_tensors=\"pt\"\n",
    "  ).input_values.to(\"cuda\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  \n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37352b8b-ddc3-4efc-b0d8-39cd1e525382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "  with torch.no_grad():\n",
    "    input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "  return batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_jupyter_env",
   "language": "python",
   "name": "my_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
