{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d319d-f9bb-4214-9066-bcad64e28313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a968b3a5-779c-4d65-91e1-f68871dfca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.29.2 in /home/z5313567/.local/lib/python3.10/site-packages (4.29.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (23.0)\n",
      "Requirement already satisfied: requests in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (4.64.1)\n",
      "Requirement already satisfied: filelock in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/python-3.10.8-pmtwsrrmcmrs6olvgx5xhepgh7gl5vro/lib/python3.10/site-packages (from transformers==4.29.2) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/z5313567/.local/lib/python3.10/site-packages (from transformers==4.29.2) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from transformers==4.29.2) (1.24.2)\n",
      "Requirement already satisfied: fsspec in /home/z5313567/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/ood-jupyterlab-3.4.8-3qopzgrtpuwifxuotyxofkeze52d6ra7/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers==4.29.2\n",
    "pip install transformers==4.29.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f48c9b-d836-4ec8-b06f-4fdb5e9480c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers==4.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518ac92-56cb-4b5f-8594-f6ffcec7862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407fda3-095e-4e59-9863-e04b33673d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5030e-27c6-4906-b7d2-d49058f60fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c977ee-0319-421b-a2ef-322e31e01d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1032c-4525-4d85-b58d-8807588a809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb0c31-2f79-47dc-9fbe-76554b32e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642919d-bbb5-4ee1-a23a-7b9d97cd1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ccbbc-8641-47b1-a115-bd98af0171ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: \n",
    "#     https://huggingface.co/blog/fine-tune-wav2vec2-english\n",
    "#     https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb#scrollTo=e7cqAWIayn6w\n",
    "#     https://github.com/monomest/Thesis/blob/3a15f747dfd934535ffb7a02bf3fee97d9c546cb/s5/wav2vec_projects/THESIS_C/run_finetune_kids_OGI.py#L239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56590245-4ce2-4076-9b50-a8b34592434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------ Importing libraries... ------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-05 12:55:50.428362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 12:55:50.755860: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-05 12:55:53.354737: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/cudnn-8.4.0.27-11.6-4tsywshvqo4jcejxttjfatv44nfj6ci4/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/cuda-11.8.0-5c2q3i32c2okq6c6xiodcedi7qdn5xc7/lib64:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/python-3.10.8-pmtwsrrmcmrs6olvgx5xhepgh7gl5vro/lib\n",
      "2023-06-05 12:55:53.355279: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/cudnn-8.4.0.27-11.6-4tsywshvqo4jcejxttjfatv44nfj6ci4/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/cuda-11.8.0-5c2q3i32c2okq6c6xiodcedi7qdn5xc7/lib64:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/tensorflow-2.11.0-wwrg4vksfz6yhvhh2tlpr6nbv4qezh26/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/pytorch-1.13.1-hwaaxko2rubtnjm53evhgibvjeapztgk/lib:/apps/z_install_tree/linux-rocky8-ivybridge/gcc-12.2.0/python-3.10.8-pmtwsrrmcmrs6olvgx5xhepgh7gl5vro/lib\n",
      "2023-06-05 12:55:53.355285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------ Importing libraries... ------------\\n\")\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d6f080-0799-4eda-8e28-7205e55f431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.0.1\n",
      "json version: 2.0.9\n",
      "librosa version: 0.10.0.post2\n",
      "Numpy version: 1.24.2\n",
      "Transformers version: 4.29.2\n",
      "Torch version: 1.13.1+cu117\n",
      "Test cuda_device_count 1\n",
      "Test cuda_is_available True\n",
      "Test get_device_name Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"json version:\", json.__version__)\n",
    "print(\"librosa version:\", librosa.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Test cuda_device_count\", torch.cuda.device_count())\n",
    "print(\"Test cuda_is_available\", torch.cuda.is_available())\n",
    "print(\"Test get_device_name\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a426fa6-9d3d-45d8-a35c-3bc48a7628a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Model arguments... ------------------\n",
      "\n",
      "hidden_dropout: 0.1\n",
      "activation_dropout: 0.1\n",
      "attention_dropout: 0.1\n",
      "feat_proj_dropout: 0.0\n",
      "layerdrop: 0.01\n",
      "mask_time_prob: 0.065\n",
      "mask_time_length: 10\n",
      "ctc_loss_reduction: mean\n",
      "ctc_zero_infinity: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Model arguments... ------------------\\n\")\n",
    "# For setting model = Wav2Vec2ForCTC.from_pretrained()\n",
    "\n",
    "set_hidden_dropout = 0.1                    # Default = 0.1\n",
    "print(\"hidden_dropout:\", set_hidden_dropout)\n",
    "set_activation_dropout = 0.1                # Default = 0.1\n",
    "print(\"activation_dropout:\", set_activation_dropout)\n",
    "set_attention_dropout = 0.1                 # Default = 0.1\n",
    "print(\"attention_dropout:\", set_attention_dropout)\n",
    "set_feat_proj_dropout = 0.0                 # Default = 0.1\n",
    "print(\"feat_proj_dropout:\", set_feat_proj_dropout)\n",
    "set_layerdrop = 0.01                        # Default = 0.1\n",
    "print(\"layerdrop:\", set_layerdrop)\n",
    "set_mask_time_prob = 0.065                  # Default = 0.05\n",
    "print(\"mask_time_prob:\", set_mask_time_prob)\n",
    "set_mask_time_length = 10                   # Default = 10\n",
    "print(\"mask_time_length:\", set_mask_time_length)\n",
    "set_ctc_loss_reduction = \"mean\"             # Default = \"sum\"\n",
    "print(\"ctc_loss_reduction:\", set_ctc_loss_reduction)\n",
    "set_ctc_zero_infinity = True               # Default = False\n",
    "print(\"ctc_zero_infinity:\", set_ctc_zero_infinity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867656ba-8933-4540-a0d4-54926761c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Training arguments... ------------------\n",
      "\n",
      "per_device_train_batch_size: 8\n",
      "group_by_length: True\n",
      "gradient_accumulation_steps: 1\n",
      "gradient_checkpointing: True\n",
      "weight_decay: 0.01\n",
      "fp16: True\n",
      "learning_rate: 4e-05\n",
      "lr_scheduler_type: linear\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.98\n",
      "adam_epsilon: 1e-08\n",
      "warmup_ratio: 0.1\n",
      "num_train_epochs: 22\n",
      "max_steps: 35000\n",
      "logging_strategy: steps\n",
      "logging_steps: 1000\n",
      "save_strategy: steps\n",
      "save_steps: 1000\n",
      "evaluation_strategy: steps\n",
      "eval_steps: 1000\n",
      "save_total_limit: 40\n",
      "load_best_model_at_end: True\n",
      "metric_for_best_model: wer\n",
      "greater_is_better: False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Training arguments... ------------------\\n\")\n",
    "# For setting training_args = TrainingArguments()\n",
    "\n",
    "set_per_device_train_batch_size = 8         # Default = 8\n",
    "print(\"per_device_train_batch_size:\", set_per_device_train_batch_size)\n",
    "set_group_by_length = True                  # Default = False\n",
    "print(\"group_by_length:\", set_group_by_length)\n",
    "set_gradient_accumulation_steps = 1         # Default = 1\n",
    "print(\"gradient_accumulation_steps:\", set_gradient_accumulation_steps)\n",
    "set_gradient_checkpointing = True           # Default = False\n",
    "print(\"gradient_checkpointing:\", set_gradient_checkpointing)\n",
    "set_weight_decay = 0.01                     # Default = 0\n",
    "print(\"weight_decay:\", set_weight_decay)\n",
    "set_fp16 = True                             # Default = False\n",
    "print(\"fp16:\", set_fp16)\n",
    "\n",
    "set_learning_rate = 0.00004                 # Default = 0.00005\n",
    "print(\"learning_rate:\", set_learning_rate)\n",
    "set_lr_scheduler_type = \"linear\"            # Default = \"linear\"\n",
    "print(\"lr_scheduler_type:\", set_lr_scheduler_type )\n",
    "set_adam_beta1 = 0.9                        # Default = 0.9\n",
    "print(\"adam_beta1:\", set_adam_beta1)\n",
    "set_adam_beta2 = 0.98                       # Default = 0.999\n",
    "print(\"adam_beta2:\", set_adam_beta2)\n",
    "set_adam_epsilon = 0.00000001               # Default = 0.00000001\n",
    "print(\"adam_epsilon:\", set_adam_epsilon)\n",
    "set_warmup_ratio = 0.1                      # Default = 0.0\n",
    "print(\"warmup_ratio:\", set_warmup_ratio)\n",
    "\n",
    "set_num_train_epochs = 22                 # Default = 3.0\n",
    "print(\"num_train_epochs:\", set_num_train_epochs)\n",
    "set_max_steps = 35000                       # Default = -1, overrides epochs\n",
    "print(\"max_steps:\", set_max_steps)\n",
    "\n",
    "set_logging_strategy = \"steps\"              # Default = \"steps\"\n",
    "print(\"logging_strategy:\", set_logging_strategy)\n",
    "set_logging_steps = 1000                      # Default = 500\n",
    "print(\"logging_steps:\", set_logging_steps)\n",
    "set_save_strategy = \"steps\"                 # Default = \"steps\"\n",
    "print(\"save_strategy:\", set_save_strategy)\n",
    "set_save_steps = 1000                         # Default = 500\n",
    "print(\"save_steps:\", set_save_steps)\n",
    "set_evaluation_strategy = \"steps\"           # Default = \"no\"\n",
    "print(\"evaluation_strategy:\", set_evaluation_strategy)\n",
    "set_eval_steps = 1000                         # Optional\n",
    "print(\"eval_steps:\", set_eval_steps)\n",
    "set_save_total_limit = 40                   # Optional                 \n",
    "print(\"save_total_limit:\", set_save_total_limit)\n",
    "\n",
    "set_load_best_model_at_end = True           # Default = False\n",
    "print(\"load_best_model_at_end:\", set_load_best_model_at_end)\n",
    "set_metric_for_best_model = \"wer\"           # Optional\n",
    "print(\"metric_for_best_model:\", set_metric_for_best_model)\n",
    "set_greater_is_better = False               # Optional\n",
    "print(\"greater_is_better:\", set_greater_is_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb7bd5d-b2c2-4c66-b654-cb37ef21f30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading files... ------------------\n",
      "\n",
      "Training dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/short_datasets/short_OGI_scripted_train_dataframe.csv\n",
      "\n",
      "Development dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/short_datasets/short_OGI_scripted_dev_dataframe.csv\n",
      "\n",
      "Testing dataset is stored at /srv/scratch/z5313567/thesis/OGI_local/short_datasets/short_OGI_scripted_test_dataframe.csv\n",
      "\n",
      "Cache filepath is /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune\n",
      "\n",
      "Model filepath is /srv/scratch/z5313567/thesis/OGI_local/short_datasets/non-offical-OGI\n",
      "\n",
      "Vocab filepath is /srv/scratch/z5313567/thesis/OGI_local/short_datasets/vocab_non-offical-OGI.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Loading files... ------------------\\n\")\n",
    "\n",
    "train_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/short_datasets/short_OGI_scripted_train_dataframe.csv'\n",
    "dev_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/short_datasets/short_OGI_scripted_dev_dataframe.csv'\n",
    "test_df_fp = '/srv/scratch/z5313567/thesis/OGI_local/short_datasets/short_OGI_scripted_test_dataframe.csv'\n",
    "cache_fp = '/srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune'\n",
    "model_fp = '/srv/scratch/z5313567/thesis/OGI_local/short_datasets/non-offical-OGI'\n",
    "vocab_fp = '/srv/scratch/z5313567/thesis/OGI_local/short_datasets/vocab_non-offical-OGI.json'\n",
    "\n",
    "print(f'Training dataset is stored at {train_df_fp}\\n')\n",
    "print(f'Development dataset is stored at {dev_df_fp}\\n')\n",
    "print(f'Testing dataset is stored at {test_df_fp}\\n')\n",
    "print(f'Cache filepath is {cache_fp}\\n')\n",
    "print(f'Model filepath is {model_fp}\\n')\n",
    "print(f'Vocab filepath is {vocab_fp}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b638bf-2870-4112-af82-385b29c17d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading datasets... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-631b16b4329d1149/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 232.47it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Loading datasets... ------------------\\n\")\n",
    "\n",
    "dataset = load_dataset('csv', \n",
    "                    data_files={'train': train_df_fp,\n",
    "                                'dev': dev_df_fp,\n",
    "                                'test': test_df_fp},\n",
    "                    cache_dir=cache_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b4ba43-f8a1-4cca-a1f8-d4a3f362cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc517131-380f-4e91-adb0-5160a0636ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 470\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 134\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'duration', 'speaker_id', 'transcription'],\n",
      "        num_rows: 132\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f37c5d-702a-4b5d-879b-3c41808fcdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Showing some random elements... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Showing some random elements... ------------------\\n\")\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), f\"Can't pick more elements than there are in the dataset {len(dataset)}.\" \n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7137dd2-7f50-4aca-b4ad-a88981955d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           transcription\n",
      "0                 we had a birdseye view\n",
      "1    five people can fit in the elevator\n",
      "2             help me unroll the new rug\n",
      "3                             pathfinder\n",
      "4            we've had a wonderful night\n",
      "5                              civilized\n",
      "6                              civilized\n",
      "7                                 ethnic\n",
      "8  mabel grows tomatoes in the courtyard\n",
      "9                  i think cats are cute\n"
     ]
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"].remove_columns([\"filepath\", \"duration\", \"speaker_id\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "867872fe-b31c-4d89-84d9-fff2dc0dcc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Extracting individual characters... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Extracting individual characters... ------------------\\n\")\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"transcription\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee24fa6-b0ba-403e-8d99-d767493fe302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "vocabs = dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=dataset.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f839ff12-6ba5-4b6f-a275-0fe0fe93d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabs is: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['vocab', 'all_text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print('vocabs is:', vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ef47eca-b158-409b-81f2-555f9f5d04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.column_names[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26888863-4a52-4c96-8ece-9fa8853260ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef97c36d-1e9f-4f55-a52f-d55c666d5060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_dict is: {' ': 0, 'i': 1, 'x': 2, \"'\": 3, 't': 4, 'm': 5, 'r': 6, 'c': 7, 'l': 8, 'v': 9, 'a': 10, 'y': 11, 'o': 12, 'e': 13, 'w': 14, 'p': 15, 'z': 16, 'u': 17, 'f': 18, 's': 19, 'n': 20, 'd': 21, 'j': 22, 'k': 23, 'q': 24, 'b': 25, 'g': 26, 'h': 27}\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "print(\"vocab_dict is:\", vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28a79c90-a38c-4b62-9045-c04a38f67083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lendth of vocab_list is 30\n"
     ]
    }
   ],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "print(f'The lendth of vocab_list is {len(vocab_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00390ae8-5166-47c8-9682-cac6abf9a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created vocab.json file at: /srv/scratch/z5313567/thesis/OGI_local/short_datasets/vocab_non-offical-OGI.json\n"
     ]
    }
   ],
   "source": [
    "with open(vocab_fp, 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "print('Successfully created vocab.json file at:', vocab_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0662038e-c029-432d-a7b5-a352ce023842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating tokenizer... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating tokenizer... ------------------\\n\")\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba52903f-e99c-49da-a41b-d234d2c8f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating feature extractor... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating feature extractor... ------------------\\n\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e1deb36-3984-4897-a874-c8a228ecfef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Creating processor... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Creating processor... ------------------\\n\")\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7ec74f0-137a-4459-abf7-8bc24cf0361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(batch):\n",
    "    batch['speech'], batch['sampling_rate'] = librosa.load(batch['filepath'], sr=feature_extractor.sampling_rate)\n",
    "    batch[\"target_text\"] = batch[\"transcription\"]\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd319db3-3329-4df9-bb1b-3c2581708000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Obtaining speech arrays, sampling rates, and target texts... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-631b16b4329d1149/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-6c63f1f1b08c919d_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-631b16b4329d1149/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5129c85c3add470e_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-631b16b4329d1149/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-09b14f759d9ab03e_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Obtaining speech arrays, sampling rates, and target texts... ------------------\\n\")\n",
    "dataset = dataset.map(process_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8532db34-a644-41b7-b8d2-98420de8e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text'],\n",
      "        num_rows: 470\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text'],\n",
      "        num_rows: 134\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text'],\n",
      "        num_rows: 132\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print('dataset is:', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a227ea84-3a8e-408f-999a-18ff4d775350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the sampling rate of the data that was used to pretrain the model should match the sampling rate of the dataset used to fine-tune the model.\n",
    "def check_sampling_rate(dataset): # say, dataset = dataset['train']\n",
    "    target_sr = feature_extractor.sampling_rate\n",
    "    if len(set(dataset['sampling_rate'])) == 1:\n",
    "        actual_sr = list(set(dataset['sampling_rate']))[0] # sampling rate = 22050 Hz for OGI corpus\n",
    "        if actual_sr != target_sr:\n",
    "            print('MISMATCH!: the sampling rate used for fine-tuning Wav2vec2.0 does not match the sampling rate used for pretraining')\n",
    "            \n",
    "            now = datetime.now()\n",
    "            # Print out dd/mm/YY H:M:S\n",
    "            # ------------------------------------------\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print('Resampling starts...', dt_string)\n",
    "            for i in range(len(dataset)):\n",
    "                dataset[i]['speech'] = librosa.resample(np.asarray(dataset[i]['speech']), orig_sr=actual_sr, target_sr=target_sr)\n",
    "            now = datetime.now()\n",
    "            # dd/mm/YY H:M:S\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print('Resampling completes', dt_string)\n",
    "        else:\n",
    "            print('MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee2c1918-c482-4491-8d30-bacbd733075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckech sampling rates of training datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckech sampling rates of training datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41ea3672-866e-41fa-8ecb-5933b0df8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckeck sampling rates of development datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckeck sampling rates of development datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7f34bdf-5011-480c-a6f2-c13041944561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Ckech sampling rates of testing datasets... ------------------\n",
      "\n",
      "MATCH!: The sampling rate used for fine-tuning Wav2vec2.0 matches the sampling rate used for pretraining\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------Ckech sampling rates of testing datasets... ------------------\\n')\n",
    "check_sampling_rate(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce796258-7bb8-4ec9-a273-f224578808ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(set(dataset['train']['sampling_rate']))[0])\n",
    "#print(len(set(dataset['train']['sampling_rate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f561fd1-b0ba-4bca-a2ec-b52d6c6d8d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Verifying some ramdom samples... ------------------\n",
      "\n",
      "Target text: join\n",
      "Input array shape: (41748,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: civilized\n",
      "Input array shape: (44745,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: zucchini\n",
      "Input array shape: (41744,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: hopeful\n",
      "Input array shape: (31813,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: picture\n",
      "Input array shape: (43835,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n",
      "Target text: athlete\n",
      "Input array shape: (31809,)\n",
      "Sampling rate: 16000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Verifying some ramdom samples... ------------------\\n\")\n",
    "# verify if the data is a 1-dimensional array, the sampling rate corresponds to 16kHz, and the target text is clean.\n",
    "for i in range(6):\n",
    "    rand_int = random.randint(0, len(dataset[\"train\"]))\n",
    "    #rand_int = i\n",
    "    print('Target text:', dataset['train'][rand_int]['target_text'])\n",
    "    print('Input array shape:', np.asarray(dataset['train'][rand_int]['speech']).shape)\n",
    "    print('Sampling rate:', dataset['train'][rand_int][\"sampling_rate\"])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b35ecb3-a6bb-4d7f-9551-af84310769ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['sampling_rate'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caa8835e-60e5-451d-94fe-d311732d3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch['speech'], sampling_rate=batch[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "977a8dd6-9d83-4234-b335-94482726a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Obtaining input values, input length, and labels... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-631b16b4329d1149/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b47afbb9ff5efe2a_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-631b16b4329d1149/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ffbd0277653c773f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/Jordan-OGI-finetune/csv/default-631b16b4329d1149/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-aa0464f6546e614b_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------ Obtaining input values, input length, and labels... ------------------\\n\")\n",
    "#prep_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)\n",
    "dataset = dataset.map(prepare_dataset, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1593bea-62fe-4291-a3ab-9ed697589ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 10, 8, 6, 13, 13, 6, 10, 0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd78d28c-dcb0-412d-a1e4-93d9676642db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text', 'input_values', 'input_length', 'labels'],\n",
      "        num_rows: 470\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text', 'input_values', 'input_length', 'labels'],\n",
      "        num_rows: 134\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['speech', 'sampling_rate', 'target_text', 'input_values', 'input_length', 'labels'],\n",
      "        num_rows: 132\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#print(prep_dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3980598b-b766-4033-8ff4-2ef0d530a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 82210])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "rannn=(torch.tensor(dataset['test'][0]['input_values']).unsqueeze(0))\n",
    "print(rannn.size())\n",
    "print(type(rannn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73221cc5-8f44-4e06-b9fb-0ac4e2e451e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------Training + Evaluation--------------------\n",
    "# ** Define a padding data collator used to pad the training samples to longest sample in their batch. In contrast to most NLP models,\n",
    "#    Wav2Vec2 has a much larger input length than output length. E.g., a sample of input length 50000 has an output length of no more than 100. \n",
    "#    Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only \n",
    "#    be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning Wav2Vec2 requires a special padding data collator, \n",
    "#    which we will define below\n",
    "# ** Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a compute_metrics function accordingly\n",
    "# ** Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n",
    "# ** Define the training configuration.\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace label ids of padding tokens with -100 so that those tokens are not taken into account when computing the loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9df96aeb-6a31-43e8-a96c-2c405903c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Setting up the padding data collator... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Setting up the padding data collator... ------------------\\n')\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25dd75e8-cff0-4c80-9577-2f9f1740aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Setting up WER metric... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pbs.4453745.kman.restech.unsw.edu.au/ipykernel_2452574/3043604959.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Setting up WER metric... ------------------\\n')\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af75c201-ed6e-460d-b900-732fcbf90089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96aeef41-a42d-4123-ae33-3b083ac9ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Loading a pretrained checkpount... ------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_q.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Loading a pretrained checkpount... ------------------\\n')\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    hidden_dropout=set_hidden_dropout,\n",
    "    activation_dropout=set_activation_dropout,\n",
    "    attention_dropout=set_attention_dropout,\n",
    "    feat_proj_dropout=set_feat_proj_dropout,\n",
    "    layerdrop=set_layerdrop,\n",
    "    mask_time_prob=set_mask_time_prob,\n",
    "    mask_time_length=set_mask_time_length,\n",
    "    ctc_loss_reduction=set_ctc_loss_reduction,\n",
    "    ctc_zero_infinity=set_ctc_zero_infinity    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea00f36b-d3ab-4fa5-9410-95cca877b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layers of Wav2vec2.0 model is sufficiently trained, hence they do not need to be finetuned anymore\n",
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7da9374-cee2-4139-8b49-768781fb4671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Setting TrainingArguments... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Setting TrainingArguments... ------------------\\n')\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_fp,\n",
    "    per_device_train_batch_size=set_per_device_train_batch_size,\n",
    "    group_by_length=set_group_by_length,\n",
    "    gradient_accumulation_steps=set_gradient_accumulation_steps,\n",
    "    gradient_checkpointing=set_gradient_checkpointing,\n",
    "    weight_decay=set_weight_decay,\n",
    "    fp16=set_fp16,\n",
    "    learning_rate=set_learning_rate,\n",
    "    lr_scheduler_type=set_lr_scheduler_type,\n",
    "    adam_beta1=set_adam_beta1,\n",
    "    adam_beta2=set_adam_beta2,\n",
    "    adam_epsilon=set_adam_epsilon,\n",
    "    warmup_ratio=set_warmup_ratio,\n",
    "    num_train_epochs=set_num_train_epochs,\n",
    "    max_steps=set_max_steps,\n",
    "    logging_strategy=set_logging_strategy,\n",
    "    logging_steps=set_logging_steps,\n",
    "    save_strategy=set_save_strategy,\n",
    "    save_steps=set_save_steps,\n",
    "    evaluation_strategy=set_evaluation_strategy,\n",
    "    eval_steps=set_eval_steps,\n",
    "    save_total_limit=set_save_total_limit,\n",
    "    load_best_model_at_end=set_load_best_model_at_end,\n",
    "    metric_for_best_model=set_metric_for_best_model,\n",
    "    greater_is_better=set_greater_is_better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6295a847-06a6-4eae-9dfa-616902fe62a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Setting Trainer... ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------ Setting Trainer... ------------------\\n')\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be5999-0d0e-485f-b3fe-6fe50c3d3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/z5313567/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n",
    "model.save_pretrained(model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abc409-7e4d-4210-8155-94129c8d20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------ Evaluation starts... ------------------\\n')\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3ed95-8fc7-4deb-bad2-30e8b024becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-timit-demo-google-colab\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f92bcf-e836-4e0e-badf-11189d554179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "  model.to(\"cuda\")\n",
    "  input_values = processor(\n",
    "      batch[\"speech\"], \n",
    "      sampling_rate=batch[\"sampling_rate\"], \n",
    "      return_tensors=\"pt\"\n",
    "  ).input_values.to(\"cuda\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  \n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37352b8b-ddc3-4efc-b0d8-39cd1e525382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "  with torch.no_grad():\n",
    "    input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "  return batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_jupyter_env",
   "language": "python",
   "name": "my_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
