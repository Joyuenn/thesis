Sun Oct 1 16:00:55 AEDT 2023
Found cached dataset csv (/srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
Running:  /srv/scratch/z5313567/thesis/whisper/code/whisper_finetune_CU.py
Started: 01/10/2023 16:00:55

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing re...
-->Importing json...
-->Importing Whisper Packages...
-->Importing soundfile...
-->Importing librosa...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

base_fp: /srv/scratch/z5313567/thesis/
model: whisper
dataset_name: CU
experiment_id: whisper_small_finetune_CU_20230930
cache_name: CU-finetune
training: True
use_checkpoint: False
use_pretrained_tokenizer: False
eval_pretrained: False
baseline_model: openai/whisper-small
eval_baseline: False

------> TRAINING ARGUMENTS... ----------------------------------------

learning_rate: 1e-05
per_device_train_batch_size: 64
per_device_eval_batch_size: 32
seed: 42
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-08
lr_scheduler_type: linear
warmup_steps: 500
max_steps: 3000
gradient_accumulation_steps: 1
gradient_checkpointing: True
fp16: True
evaluation_strategy: steps
predict_with_generate: True
generation_max_length: 225
save_steps: 1000
eval_steps: 1000
logging_steps: 1000
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: /srv/scratch/z5313567/thesis/CU_local/CU_full_train_15sec_only_transcription_filepath.csv
--> data_dev_fp: /srv/scratch/z5313567/thesis/CU_local/CU_full_dev_15sec_only_transcription_filepath.csv
--> data_test_fp: /srv/scratch/z5313567/thesis/CU_local/CU_full_test_15sec_only_transcription_filepath.csv
--> data_cache_fp: /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune
--> vocab_fp: /srv/scratch/z5313567/thesis/whisper/vocab/CU/whisper_small_finetune_CU_20230930_vocab.json
--> model_fp: /srv/scratch/z5313567/thesis/whisper/model/CU/whisper_small_finetune_CU_20230930
--> baseline_results_fp: /srv/scratch/z5313567/thesis/whisper/baseline_result/CU/whisper_small_finetune_CU_20230930_baseline_result.csv
--> finetuned_results_fp: /srv/scratch/z5313567/thesis/whisper/finetuned_result/CU/whisper_small_finetune_CU_20230930_finetuned_result.csv
--> pretrained_mod: openai/whisper-small

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/3 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 228.02it/s]
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-8668a786c0ab1e4f.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-32cb602fc53abd33.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f6c66d4630e34ab3.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-3aa643d6e7c0210a_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f00435ac0d422f8d_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-64a016d94532139b_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fbcde1b462bb6724_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-85e2b696fa162a1d_*_of_00004.arrow
Loading cached processed dataset at /srv/scratch/chacmod/.cache/huggingface/datasets/CU-finetune/csv/default-d9d1ee30c1275be4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7baa6b08a7b12432_*_of_00004.arrow
/srv/scratch/z5313567/thesis/whisper/code/whisper_finetune_CU.py:507: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
/home/z5313567/.local/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 57328
    })
    dev: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 11056
    })
    test: Dataset({
        features: ['filepath', 'transcription_clean'],
        num_rows: 10489
    })
})
--> Printing some random samples...
                                            filepath                                transcription_clean
0  /srv/scratch/chacmod/CU_2/corpus/data/train-pa...                                          GOLDFIELD
1  /srv/scratch/chacmod/CU_2/corpus/data/train-pa...                                              SHOTS
2  /srv/scratch/chacmod/CU_2/corpus/data/train-pa...                                     LITTLE BROTHER
3  /srv/scratch/chacmod/CU_2/corpus/data/train-pa...                                               LIKE
4  /srv/scratch/chacmod/CU_2/corpus/data/train-pa...  BY AND BY THEY FLEW TO THE RED AND YELLOW STRI...
SUCCESS: Prepared dataset.

------> PROCESSING TRANSCRIPTION... ---------------------------------------


------> Defining feature extractor... ---------------------------------------

SUCCESS: Feature extractor defined.

------> Defining tokenizer... ---------------------------------------

SUCCESS: Tokenizer defined.

------> Preparaing processor... ---------------------------------------

SUCCESS: Processor defined.

------> PRE-PROCESSING DATA... ----------------------------------------- 

--> Verifying data with a random sample...
Target text: WI
Input array shape: (14082,)
Sampling rate: 16000
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined WER evaluation metric.
--> Loading pre-trained checkpoint...

------> STARTING TRAINING... ----------------------------------------- 

  0%|          | 0/3000 [00:00<?, ?it/s]`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...
/home/z5313567/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Traceback (most recent call last):
  File "/srv/scratch/z5313567/thesis/whisper/code/whisper_finetune_CU.py", line 632, in <module>
    trainer.train()
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/trainer.py", line 1664, in train
    return inner_training_loop(
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/trainer.py", line 1940, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/z5313567/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2745, in training_step
    self.scaler.scale(loss).backward()
  File "/home/z5313567/.local/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/z5313567/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/z5313567/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/home/z5313567/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 157, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/z5313567/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.44 GiB (GPU 1; 31.74 GiB total capacity; 22.86 GiB already allocated; 2.56 GiB free; 28.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  0%|          | 0/3000 [00:17<?, ?it/s]
